apiVersion: v1
kind: Namespace
metadata:
  name: eirini
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: bits-service
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: runtime/default
    seccomp.security.alpha.kubernetes.io/defaultProfileName: runtime/default
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
  - ALL
  volumes:
  - configMap
  - secret
  - emptyDir
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: MustRunAsNonRoot
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  fsGroup:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  readOnlyRootFilesystem: false
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: eirini-app-psp
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default,runtime/default
    seccomp.security.alpha.kubernetes.io/defaultProfileName: runtime/default
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
  - ALL
  volumes:
  - configMap
  - emptyDir
  - projected
  - secret
  - downwardAPI
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  fsGroup:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  readOnlyRootFilesystem: false
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: eirini
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: runtime/default
    seccomp.security.alpha.kubernetes.io/defaultProfileName: runtime/default
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
  - ALL
  volumes:
  - configMap
  - emptyDir
  - projected
  - secret
  - downwardAPI
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: MustRunAsNonRoot
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  fsGroup:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  readOnlyRootFilesystem: false
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: eirini-events
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: runtime/default
    seccomp.security.alpha.kubernetes.io/defaultProfileName: runtime/default
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
  - ALL
  volumes:
  - configMap
  - secret
  - projected
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: MustRunAsNonRoot
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  fsGroup:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  readOnlyRootFilesystem: false
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: eirini-metrics
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: runtime/default
    seccomp.security.alpha.kubernetes.io/defaultProfileName: runtime/default
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
  - ALL
  volumes:
  - configMap
  - secret
  - projected
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: MustRunAsNonRoot
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  fsGroup:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  readOnlyRootFilesystem: false
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: eirini-routing
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: runtime/default
    seccomp.security.alpha.kubernetes.io/defaultProfileName: runtime/default
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
  - ALL
  volumes:
  - configMap
  - secret
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: MustRunAsNonRoot
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  fsGroup:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  readOnlyRootFilesystem: false
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: eirini-staging-reporter
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: runtime/default
    seccomp.security.alpha.kubernetes.io/defaultProfileName: runtime/default
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
  - ALL
  volumes:
  - configMap
  - secret
  - projected
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: MustRunAsNonRoot
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  fsGroup:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  readOnlyRootFilesystem: false
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
  name: kubecf-default
spec:
  allowPrivilegeEscalation: true
  allowedCapabilities:
  - NET_BIND_SERVICE
  - SYS_ADMIN
  - SYS_RESOURCE
  defaultAllowPrivilegeEscalation: true
  fsGroup:
    rule: RunAsAny
  hostPorts:
  - max: 65535
    min: 0
  privileged: true
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  volumes:
  - configMap
  - secret
  - emptyDir
  - downwardAPI
  - projected
  - persistentVolumeClaim
  - nfs
  - rbd
  - cephFS
  - glusterfs
  - fc
  - iscsi
  - cinder
  - gcePersistentDisk
  - awsElasticBlockStore
  - azureDisk
  - azureFile
  - vsphereVolume
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: bits-service
  namespace: default
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: eirini
  namespace: eirini
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: eirini-events
  namespace: default
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: eirini-metrics
  namespace: default
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: eirini-routing
  namespace: default
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: opi
  namespace: default
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: eirini-staging-reporter
  namespace: default
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: eirinix
  namespace: default
---
apiVersion: v1
kind: Secret
metadata:
  name: bits
type: Opaque
stringData:
  bits-config-key: |
    logging:
      level: debug
    private_endpoint: "https://bits.default.svc.cluster.local"
    public_endpoint: "https://registry.vcap.me:443"
    registry_endpoint: "https://registry.vcap.me"
    cert_file: /workspace/jobs/bits-service/certs/certificate
    key_file: /workspace/jobs/bits-service/certs/private_key
    port: 6666
    enable_http: true
    http_port: 8888
    secret:  secret
    skip_cert_verify: true
    max_body_size: 2M
    signing_users:
      - username: admin
        password:  notpassword123
    app_stash_config:
      maximum_size: 512M
      minimum_size: 64K
    buildpacks:
      blobstore_type: webdav
      webdav_config: &webdav_config
        directory_key: cc-buildpacks
        private_endpoint: https://singleton-blobstore.default.svc.cluster.local:4443
        public_endpoint: https://blobstore.vcap.me
        username: blobstore-user
        password: blobstore
        # TODO: provide proper cert file here
        ca_cert_path: /workspace/jobs/bits-service/certs/ca
        # TODO: remove this skip, when we have propert cert file above
        skip_cert_verify: true
    droplets:
      blobstore_type: webdav
      webdav_config:
        <<: *webdav_config
        directory_key: cc-droplets
    packages:
      blobstore_type: webdav
      webdav_config:
        <<: *webdav_config
        directory_key: cc-packages
    app_stash:
      blobstore_type: webdav
      webdav_config:
        <<: *webdav_config
        directory_key: cc-resources
    enable_registry: true
---
apiVersion: v1
kind: Secret
metadata:
  name: persi-broker-auth-password
  namespace: default
  labels:
    app.kubernetes.io/component: persi-broker
    app.kubernetes.io/name: eirinix
    helm.sh/chart: eirinix-0.0.0-10_g3df5bb4
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
type: Opaque
stringData:
  password: DJuLfIVjggCvjAvubJfSU0OxMIV7D4Q2
---
apiVersion: v1
kind: Secret
metadata:
  name: eirini-registry-credentials
  namespace: eirini
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: eyJhdXRocyI6IHsicmVnaXN0cnkuMTI3LjAuMC4xLm5pcC5pbzozMTY2NiI6IHsiYXV0aCI6ICJZV1J0YVc0NmJtOTBjR0Z6YzNkdmNtUXhNak09In19fQ==
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    quarks.cloudfoundry.org/secret-kind: generated
  annotations:
    quarks.cloudfoundry.org/secret-copy-of: default/var-eirini-tls-client-cert
  name: var-eirini-tls-client-cert
  namespace: eirini
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    quarks.cloudfoundry.org/secret-kind: generated
  annotations:
    quarks.cloudfoundry.org/secret-copy-of: default/var-cc-bridge-cc-uploader
  name: var-cc-bridge-cc-uploader
  namespace: eirini
---
apiVersion: v1
kind: Secret
metadata:
  name: var-system-domain
  namespace: default
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
type: Opaque
stringData:
  value: kubecf.vcap.me
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: eirini
  namespace: default
data:
  opi.yml: |
    opi:
      app_namespace: eirini
      registry_address: "registry.vcap.me:443"
      registry_secret_name: eirini-registry-credentials
      eirini_address: "https://eirini-opi.default.svc.cluster.local:8085"
      downloader_image: "registry.suse.com/cap-staging/recipe-downloader:1.5.0-24.1"
      executor_image: "registry.suse.com/cap-staging/recipe-executor:1.5.0-24.1"
      uploader_image: "registry.suse.com/cap-staging/recipe-uploader:1.5.0-24.1"

      cc_uploader_secret_name: var-cc-bridge-cc-uploader
      cc_uploader_cert_path: certificate
      cc_uploader_key_path: private_key

      client_certs_secret_name: var-eirini-tls-client-cert
      client_cert_path: certificate
      client_key_path: private_key

      ca_cert_secret_name: var-eirini-tls-client-cert
      ca_cert_path: ca

      cc_cert_path: "/workspace/jobs/opi/secrets/cc.crt"
      cc_key_path: "/workspace/jobs/opi/secrets/cc.key"
      cc_ca_path: "/workspace/jobs/opi/secrets/cc.ca"
      rootfs_version: v75.0.0
      client_ca_path: "/workspace/jobs/opi/secrets/eirini.ca"
      server_cert_path: "/workspace/jobs/opi/secrets/eirini-server.crt"
      server_key_path: "/workspace/jobs/opi/secrets/eirini-server.key"
      tls_port: 8085
      disk_limit_mb: 2048
      application_service_account: eirini
      allow_run_image_as_root: false
  routing.yml: |
    app_namespace: eirini
    nats_ip: "nats.default.svc.cluster.local"
    nats_port: 4222
  metrics.yml: |
    app_namespace: eirini
    loggregator_address: "doppler.default.svc.cluster.local:8082"
    loggergator_cert_path: "/etc/eirini/secrets/doppler.crt"
    loggregator_key_path: "/etc/eirini/secrets/doppler.key"
    loggregator_ca_path: "/etc/eirini/secrets/doppler.ca"
  events.yml: |
    app_namespace: eirini
    cc_internal_api: "https://api.default.svc.cluster.local:9023"
    cc_cert_path: "/etc/eirini/secrets/cc.crt"
    cc_key_path: "/etc/eirini/secrets/cc.key"
    cc_ca_path: "/etc/eirini/secrets/cc.ca"
  staging-reporter.yml: |
    app_namespace: eirini
    eirini_cert_path: "/etc/eirini/secrets/eirini-client.crt"
    eirini_key_path: "/etc/eirini/secrets/eirini-client.key"
    ca_path: "/etc/eirini/secrets/eirini-client.ca"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: use-native-eirini
  namespace: default
  labels:
    app.kubernetes.io/component: operations
data:
  ops: "# Remove the whole diego-cell instance group.\n- type: remove\n  path: /instance_groups/name=diego-cell\n\n# Remove bbs from diego-api.\n# TODO: remove bbs in the future - when the clock and worker no longer need it.\n# - type: remove\n#   path: /instance_groups/name=diego-api/jobs/name=bbs\n\n# Remove auctioneer, tps and ssh_proxy from scheduler.\n- type: remove\n  path: /instance_groups/name=auctioneer\n- type: remove\n  path: /instance_groups/name=scheduler/jobs/name=tps\n- type: remove\n  path: /instance_groups/name=scheduler/jobs/name=ssh_proxy\n\n# Enable OPI in CC\n# cloud_controller_ng\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/opi?/enabled?\n  value: true\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/opi?/url?\n  value: https://eirini-opi.default.svc.cluster.local:8085 \n\n  # --- Set endpoints and alternative names for eirini staging. See: \n  # https://github.com/cloudfoundry-incubator/kubecf/blob/master/deploy/helm/kubecf/assets/operations/instance_groups/eirini.yaml#L177-L201\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/packages?/webdav_config?/private_endpoint?\n  value: https://kubecf-singleton-blobstore.default.svc.cluster.local:4443\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/buildpacks?/webdav_config?/private_endpoint?\n  value: https://kubecf-singleton-blobstore.default.svc.cluster.local:4443\n- type: replace\n  path: /variables/name=blobstore_tls/options/alternative_names?/-\n  value: \"kubecf-singleton-blobstore.default.svc.cluster.local\"\n- type: replace\n  path: /variables/name=cc_bridge_cc_uploader_server/options/alternative_names?/-\n  value: \"kubecf-api.default.svc.cluster.local\"\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cc_uploader/properties/internal_hostname?\n  value: \"kubecf-api.default.svc.cluster.local\"\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/internal_service_hostname?\n  value: \"kubecf-api.default.svc.cluster.local\"\n- type: replace\n  path: /variables/name=cc_public_tls/options/alternative_names?/-\n  value: \"kubecf-api.default.svc.cluster.local\"\n- type: replace\n  path: /variables/name=cc_tls/options/alternative_names?/-\n  value: \"kubecf-api.default.svc.cluster.local\"\n\n  #---------------\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/opi?/opi_staging?\n  value: true\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/opi?/client_cert?\n  value: ((eirini_tls_client_cert.certificate))\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/opi?/client_key?\n  value: ((eirini_tls_client_cert.private_key))\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/opi?/ca_cert?\n  value: ((eirini_tls_server_cert.ca))\n\n# cloud_controller_worker\n- type: replace\n  path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/cc/opi?/enabled?\n  value: true\n- type: replace\n  path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/cc/opi?/url?\n  value: https://eirini-opi.default.svc.cluster.local:8085 \n- type: replace\n  path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/cc/opi?/opi_staging?\n  value: true\n- type: replace\n  path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/cc/opi?/client_cert?\n  value: ((eirini_tls_client_cert.certificate))\n- type: replace\n  path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/cc/opi?/client_key?\n  value: ((eirini_tls_client_cert.private_key))\n- type: replace\n  path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/cc/opi?/ca_cert?\n  value: ((eirini_tls_server_cert.ca))\n\n# cloud_controller_clock\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/cc/opi?/enabled?\n  value: true\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/cc/opi?/url?\n  value: https://eirini-opi.default.svc.cluster.local:8085 \n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/cc/opi?/opi_staging?\n  value: true\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/cc/opi?/client_cert?\n  value: ((eirini_tls_client_cert.certificate))\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/cc/opi?/client_key?\n  value: ((eirini_tls_client_cert.private_key))\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/cc/opi?/ca_cert?\n  value: ((eirini_tls_server_cert.ca))\n\n# Make loggregator agent cert validate correctly for fluentd in k8s nodes\n- type: replace\n  path: /variables/name=loggregator_tls_agent/options/alternative_names?\n  value:\n    - localhost\n    - metron\n- type: replace\n  path: /variables/name=loggregator_tls_doppler/options/alternative_names?/-\n  value: metron\n\n- type: replace\n  path: /variables/name=eirini_tls_server_cert?\n  value:\n    name: eirini_tls_server_cert\n    type: certificate\n    options:\n      ca: service_cf_internal_ca\n      common_name: \"eirini-opi\"\n      alternative_names:\n      - eirini-opi.kubecf.svc.cluster.local\n      extended_key_usage:\n      - server_auth\n- type: replace\n  path: /variables/name=eirini_tls_client_cert?\n  value:\n    name: eirini_tls_client_cert\n    type: certificate\n    options:\n      ca: service_cf_internal_ca\n      common_name: cloud_controller\n      extended_key_usage:\n      - client_auth\n"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cf-deployment
  namespace: default
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  manifest: |-
    ---
    name: cf
    manifest_version: v12.36.0
    update:
      canaries: 1
      canary_watch_time: 30000-1200000
      max_in_flight: 1
      serial: false
      update_watch_time: 5000-1200000
    addons:
    - name: loggregator_agent
      include:
        stemcell:
        - os: ubuntu-xenial
      exclude:
        jobs:
        - name: smoke_tests
          release: cf-smoke-tests
      jobs:
      - name: loggregator_agent
        release: loggregator-agent
        properties:
          grpc_port: 3459
          disable_udp: true
          loggregator:
            tls:
              ca_cert: "((loggregator_tls_agent.ca))"
              agent:
                cert: "((loggregator_tls_agent.certificate))"
                key: "((loggregator_tls_agent.private_key))"
          metrics:
            ca_cert: "((loggregator_agent_metrics_tls.ca))"
            cert: "((loggregator_agent_metrics_tls.certificate))"
            key: "((loggregator_agent_metrics_tls.private_key))"
            server_name: loggregator_agent_metrics

    - name: forwarder_agent
      include:
        stemcell:
        - os: ubuntu-xenial
      jobs:
      - name: loggr-forwarder-agent
        release: loggregator-agent
        properties:
          tls:
            ca_cert: "((loggregator_tls_agent.ca))"
            cert: "((loggregator_tls_agent.certificate))"
            key: "((loggregator_tls_agent.private_key))"
          metrics:
            ca_cert: "((forwarder_agent_metrics_tls.ca))"
            cert: "((forwarder_agent_metrics_tls.certificate))"
            key: "((forwarder_agent_metrics_tls.private_key))"
            server_name: forwarder_agent_metrics

    - name: prom_scraper
      include:
        stemcell:
          - os: ubuntu-xenial
      exclude:
        jobs:
        - name: smoke_tests
          release: cf-smoke-tests
      jobs:
      - name: prom_scraper
        release: loggregator-agent
        properties:
          scrape:
            tls:
              ca_cert: "((prom_scraper_scrape_tls.ca))"
              cert: "((prom_scraper_scrape_tls.certificate))"
              key: "((prom_scraper_scrape_tls.private_key))"
          metrics:
            ca_cert: "((prom_scraper_metrics_tls.ca))"
            cert: "((prom_scraper_metrics_tls.certificate))"
            key: "((prom_scraper_metrics_tls.private_key))"
            server_name: prom_scraper_metrics

    - name: bpm
      include:
        stemcell:
        - os: ubuntu-xenial
      jobs:
      - name: bpm
        release: bpm
    - name: bosh-dns-aliases
      jobs:
      - name: bosh-dns-aliases
        release: bosh-dns-aliases
        properties:
          aliases:
          - domain: '_.cell.service.cf.internal'
            targets:
            - query: '_'
              instance_group: diego-cell
              deployment: cf
              network: default
              domain: bosh
            - query: '_'
              instance_group: windows2012R2-cell
              deployment: cf
              network: default
              domain: bosh
            - query: '_'
              instance_group: windows2016-cell
              deployment: cf
              network: default
              domain: bosh
            - query: '_'
              instance_group: windows1803-cell
              deployment: cf
              network: default
              domain: bosh
            - query: '_'
              instance_group: windows2019-cell
              deployment: cf
              network: default
              domain: bosh
            - query: '_'
              instance_group: isolated-diego-cell
              deployment: cf
              network: default
              domain: bosh
          - domain: auctioneer.service.cf.internal
            targets:
            - query: 'q-s4'
              instance_group: scheduler
              deployment: cf
              network: default
              domain: bosh
          - domain: bbs.service.cf.internal
            targets:
            - query: 'q-s4'
              instance_group: diego-api
              deployment: cf
              network: default
              domain: bosh
          - domain: bits.service.cf.internal
            targets:
            - query: '*'
              instance_group: bits
              deployment: cf
              network: default
              domain: bosh
          - domain: blobstore.service.cf.internal
            targets:
            - query: '*'
              instance_group: singleton-blobstore
              deployment: cf
              network: default
              domain: bosh
          - domain: cc-uploader.service.cf.internal
            targets:
            - query: '*'
              instance_group: api
              deployment: cf
              network: default
              domain: bosh
          - domain: cloud-controller-ng.service.cf.internal
            targets:
            - query: '*'
              instance_group: api
              deployment: cf
              network: default
              domain: bosh
          - domain: credhub.service.cf.internal
            targets:
            - query: '*'
              instance_group: credhub
              deployment: cf
              network: default
              domain: bosh
          - domain: doppler.service.cf.internal
            targets:
            - query: '*'
              instance_group: doppler
              deployment: cf
              network: default
              domain: bosh
          - domain: file-server.service.cf.internal
            targets:
            - query: '*'
              instance_group: api
              deployment: cf
              network: default
              domain: bosh
          - domain: gorouter.service.cf.internal
            targets:
            - query: '*'
              instance_group: router
              deployment: cf
              network: default
              domain: bosh
          - domain: locket.service.cf.internal
            targets:
            - query: '*'
              instance_group: diego-api
              deployment: cf
              network: default
              domain: bosh
          - domain: loggregator-trafficcontroller.service.cf.internal
            targets:
            - query: '*'
              instance_group: log-api
              deployment: cf
              network: default
              domain: bosh
          - domain: policy-server.service.cf.internal
            targets:
            - query: '*'
              instance_group: api
              deployment: cf
              network: default
              domain: bosh
          - domain: reverse-log-proxy.service.cf.internal
            targets:
            - query: '*'
              instance_group: log-api
              deployment: cf
              network: default
              domain: bosh
          - domain: routing-api.service.cf.internal
            targets:
            - query: '*'
              instance_group: api
              deployment: cf
              network: default
              domain: bosh
          - domain: silk-controller.service.cf.internal
            targets:
            - query: '*'
              instance_group: diego-api
              deployment: cf
              network: default
              domain: bosh
          - domain: sql-db.service.cf.internal
            targets:
            - query: '*'
              instance_group: database
              deployment: cf
              network: default
              domain: bosh
          - domain: ssh-proxy.service.cf.internal
            targets:
            - query: '*'
              instance_group: scheduler
              deployment: cf
              network: default
              domain: bosh
          - domain: tps.service.cf.internal
            targets:
            - query: '*'
              instance_group: scheduler
              deployment: cf
              network: default
              domain: bosh
          - domain: uaa.service.cf.internal
            targets:
            - query: '*'
              instance_group: uaa
              deployment: cf
              network: default
              domain: bosh

    instance_groups:
    - name: smoke-tests
      lifecycle: errand
      azs:
      - z1
      instances: 1
      vm_type: minimal
      stemcell: default
      update:
        serial: true
      networks:
      - name: default
      jobs:
      - name: smoke_tests
        release: cf-smoke-tests
        properties:
          bpm:
            enabled: true
          smoke_tests:
            api: "https://api.((system_domain))"
            apps_domain: "((system_domain))"
            client: cf_smoke_tests
            client_secret: "((uaa_clients_cf_smoke_tests_secret))"
            org: cf_smoke_tests_org
            space: cf_smoke_tests_space
            cf_dial_timeout_in_seconds: 300
            skip_ssl_validation: true
      - name: cf-cli-6-linux
        release: cf-cli
    - name: nats
      azs:
      - z1
      - z2
      instances: 2
      vm_type: minimal
      stemcell: default
      networks:
      - name: default
      jobs:
      - name: nats
        release: nats
        provides:
          nats: {as: nats, shared: true}
        properties:
          nats:
            password: "((nats_password))"
            user: nats
    - name: adapter
      azs:
      - z1
      - z2
      instances: 2
      vm_type: minimal
      stemcell: default
      networks:
      - name: default
      jobs:
      - name: adapter
        release: cf-syslog-drain
        properties:
          scalablesyslog:
            adapter:
              tls:
                ca: "((adapter_tls.ca))"
                cert: "((adapter_tls.certificate))"
                key: "((adapter_tls.private_key))"
                cn: ss-adapter
            adapter_rlp:
              tls:
                ca: "((adapter_rlp_tls.ca))"
                cert: "((adapter_rlp_tls.certificate))"
                key: "((adapter_rlp_tls.private_key))"
                cn: reverselogproxy
    - name: database
      migrated_from:
      - name: mysql
      - name: singleton-database
      azs:
      - z1
      persistent_disk_type: 10GB
      instances: 1
      vm_type: small
      stemcell: default
      update:
        serial: true
      networks:
      - name: default
      jobs:
      - name: pxc-mysql
        release: pxc
        properties:
          admin_password: ((cf_mysql_mysql_admin_password))
          engine_config:
            binlog:
              enabled: false
            galera:
              enabled: true
          port: 13306
          seeded_databases:
          - name: cloud_controller
            password: ((cc_database_password))
            username: cloud_controller
          - name: diego
            password: ((diego_database_password))
            username: diego
          - name: network_connectivity
            password: ((network_connectivity_database_password))
            username: network_connectivity
          - name: network_policy
            password: ((network_policy_database_password))
            username: network_policy
          - name: routing-api
            password: ((routing_api_database_password))
            username: routing-api
          - name: uaa
            password: ((uaa_database_password))
            username: uaa
          - name: locket
            password: ((locket_database_password))
            username: locket
          - name: credhub
            password: ((credhub_database_password))
            username: credhub
          tls:
            galera: ((galera_server_certificate))
            server: ((mysql_server_certificate))
      - name: proxy
        release: pxc
        properties:
          api_password: ((cf_mysql_proxy_api_password))
          api_port: 8083
          api_uri: proxy.((system_domain))
      - name: galera-agent
        release: pxc
        properties:
          db_password: ((cf_mysql_mysql_galera_healthcheck_password))
          endpoint_password: ((cf_mysql_mysql_galera_healthcheck_endpoint_password))
      - name: gra-log-purger
        release: pxc
      - name: cluster-health-logger
        release: pxc
        properties:
          db_password: ((cf_mysql_mysql_cluster_health_password))
      - name: route_registrar
        release: routing
        properties:
          route_registrar:
            routes:
            - name: cf-mysql-proxy
              port: 8083
              prepend_instance_index: true
              registration_interval: 10s
              uris:
              - proxy.((system_domain))
            - name: cf-mysql-proxy-aggregator
              port: 8082
              registration_interval: 10s
              uris:
              - proxy.((system_domain))
      - name: bootstrap
        release: pxc
    - name: diego-api
      migrated_from:
      - name: diego-bbs
      azs:
      - z1
      - z2
      instances: 2
      vm_type: small
      stemcell: default
      update:
        serial: true
      networks:
      - name: default
      jobs:
      - name: cfdot
        release: diego
        properties:
          tls: &cfdot_tls_client_properties
            ca_certificate: "((diego_rep_client.ca))"
            certificate: "((diego_rep_client.certificate))"
            private_key: "((diego_rep_client.private_key))"
      - name: bbs
        release: diego
        properties:
          bpm:
            enabled: true
          diego:
            bbs:
              active_key_label: key-2016-06
              detect_consul_cell_registrations: false
              encryption_keys:
              - label: key-2016-06
                passphrase: "((diego_bbs_encryption_keys_passphrase))"
              sql:
                db_host: sql-db.service.cf.internal
                db_port: 3306
                db_schema: diego
                db_username: diego
                db_password: "((diego_database_password))"
                db_driver: mysql
                ca_cert: "((mysql_server_certificate.ca))"
                require_ssl: true
              ca_cert: "((diego_bbs_server.ca))"
              auctioneer: &diego_auctioneer_client_properties
                ca_cert: "((diego_auctioneer_client.ca))"
                client_cert: "((diego_auctioneer_client.certificate))"
                client_key: "((diego_auctioneer_client.private_key))"
              server_cert: "((diego_bbs_server.certificate))"
              server_key: "((diego_bbs_server.private_key))"
              skip_consul_lock: true
              rep:
                require_tls: true
                ca_cert: "((diego_rep_client.ca))"
                client_cert: "((diego_rep_client.certificate))"
                client_key: "((diego_rep_client.private_key))"
          enable_consul_service_registration: false
          loggregator: &diego_loggregator_client_properties
            use_v2_api: true
            ca_cert: "((loggregator_tls_agent.ca))"
            cert: "((loggregator_tls_agent.certificate))"
            key: "((loggregator_tls_agent.private_key))"
          logging:
            format:
              timestamp: "rfc3339"
      - name: silk-controller
        release: silk
        properties:
          ca_cert: ((silk_controller.ca))
          server_cert: ((silk_controller.certificate))
          server_key: ((silk_controller.private_key))
          database:
            type: mysql
            username: network_connectivity
            password: ((network_connectivity_database_password))
            host: sql-db.service.cf.internal
            port: 3306
            name: network_connectivity
            ca_cert: "((mysql_server_certificate.ca))"
            require_ssl: true
          silk_daemon:
            ca_cert: ((silk_daemon.ca))
            client_cert: ((silk_daemon.certificate))
            client_key: ((silk_daemon.private_key))
      - name: locket
        release: diego
        properties:
          bpm:
            enabled: true
          tls:
            ca_cert: "((diego_locket_server.ca))"
            cert: "((diego_locket_server.certificate))"
            key: "((diego_locket_server.private_key))"
          diego:
            locket:
              sql:
                db_host: sql-db.service.cf.internal
                db_port: 3306
                db_schema: locket
                db_username: locket
                db_password: "((locket_database_password))"
                db_driver: mysql
                ca_cert: "((mysql_server_certificate.ca))"
                require_ssl: true
          enable_consul_service_registration: false
          loggregator:
            use_v2_api: true
            ca_cert: "((loggregator_tls_agent.ca))"
            cert: "((loggregator_tls_agent.certificate))"
            key: "((loggregator_tls_agent.private_key))"
          logging:
            format:
              timestamp: "rfc3339"
      - name: loggr-udp-forwarder
        release: loggregator-agent
        properties:
          loggregator:
            tls:
              ca: "((loggregator_tls_agent.ca))"
              cert: "((loggregator_tls_agent.certificate))"
              key: "((loggregator_tls_agent.private_key))"
          metrics:
            ca_cert: "((loggr_udp_forwarder_tls.ca))"
            cert: "((loggr_udp_forwarder_tls.certificate))"
            key: "((loggr_udp_forwarder_tls.private_key))"
            server_name: loggr_udp_forwarder_metrics

    - name: uaa
      azs:
      - z1
      - z2
      instances: 2
      vm_type: minimal
      stemcell: default
      networks:
      - name: default
      jobs:
      - name: uaa
        release: uaa
        properties:
          encryption:
            active_key_label: default_key
            encryption_keys:
            - label: default_key
              passphrase: ((uaa_default_encryption_passphrase))
          login:
            saml:
              activeKeyId: key-1
              keys:
                key-1:
                  key: "((uaa_login_saml.private_key))"
                  certificate: "((uaa_login_saml.certificate))"
                  passphrase: ""
          uaa:
            sslCertificate: "((uaa_ssl.certificate))"
            sslPrivateKey: "((uaa_ssl.private_key))"
            zones:
              internal:
                hostnames:
                - uaa.service.cf.internal
            url: https://uaa.((system_domain))
            admin:
              client_secret: "((uaa_admin_client_secret))"
            logging_level: INFO
            scim:
              users:
              - name: admin
                password: "((cf_admin_password))"
                groups:
                - clients.read
                - cloud_controller.admin
                - doppler.firehose
                - network.admin
                - openid
                - routing.router_groups.read
                - routing.router_groups.write
                - scim.read
                - scim.write
            jwt:
              policy:
                active_key_id: key-1
                keys:
                  key-1:
                    signingKey: "((uaa_jwt_signing_key.private_key))"
            clients:
              cc_routing:
                authorities: routing.router_groups.read
                authorized-grant-types: client_credentials
                secret: "((uaa_clients_cc-routing_secret))"
              cc-service-dashboards:
                authorities: clients.read,clients.write,clients.admin
                authorized-grant-types: client_credentials
                scope: openid,cloud_controller_service_permissions.read
                secret: "((uaa_clients_cc-service-dashboards_secret))"
              cc_service_key_client:
                authorities: credhub.read,credhub.write
                authorized-grant-types: client_credentials
                secret: "((uaa_clients_cc_service_key_client_secret))"
              cf:
                access-token-validity: 600
                authorities: uaa.none
                authorized-grant-types: password,refresh_token
                override: true
                refresh-token-validity: 2592000
                scope: network.admin,network.write,cloud_controller.read,cloud_controller.write,openid,password.write,cloud_controller.admin,scim.read,scim.write,doppler.firehose,uaa.user,routing.router_groups.read,routing.router_groups.write,cloud_controller.admin_read_only,cloud_controller.global_auditor,perm.admin,clients.read
                secret: ''
              cf_smoke_tests:
                authorities: cloud_controller.admin
                authorized-grant-types: client_credentials
                secret: "((uaa_clients_cf_smoke_tests_secret))"
              cloud_controller_username_lookup:
                authorities: scim.userids
                authorized-grant-types: client_credentials
                secret: "((uaa_clients_cloud_controller_username_lookup_secret))"
              credhub_admin_client:
                authorities: credhub.read,credhub.write
                authorized-grant-types: client_credentials
                secret: ((credhub_admin_client_secret))
              doppler:
                authorities: uaa.resource
                override: true
                authorized-grant-types: client_credentials
                secret: "((uaa_clients_doppler_secret))"
              gorouter:
                authorities: routing.routes.read
                authorized-grant-types: client_credentials
                secret: "((uaa_clients_gorouter_secret))"
              ssh-proxy:
                authorized-grant-types: authorization_code
                autoapprove: true
                override: true
                redirect-uri: "https://uaa.((system_domain))/login"
                scope: openid,cloud_controller.read,cloud_controller.write,cloud_controller.admin
                secret: "((uaa_clients_ssh-proxy_secret))"
              routing_api_client:
                authorities: routing.routes.write,routing.routes.read,routing.router_groups.read
                authorized-grant-types: client_credentials
                secret: "((uaa_clients_routing_api_client_secret))"
              network-policy:
                authorities: uaa.resource,cloud_controller.admin_read_only
                authorized-grant-types: client_credentials
                secret: ((uaa_clients_network_policy_secret))
              tcp_emitter:
                authorities: routing.routes.write,routing.routes.read
                authorized-grant-types: client_credentials
                secret: "((uaa_clients_tcp_emitter_secret))"
              tcp_router:
                authorities: routing.routes.read
                authorized-grant-types: client_credentials
                secret: "((uaa_clients_tcp_router_secret))"
            ca_certs:
            - "((mysql_server_certificate.ca))"
          uaadb:
            address: sql-db.service.cf.internal
            databases:
            - name: uaa
              tag: uaa
            db_scheme: mysql
            port: 3306
            roles:
            - name: uaa
              password: "((uaa_database_password))"
              tag: admin
      - name: route_registrar
        release: routing
        properties:
          route_registrar:
            routes:
            - health_check:
                name: uaa-healthcheck
                script_path: "/var/vcap/jobs/uaa/bin/dns/healthy"
              name: uaa
              tls_port: 8443
              server_cert_domain_san: "uaa.service.cf.internal"
              registration_interval: 10s
              tags:
                component: uaa
              uris:
              - uaa.((system_domain))
              - "*.uaa.((system_domain))"
              - login.((system_domain))
              - "*.login.((system_domain))"
      - name: statsd_injector
        release: statsd-injector
        properties: &statsd_injector_properties
          loggregator:
            tls:
              ca_cert: "((loggregator_tls_statsdinjector.ca))"
              statsd_injector:
                cert: "((loggregator_tls_statsdinjector.certificate))"
                key: "((loggregator_tls_statsdinjector.private_key))"
    - name: singleton-blobstore
      migrated_from:
      - name: blobstore
      azs:
      - z1
      instances: 1
      vm_type: small
      persistent_disk_type: 100GB
      stemcell: default
      update:
        serial: true
      networks:
      - name: default
      jobs:
      - name: blobstore
        release: capi
        properties:
          select_directories_to_backup:
          - "buildpacks"
          - "packages"
          - "droplets"
          system_domain: "((system_domain))"
          blobstore:
            admin_users:
            - username: blobstore-user
              password: "((blobstore_admin_users_password))"
            secure_link:
              secret: "((blobstore_secure_link_secret))"
            tls:
              cert: "((blobstore_tls.certificate))"
              private_key: "((blobstore_tls.private_key))"
      - name: route_registrar
        release: routing
        properties:
          route_registrar:
            routes:
            - name: blobstore
              port: 8080
              registration_interval: 20s
              tags:
                component: blobstore
              uris:
              - blobstore.((system_domain))
    - name: api
      azs:
      - z1
      - z2
      instances: 2
      vm_type: small
      vm_extensions:
      - 50GB_ephemeral_disk
      stemcell: default
      update:
        serial: true
      networks:
      - name: default
      jobs:
      - name: cloud_controller_ng
        release: capi
        provides:
          cloud_controller: {as: cloud_controller, shared: true}
        properties:
          router:
            route_services_secret: "((router_route_services_secret))"
          system_domain: "((system_domain))"
          app_domains:
          - "((system_domain))"
          app_ssh:
            host_key_fingerprint: "((diego_ssh_proxy_host_key.public_key_fingerprint))"
          routing_api: &routing_api
            enabled: true
          credhub_api:
            ca_cert: ((credhub_tls.ca))
          ssl:
            skip_cert_verify: true
          uaa:
            ca_cert: "((uaa_ssl.ca))"
            clients:
              cc_routing:
                secret: "((uaa_clients_cc-routing_secret))"
              cloud_controller_username_lookup:
                secret: "((uaa_clients_cloud_controller_username_lookup_secret))"
              cc-service-dashboards:
                secret: "((uaa_clients_cc-service-dashboards_secret))"
              cc_service_key_client:
                secret: "((uaa_clients_cc_service_key_client_secret))"
            url: https://uaa.((system_domain))
          cc:
            diego:
              docker_staging_stack: cflinuxfs3
            stacks:
            - name: cflinuxfs3
              description: Cloud Foundry Linux-based filesystem (Ubuntu 18.04)
            default_running_security_groups:
            - public_networks
            - dns
            default_staging_security_groups:
            - public_networks
            - dns
            security_group_definitions:
            - name: public_networks
              rules:
              - destination: 0.0.0.0-9.255.255.255
                protocol: all
              - destination: 11.0.0.0-169.253.255.255
                protocol: all
              - destination: 169.255.0.0-172.15.255.255
                protocol: all
              - destination: 172.32.0.0-192.167.255.255
                protocol: all
              - destination: 192.169.0.0-255.255.255.255
                protocol: all
            - name: dns
              rules:
              - destination: 0.0.0.0/0
                ports: '53'
                protocol: tcp
              - destination: 0.0.0.0/0
                ports: '53'
                protocol: udp
            install_buildpacks:
            ## Order is important here
            - name: staticfile_buildpack
              package: staticfile-buildpack-cflinuxfs3
            - name: java_buildpack
              package: java-buildpack-cflinuxfs3
            - name: ruby_buildpack
              package: ruby-buildpack-cflinuxfs3
            - name: dotnet_core_buildpack
              package: dotnet-core-buildpack-cflinuxfs3
            - name: nodejs_buildpack
              package: nodejs-buildpack-cflinuxfs3
            - name: go_buildpack
              package: go-buildpack-cflinuxfs3
            - name: python_buildpack
              package: python-buildpack-cflinuxfs3
            - name: php_buildpack
              package: php-buildpack-cflinuxfs3
            - name: nginx_buildpack
              package: nginx-buildpack-cflinuxfs3
            - name: r_buildpack
              package: r-buildpack-cflinuxfs3
            - name: binary_buildpack
              package: binary-buildpack-cflinuxfs3
            db_encryption_key: "((cc_db_encryption_key))"
            database_encryption: &cc-database-encryption
              current_key_label: "encryption_key_0"
              keys:
                encryption_key_0: "((cc_db_encryption_key))"
            bulk_api_password: "((cc_bulk_api_password))"
            internal_api_password: "((cc_internal_api_password))"
            staging_upload_user: staging_user
            staging_upload_password: "((cc_staging_upload_password))"
            temporary_use_logcache: true
            logcache_tls:
              private_key: "((cc_logcache_tls.private_key))"
              certificate: "((cc_logcache_tls.certificate))"
            buildpacks: &blobstore-properties
              blobstore_type: webdav
              webdav_config:
                ca_cert: "((blobstore_tls.ca))"
                blobstore_timeout: 5
                password: "((blobstore_admin_users_password))"
                private_endpoint: https://blobstore.service.cf.internal:4443
                public_endpoint: https://blobstore.((system_domain))
                username: blobstore-user
            resource_pool: *blobstore-properties
            packages: *blobstore-properties
            droplets: *blobstore-properties
            mutual_tls: &cc_mutual_tls
              ca_cert: "((cc_tls.ca))"
              public_cert: "((cc_tls.certificate))"
              private_key: "((cc_tls.private_key))"
            public_tls:
              ca_cert: "((cc_public_tls.ca))"
              certificate: "((cc_public_tls.certificate))"
              private_key: "((cc_public_tls.private_key))"
          ccdb: &ccdb
            address: sql-db.service.cf.internal
            databases:
            - name: cloud_controller
              tag: cc
            db_scheme: mysql
            port: 3306
            roles:
            - name: cloud_controller
              password: "((cc_database_password))"
              tag: admin
            ca_cert: "((mysql_server_certificate.ca))"
      - name: binary-buildpack
        release: binary-buildpack
      - name: dotnet-core-buildpack
        release: dotnet-core-buildpack
      - name: go-buildpack
        release: go-buildpack
      - name: java-buildpack
        release: java-buildpack
      - name: nodejs-buildpack
        release: nodejs-buildpack
      - name: nginx-buildpack
        release: nginx-buildpack
      - name: r-buildpack
        release: r-buildpack
      - name: php-buildpack
        release: php-buildpack
      - name: python-buildpack
        release: python-buildpack
      - name: ruby-buildpack
        release: ruby-buildpack
      - name: staticfile-buildpack
        release: staticfile-buildpack
      - name: route_registrar
        release: routing
        properties:
          route_registrar:
            routes:
            - name: api
              registration_interval: 10s
              port: 9022
              tls_port: 9024
              server_cert_domain_san: "api.((system_domain))"
              tags:
                component: CloudController
              uris:
              - api.((system_domain))
              health_check:
                name: api-health-check
                script_path: "/var/vcap/jobs/cloud_controller_ng/bin/cloud_controller_ng_health_check"
                timeout: 6s
            - name: policy-server
              tls_port: 4002
              server_cert_domain_san: "api.((system_domain))"
              registration_interval: 20s
              uris:
              - api.((system_domain))/networking
      - name: statsd_injector
        release: statsd-injector
        properties: *statsd_injector_properties
      - name: file_server
        release: diego
        properties:
          bpm:
            enabled: true
          enable_consul_service_registration: false
          logging:
            format:
              timestamp: "rfc3339"
          loggregator: *diego_loggregator_client_properties
      - name: routing-api
        release: routing
        properties:
          routing_api:
            enabled_api_endpoints: "both"
            mtls_ca: "((routing_api_tls_client.ca))"
            mtls_server_cert: "((routing_api_tls.certificate))"
            mtls_server_key: "((routing_api_tls.private_key))"
            mtls_client_cert: "((routing_api_tls_client.certificate))"
            mtls_client_key: "((routing_api_tls_client.private_key))"
            skip_consul_lock: true
            system_domain: "((system_domain))"
            router_groups:
            - name: default-tcp
              type: tcp
              reservable_ports: 1024-1033
            sqldb:
              host: sql-db.service.cf.internal
              type: mysql
              port: 3306
              schema: routing-api
              username: routing-api
              password: "((routing_api_database_password))"
              ca_cert: "((mysql_server_certificate.ca))"
            locket:
              api_location: "locket.service.cf.internal:8891"
              ca_cert: "((diego_locket_client.ca))"
              client_cert: "((diego_locket_client.certificate))"
              client_key: "((diego_locket_client.private_key))"
          uaa:
            ca_cert: "((uaa_ssl.ca))"
            tls_port: 8443
      - name: policy-server
        release: cf-networking
        properties:
          uaa_client_secret: ((uaa_clients_network_policy_secret))
          uaa_ca: ((uaa_ssl.ca))
          enable_space_developer_self_service: true
          enable_tls: true
          database:
            type: mysql
            username: network_policy
            password: ((network_policy_database_password))
            host: sql-db.service.cf.internal
            port: 3306
            name: network_policy
            ca_cert: "((mysql_server_certificate.ca))"
            require_ssl: true
          server_cert: ((network_policy_server_external.certificate))
          server_key: ((network_policy_server_external.private_key))
      - name: policy-server-internal
        release: cf-networking
        properties:
          ca_cert: ((network_policy_server.ca))
          server_cert: ((network_policy_server.certificate))
          server_key: ((network_policy_server.private_key))
      - name: cc_uploader
        release: capi
        properties:
          capi:
            cc_uploader:
              cc:
                ca_cert: "((cc_bridge_cc_uploader.ca))"
                client_cert: "((cc_bridge_cc_uploader.certificate))"
                client_key: "((cc_bridge_cc_uploader.private_key))"
              mutual_tls:
                ca_cert: "((cc_bridge_cc_uploader_server.ca))"
                server_cert: "((cc_bridge_cc_uploader_server.certificate))"
                server_key: "((cc_bridge_cc_uploader_server.private_key))"
      - name: loggr-udp-forwarder
        release: loggregator-agent
        properties:
          loggregator:
            tls:
              ca: "((loggregator_tls_agent.ca))"
              cert: "((loggregator_tls_agent.certificate))"
              key: "((loggregator_tls_agent.private_key))"
          metrics:
            ca_cert: "((loggr_udp_forwarder_tls.ca))"
            cert: "((loggr_udp_forwarder_tls.certificate))"
            key: "((loggr_udp_forwarder_tls.private_key))"
            server_name: loggr_udp_forwarder_metrics
    - name: cc-worker
      azs:
      - z1
      - z2
      instances: 2
      vm_type: minimal
      stemcell: default
      networks:
      - name: default
      jobs:
      - name: cloud_controller_worker
        release: capi
        properties:
          cc:
            db_encryption_key: "((cc_db_encryption_key))"
            database_encryption: *cc-database-encryption
            internal_api_password: "((cc_internal_api_password))"
            staging_upload_user: staging_user
            staging_upload_password: "((cc_staging_upload_password))"
            resource_pool: *blobstore-properties
            packages: *blobstore-properties
            droplets: *blobstore-properties
            buildpacks: *blobstore-properties
            mutual_tls: *cc_mutual_tls
          ccdb: *ccdb
          system_domain: "((system_domain))"
          routing_api: *routing_api
          ssl:
            skip_cert_verify: true
          uaa:
            ca_cert: "((uaa_ssl.ca))"
            clients:
              cc-service-dashboards:
                secret: "((uaa_clients_cc-service-dashboards_secret))"
              cc_routing:
                secret: "((uaa_clients_cc-routing_secret))"
    - name: scheduler
      azs:
      - z1
      - z2
      instances: 2
      migrated_from:
      - {name: cc-bridge}
      - {name: cc-clock}
      - {name: diego-brain}
      vm_type: minimal
      vm_extensions:
      - diego-ssh-proxy-network-properties
      stemcell: default
      update:
        serial: true
      networks:
      - name: default
      jobs:
      - name: cfdot
        release: diego
        properties:
          tls: *cfdot_tls_client_properties
      - name: auctioneer
        release: diego
        properties:
          bpm:
            enabled: true
          diego:
            auctioneer:
              bbs: &diego_bbs_client_properties
                ca_cert: "((diego_bbs_client.ca))"
                client_cert: "((diego_bbs_client.certificate))"
                client_key: "((diego_bbs_client.private_key))"
              ca_cert: "((diego_auctioneer_server.ca))"
              rep:
                require_tls: true
                ca_cert: "((diego_rep_client.ca))"
                client_cert: "((diego_rep_client.certificate))"
                client_key: "((diego_rep_client.private_key))"
              server_cert: "((diego_auctioneer_server.certificate))"
              server_key: "((diego_auctioneer_server.private_key))"
              skip_consul_lock: true
          enable_consul_service_registration: false
          loggregator: *diego_loggregator_client_properties
          logging:
            format:
              timestamp: "rfc3339"
      - name: cloud_controller_clock
        release: capi
        properties:
          cc:
            db_encryption_key: "((cc_db_encryption_key))"
            database_encryption: *cc-database-encryption
            internal_api_password: "((cc_internal_api_password))"
            staging_upload_user: staging_user
            staging_upload_password: "((cc_staging_upload_password))"
            resource_pool: *blobstore-properties
            packages: *blobstore-properties
            droplets: *blobstore-properties
            buildpacks: *blobstore-properties
            mutual_tls: *cc_mutual_tls
          ccdb: *ccdb
          system_domain: "((system_domain))"
          routing_api: *routing_api
          ssl:
            skip_cert_verify: true
          uaa:
            ca_cert: "((uaa_ssl.ca))"
            clients:
              cc-service-dashboards:
                secret: "((uaa_clients_cc-service-dashboards_secret))"
              cc_routing:
                secret: "((uaa_clients_cc-routing_secret))"
            ssl:
              port: 8443
      - name: cc_deployment_updater
        release: capi
        properties:
          cc:
            db_encryption_key: ((cc_db_encryption_key))
            mutual_tls:
              ca_cert: "((cc_tls.ca))"
              private_key: "((cc_tls.private_key))"
              public_cert: "((cc_tls.certificate))"
          ccdb:
            databases:
            - name: cloud_controller
              tag: cc
            db_scheme: mysql
            port: 3306
            roles:
            - name: cloud_controller
              password: ((cc_database_password))
              tag: admin
      - name: statsd_injector
        release: statsd-injector
        properties: *statsd_injector_properties
      - name: tps
        release: capi
        properties:
          capi:
            tps:
              bbs: *diego_bbs_client_properties
              watcher:
                locket:
                  api_location: "locket.service.cf.internal:8891"
                skip_consul_lock: true
              cc:
                ca_cert: "((cc_bridge_tps.ca))"
                client_cert: "((cc_bridge_tps.certificate))"
                client_key: "((cc_bridge_tps.private_key))"
      - name: ssh_proxy
        release: diego
        properties:
          bpm:
            enabled: true
          diego:
            ssh_proxy:
              enable_cf_auth: true
              host_key: "((diego_ssh_proxy_host_key.private_key))"
              uaa_secret: "((uaa_clients_ssh-proxy_secret))"
              uaa:
                ca_cert: "((uaa_ssl.ca))"
              bbs: *diego_bbs_client_properties
              disable_healthcheck_server: true
          backends:
            tls:
              enabled: true
              ca_certificates:
              - ((diego_instance_identity_ca.ca))
              client_certificate: ((ssh_proxy_backends_tls.certificate))
              client_private_key: ((ssh_proxy_backends_tls.private_key))
          enable_consul_service_registration: false
          loggregator: *diego_loggregator_client_properties
          logging:
            format:
              timestamp: "rfc3339"
      - name: scheduler
        release: cf-syslog-drain
        properties:
          scalablesyslog:
            scheduler:
              api:
                url: https://cloud-controller-ng.service.cf.internal:9023
              tls:
                client:
                  ca: "((scheduler_client_tls.ca))"
                  cert: "((scheduler_client_tls.certificate))"
                  key: "((scheduler_client_tls.private_key))"
                  adapter_cn: "ss-adapter"
                api:
                  ca: "((scheduler_api_tls.ca))"
                  cert: "((scheduler_api_tls.certificate))"
                  key: "((scheduler_api_tls.private_key))"
                  cn: "cloud-controller-ng.service.cf.internal"
      - name: loggr-udp-forwarder
        release: loggregator-agent
        properties:
          loggregator:
            tls:
              ca: "((loggregator_tls_agent.ca))"
              cert: "((loggregator_tls_agent.certificate))"
              key: "((loggregator_tls_agent.private_key))"
          metrics:
            ca_cert: "((loggr_udp_forwarder_tls.ca))"
            cert: "((loggr_udp_forwarder_tls.certificate))"
            key: "((loggr_udp_forwarder_tls.private_key))"
            server_name: loggr_udp_forwarder_metrics
    - name: router
      azs:
      - z1
      - z2
      instances: 2
      vm_type: minimal
      vm_extensions:
      - cf-router-network-properties
      stemcell: default
      update:
        serial: true
      networks:
      - name: default
      jobs:
      - name: gorouter
        release: routing
        properties:
          router:
            enable_ssl: true
            tls_pem:
            - cert_chain: "((router_ssl.certificate))"
              private_key: "((router_ssl.private_key))"
            ca_certs: |
              ((diego_instance_identity_ca.ca))
              ((cc_tls.ca))
              ((uaa_ssl.ca))
              ((network_policy_server_external.ca))
            backends:
              cert_chain: ((gorouter_backend_tls.certificate))
              private_key: ((gorouter_backend_tls.private_key))
            status:
              password: "((router_status_password))"
              user: router-status
            route_services_secret: "((router_route_services_secret))"
            tracing:
              enable_zipkin: true
          routing_api:
            enabled: true
          uaa:
            clients:
              gorouter:
                secret: "((uaa_clients_gorouter_secret))"
            ca_cert: "((uaa_ssl.ca))"
            ssl:
              port: 8443
      - name: loggr-udp-forwarder
        release: loggregator-agent
        properties:
          loggregator:
            tls:
              ca: "((loggregator_tls_agent.ca))"
              cert: "((loggregator_tls_agent.certificate))"
              key: "((loggregator_tls_agent.private_key))"
          metrics:
            ca_cert: "((loggr_udp_forwarder_tls.ca))"
            cert: "((loggr_udp_forwarder_tls.certificate))"
            key: "((loggr_udp_forwarder_tls.private_key))"
            server_name: loggr_udp_forwarder_metrics
    - name: tcp-router
      azs:
      - z1
      - z2
      instances: 2
      vm_type: minimal
      stemcell: default
      vm_extensions:
      - cf-tcp-router-network-properties
      networks:
      - name: default
      jobs:
      - name: tcp_router
        release: routing
        properties:
          tcp_router:
            oauth_secret: "((uaa_clients_tcp_router_secret))"
            router_group: default-tcp
          uaa:
            ca_cert: "((uaa_ssl.ca))"
            tls_port: 8443
      - name: loggr-udp-forwarder
        release: loggregator-agent
        properties:
          loggregator:
            tls:
              ca: "((loggregator_tls_agent.ca))"
              cert: "((loggregator_tls_agent.certificate))"
              key: "((loggregator_tls_agent.private_key))"
          metrics:
            ca_cert: "((loggr_udp_forwarder_tls.ca))"
            cert: "((loggr_udp_forwarder_tls.certificate))"
            key: "((loggr_udp_forwarder_tls.private_key))"
            server_name: loggr_udp_forwarder_metrics
    - name: doppler
      azs:
      - z1
      - z2
      instances: 4
      vm_type: minimal
      stemcell: default
      networks:
      - name: default
      jobs:
      - name: doppler
        release: loggregator
        provides:
          doppler: {as: doppler, shared: true}
        properties:
          loggregator:
            tls:
              ca_cert: "((loggregator_tls_doppler.ca))"
              doppler:
                cert: "((loggregator_tls_doppler.certificate))"
                key: "((loggregator_tls_doppler.private_key))"
      - name: log-cache
        provides:
          log-cache: {shared: true}
        properties:
          metrics:
            ca_cert: "((log_cache_metrics_tls.ca))"
            cert: "((log_cache_metrics_tls.certificate))"
            key: "((log_cache_metrics_tls.private_key))"
            server_name: log_cache_metrics
          health_addr: localhost:6060
          tls:
            ca_cert: ((log_cache.ca))
            cert: ((log_cache.certificate))
            key: ((log_cache.private_key))
        release: log-cache
      - name: log-cache-gateway
        properties:
          gateway_addr: localhost:8081
          proxy_cert: "((log_cache_proxy_tls.certificate))"
          proxy_key: "((log_cache_proxy_tls.private_key))"
          metrics:
            ca_cert: "((log_cache_gateway_metrics_tls.ca))"
            cert: "((log_cache_gateway_metrics_tls.certificate))"
            key: "((log_cache_gateway_metrics_tls.private_key))"
            server_name: log_cache_gateway_metrics
        release: log-cache
      - consumes:
          reverse_log_proxy: {from: reverse_log_proxy}
        name: log-cache-nozzle
        properties:
          logs_provider:
            tls:
              ca_cert: ((logs_provider.ca))
              cert: ((logs_provider.certificate))
              key: ((logs_provider.private_key))
        release: log-cache
      - name: route_registrar
        properties:
          route_registrar:
            routes:
            - name: log-cache-reverse-proxy
              port: 8083
              tls_port: 8083
              registration_interval: 20s
              server_cert_domain_san: log-cache.((system_domain))
              uris:
              - log-cache.((system_domain))
              - '*.log-cache.((system_domain))'
        release: routing
      - name: log-cache-cf-auth-proxy
        properties:
          metrics:
            ca_cert: "((log_cache_cf_auth_proxy_metrics_tls.ca))"
            cert: "((log_cache_cf_auth_proxy_metrics_tls.certificate))"
            key: "((log_cache_cf_auth_proxy_metrics_tls.private_key))"
            server_name: log_cache_cf_auth_proxy_metrics
          cc:
            ca_cert: ((cc_tls.ca))
            common_name: cloud-controller-ng.service.cf.internal
          proxy_ca_cert: "((log_cache.ca))"
          proxy_port: 8083
          external_cert: ((logcache_ssl.certificate))
          external_key: ((logcache_ssl.private_key))
          uaa:
            ca_cert: ((uaa_ssl.ca))
            client_id: doppler
            client_secret: ((uaa_clients_doppler_secret))
            internal_addr: https://uaa.service.cf.internal:8443
        release: log-cache
    - name: diego-cell
      azs:
      - z1
      - z2
      instances: 3
      vm_type: small-highmem
      vm_extensions:
      - 100GB_ephemeral_disk
      stemcell: default
      networks:
      - name: default
      jobs:
      - name: cflinuxfs3-rootfs-setup
        release: cflinuxfs3
        properties:
          cflinuxfs3-rootfs:
            trusted_certs:
            - ((diego_instance_identity_ca.ca))
            - ((credhub_tls.ca))
            - ((uaa_ssl.ca))
      - name: garden
        release: garden-runc
        properties:
          garden:
            containerd_mode: true
            cleanup_process_dirs_on_wait: true
            debug_listen_address: 127.0.0.1:17019
            default_container_grace_time: 0
            destroy_containers_on_start: true
            deny_networks:
            - 0.0.0.0/0
            network_plugin: /var/vcap/packages/runc-cni/bin/garden-external-networker
            network_plugin_extra_args:
            - --configFile=/var/vcap/jobs/garden-cni/config/adapter.json
          logging:
            format:
              timestamp: "rfc3339"
      - name: rep
        release: diego
        properties:
          bpm:
            enabled: true
          diego:
            executor:
              instance_identity_ca_cert: ((diego_instance_identity_ca.certificate))
              instance_identity_key: ((diego_instance_identity_ca.private_key))
            rep:
              preloaded_rootfses:
              - cflinuxfs3:/var/vcap/packages/cflinuxfs3/rootfs.tar
          containers:
            proxy:
              enabled: true
              require_and_verify_client_certificates: true
              trusted_ca_certificates:
              - ((gorouter_backend_tls.ca))
              - ((ssh_proxy_backends_tls.ca))
              verify_subject_alt_name:
              - gorouter.service.cf.internal
              - ssh-proxy.service.cf.internal
            trusted_ca_certificates:
            - ((diego_instance_identity_ca.ca))
            - ((credhub_tls.ca))
            - ((uaa_ssl.ca))
          enable_consul_service_registration: false
          enable_declarative_healthcheck: true
          loggregator: *diego_loggregator_client_properties
          tls:
            ca_cert: "((diego_rep_agent_v2.ca))"
            cert: "((diego_rep_agent_v2.certificate))"
            key: "((diego_rep_agent_v2.private_key))"
          logging:
            format:
              timestamp: "rfc3339"
      - name: cfdot
        release: diego
        properties:
          tls: *cfdot_tls_client_properties
      - name: route_emitter
        release: diego
        properties:
          bpm:
            enabled: true
          loggregator: *diego_loggregator_client_properties
          diego:
            route_emitter:
              local_mode: true
              bbs:
                ca_cert: "((diego_bbs_client.ca))"
                client_cert: "((diego_bbs_client.certificate))"
                client_key: "((diego_bbs_client.private_key))"
          tcp:
            enabled: true
          uaa:
            ca_cert: "((uaa_ssl.ca))"
            client_secret: "((uaa_clients_tcp_emitter_secret))"
          logging:
            format:
              timestamp: "rfc3339"
      - name: garden-cni
        release: cf-networking
        properties:
          cni_plugin_dir: /var/vcap/packages/silk-cni/bin
          cni_config_dir: /var/vcap/jobs/silk-cni/config/cni
      - name: netmon
        release: silk
      - name: vxlan-policy-agent
        release: silk
        properties:
          ca_cert: ((network_policy_client.ca))
          client_cert: ((network_policy_client.certificate))
          client_key: ((network_policy_client.private_key))
      - name: silk-daemon
        release: silk
        properties:
          ca_cert: ((silk_daemon.ca))
          client_cert: ((silk_daemon.certificate))
          client_key: ((silk_daemon.private_key))
      - name: silk-cni
        release: silk
        properties:
          dns_servers:
          - 169.254.0.2
      - name: loggr-udp-forwarder
        release: loggregator-agent
        properties:
          loggregator:
            tls:
              ca: "((loggregator_tls_agent.ca))"
              cert: "((loggregator_tls_agent.certificate))"
              key: "((loggregator_tls_agent.private_key))"
          metrics:
            ca_cert: "((loggr_udp_forwarder_tls.ca))"
            cert: "((loggr_udp_forwarder_tls.certificate))"
            key: "((loggr_udp_forwarder_tls.private_key))"
            server_name: loggr_udp_forwarder_metrics
    - name: log-api
      azs:
      - z1
      - z2
      instances: 2
      vm_type: minimal
      stemcell: default
      update:
        serial: true
      networks:
      - name: default
      jobs:
      - name: loggregator_trafficcontroller
        release: loggregator
        consumes:
          doppler: {from: doppler}
        properties:
          uaa:
            internal_url: https://uaa.service.cf.internal:8443
            ca_cert: "((uaa_ssl.ca))"
          loggregator:
            outgoing_cert: "((loggregator_trafficcontroller_tls.certificate))"
            outgoing_key: "((loggregator_trafficcontroller_tls.private_key))"
            tls:
              cc_trafficcontroller:
                cert: "((loggregator_tls_cc_tc.certificate))"
                key: "((loggregator_tls_cc_tc.private_key))"
              ca_cert: "((loggregator_tls_tc.ca))"
              trafficcontroller:
                cert: "((loggregator_tls_tc.certificate))"
                key: "((loggregator_tls_tc.private_key))"
            uaa:
              client_secret: "((uaa_clients_doppler_secret))"
          system_domain: "((system_domain))"
          ssl:
            skip_cert_verify: true
          cc:
            internal_service_hostname: "cloud-controller-ng.service.cf.internal"
            tls_port: 9023
            mutual_tls:
              ca_cert: "((cc_tls.ca))"
      - name: reverse_log_proxy
        release: loggregator
        provides:
          reverse_log_proxy: {as: reverse_log_proxy, shared: true}
        properties:
          loggregator:
            tls:
              ca_cert: "((loggregator_tls_rlp.ca))"
              reverse_log_proxy:
                cert: "((loggregator_tls_rlp.certificate))"
                key: "((loggregator_tls_rlp.private_key))"
      - name: reverse_log_proxy_gateway
        release: loggregator
        properties:
          http:
            address: "0.0.0.0:8088"
            cert: "((loggregator_rlp_gateway_tls.certificate))"
            key: "((loggregator_rlp_gateway_tls.private_key))"
          logs_provider:
            ca_cert: "((loggregator_rlp_gateway.ca))"
            client_cert: "((loggregator_rlp_gateway.certificate))"
            client_key: "((loggregator_rlp_gateway.private_key))"
          cc:
            capi_internal_addr: https://cloud-controller-ng.service.cf.internal:9023
            ca_cert: ((loggregator_rlp_gateway_tls_cc.ca))
            cert: ((loggregator_rlp_gateway_tls_cc.certificate))
            key: ((loggregator_rlp_gateway_tls_cc.private_key))
            common_name: cloud-controller-ng.service.cf.internal
          uaa:
            ca_cert: ((uaa_ssl.ca))
            client_id: doppler
            client_secret: ((uaa_clients_doppler_secret))
            internal_addr: https://uaa.service.cf.internal:8443
          metrics:
            ca_cert: "((rlp_gateway_metrics_tls.ca))"
            cert: "((rlp_gateway_metrics_tls.certificate))"
            key: "((rlp_gateway_metrics_tls.private_key))"
            server_name: rlp_gateway_metrics
      - name: route_registrar
        release: routing
        properties:
          route_registrar:
            routes:
            - name: doppler
              tls_port: 8081
              registration_interval: 20s
              server_cert_domain_san: doppler.((system_domain))
              uris:
              - doppler.((system_domain))
              - "*.doppler.((system_domain))"
            - name: rlp-gateway
              tls_port: 8088
              server_cert_domain_san: log-stream.((system_domain))
              registration_interval: 20s
              uris:
              - log-stream.((system_domain))
              - "*.log-stream.((system_domain))"
    - name: credhub
      azs:
      - z1
      - z2
      instances: 2
      networks:
      - name: default
      stemcell: default
      vm_type: minimal
      jobs:
      - name: credhub
        properties:
          credhub:
            authentication:
              mutual_tls:
                trusted_cas:
                - ((diego_instance_identity_ca.ca))
              uaa:
                ca_certs:
                - ((uaa_ssl.ca))
                url: https://uaa.service.cf.internal:8443
            authorization:
              acls:
                enabled: true
              permissions:
              - path: /*
                actors: ["uaa-client:credhub_admin_client"]
                operations: [read, write, delete, read_acl, write_acl]
              - path: /*
                actors: ["uaa-client:cc_service_key_client"]
                operations: [read]
            ca_certificate: |
              ((credhub_tls.ca))
            data_storage:
              database: credhub
              host: sql-db.service.cf.internal
              password: ((credhub_database_password))
              port: 3306
              type: mysql
              username: credhub
              tls_ca: "((mysql_server_certificate.ca))"
            encryption:
              keys:
              - active: true
                key_properties:
                  encryption_password: ((credhub_encryption_password))
                provider_name: internal-provider
              providers:
              - name: internal-provider
                type: internal
            internal_url: https://credhub.service.cf.internal
            tls: ((credhub_tls))
        release: credhub
    - name: rotate-cc-database-key
      azs:
      - z1
      instances: 1
      lifecycle: errand
      vm_type: minimal
      stemcell: default
      networks:
      - name: default
      jobs:
      - name: rotate_cc_database_key
        release: capi
        properties: {}

    variables:
    - name: blobstore_admin_users_password
      type: password
    - name: blobstore_secure_link_secret
      type: password
    - name: cc_bulk_api_password
      type: password
    - name: cc_db_encryption_key
      type: password
    - name: cc_internal_api_password
      type: password
    - name: cc_staging_upload_password
      type: password
    - name: cf_mysql_mysql_admin_password
      type: password
    - name: cf_mysql_mysql_cluster_health_password
      type: password
    - name: cf_mysql_mysql_galera_healthcheck_endpoint_password
      type: password
    - name: cf_mysql_mysql_galera_healthcheck_password
      type: password
    - name: cf_mysql_proxy_api_password
      type: password
    - name: cc_database_password
      type: password
    - name: credhub_database_password
      type: password
    - name: diego_database_password
      type: password
    - name: uaa_database_password
      type: password
    - name: routing_api_database_password
      type: password
    - name: network_policy_database_password
      type: password
    - name: network_connectivity_database_password
      type: password
    - name: uaa_default_encryption_passphrase
      type: password
    - name: silk_ca
      type: certificate
      options:
        is_ca: true
        common_name: silk-ca
    - name: silk_controller
      type: certificate
      options:
        ca: silk_ca
        common_name: silk-controller.service.cf.internal
        extended_key_usage:
        - server_auth
    - name: silk_daemon
      type: certificate
      options:
        ca: silk_ca
        common_name: silk-daemon
        extended_key_usage:
        - client_auth
    - name: network_policy_ca
      type: certificate
      options:
        is_ca: true
        common_name: networkPolicyCA
    - name: network_policy_server_external
      type: certificate
      options:
        ca: network_policy_ca
        common_name: "api.((system_domain))"
        alternative_names:
        - "api.((system_domain))"
        extended_key_usage:
        - server_auth
    - name: network_policy_server
      type: certificate
      options:
        ca: network_policy_ca
        common_name: policy-server.service.cf.internal
        extended_key_usage:
        - server_auth
    - name: network_policy_client
      type: certificate
      options:
        ca: network_policy_ca
        common_name: clientName
        extended_key_usage:
        - client_auth
    - name: uaa_clients_routing_api_client_secret
      type: password
    - name: uaa_clients_tcp_emitter_secret
      type: password
    - name: nats_password
      type: password
    - name: router_status_password
      type: password
    - name: cf_admin_password
      type: password
    - name: cf_bosh_password
      type: password
    - name: router_route_services_secret
      type: password
    - name: uaa_admin_client_secret
      type: password
    - name: uaa_clients_cc-routing_secret
      type: password
    - name: uaa_clients_cc-service-dashboards_secret
      type: password
    - name: uaa_clients_cc_service_key_client_secret
      type: password
    - name: uaa_clients_cf_smoke_tests_secret
      type: password
    - name: uaa_clients_cloud_controller_username_lookup_secret
      type: password
    - name: uaa_clients_doppler_secret
      type: password
    - name: uaa_clients_gorouter_secret
      type: password
    - name: uaa_clients_network_policy_secret
      type: password
    - name: uaa_clients_ssh-proxy_secret
      type: password
    - name: uaa_clients_tcp_router_secret
      type: password
    - name: diego_bbs_encryption_keys_passphrase
      type: password
    - name: credhub_encryption_password
      type: password
    - name: credhub_admin_client_secret
      type: password
    - name: diego_ssh_proxy_host_key
      type: ssh
    - name: uaa_jwt_signing_key
      type: rsa
    - name: service_cf_internal_ca
      type: certificate
      options:
        is_ca: true
        common_name: internalCA
    - name: blobstore_tls
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: blobstore.service.cf.internal
    - name: diego_auctioneer_client
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: auctioneer client
        extended_key_usage:
        - client_auth
    - name: diego_auctioneer_server
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: auctioneer.service.cf.internal
        extended_key_usage:
        - server_auth
        alternative_names:
        - "*.auctioneer.service.cf.internal"
        - auctioneer.service.cf.internal
    - name: diego_bbs_client
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: bbs client
        extended_key_usage:
        - client_auth
    - name: diego_bbs_server
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: bbs.service.cf.internal
        extended_key_usage:
        - server_auth
        - client_auth
        alternative_names:
        - "*.bbs.service.cf.internal"
        - bbs.service.cf.internal
    - name: diego_rep_client
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: rep client
        extended_key_usage:
        - client_auth
    - name: diego_rep_agent_v2
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: cell.service.cf.internal
        extended_key_usage:
        - client_auth
        - server_auth
        alternative_names:
        - "*.cell.service.cf.internal"
        - cell.service.cf.internal
        - 127.0.0.1
        - localhost
    - name: loggregator_ca
      type: certificate
      options:
        is_ca: true
        common_name: loggregatorCA
    - name: loggregator_tls_statsdinjector
      type: certificate
      options:
        ca: loggregator_ca
        common_name: statsdinjector
        extended_key_usage:
        - client_auth
    - name: loggregator_tls_agent
      type: certificate
      options:
        ca: loggregator_ca
        common_name: metron
        extended_key_usage:
        - client_auth
        - server_auth
    - name: loggregator_tls_doppler
      type: certificate
      options:
        ca: loggregator_ca
        common_name: doppler
        extended_key_usage:
        - client_auth
        - server_auth
    - name: loggregator_tls_tc
      type: certificate
      options:
        ca: loggregator_ca
        common_name: trafficcontroller
        extended_key_usage:
        - client_auth
        - server_auth
    - name: loggregator_tls_cc_tc
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: trafficcontroller
        extended_key_usage:
        - client_auth
    - name: loggregator_rlp_gateway_tls_cc
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: rlp-gateway
        extended_key_usage:
        - client_auth
    - name: loggregator_tls_rlp
      type: certificate
      options:
        ca: loggregator_ca
        common_name: reverselogproxy
        extended_key_usage:
        - client_auth
        - server_auth
    - name: loggregator_rlp_gateway
      type: certificate
      options:
        ca: loggregator_ca
        common_name: rlp_gateway
        extended_key_usage:
        - client_auth
    - name: adapter_rlp_tls
      type: certificate
      options:
        ca: loggregator_ca
        common_name: ss-adapter-rlp
        extended_key_usage:
        - client_auth
        - server_auth
    - name: scheduler_api_tls
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: ss-scheduler
        extended_key_usage:
        - client_auth
        - server_auth
    - name: adapter_tls
      type: certificate
      options:
        ca: loggregator_ca
        common_name: ss-adapter
        extended_key_usage:
        - server_auth
        - client_auth
    - name: scheduler_client_tls
      type: certificate
      options:
        ca: loggregator_ca
        common_name: ss-scheduler
        extended_key_usage:
        - client_auth
    - name: logs_provider
      options:
        ca: loggregator_ca
        common_name: log-cache
        extended_key_usage:
        - client_auth
        - server_auth
      type: certificate
    - name: log_cache_ca
      options:
        common_name: log-cache
        is_ca: true
      type: certificate
    - name: log_cache
      options:
        alternative_names:
        - log_cache
        - log-cache
        - logcache
        ca: log_cache_ca
        common_name: log-cache
        extended_key_usage:
        - client_auth
        - server_auth
      type: certificate
    - name: log_cache_to_loggregator_agent
      options:
        ca: loggregator_ca
        common_name: log-cache
        extended_key_usage:
        - client_auth
      type: certificate
    - name: cc_logcache_tls
      type: certificate
      options:
        ca: log_cache_ca
        common_name: "api.((system_domain))"
        alternative_names:
        - "api.((system_domain))"
        - cloud-controller-ng.service.cf.internal
    - name: logcache_ssl
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: log-cache
        alternative_names:
        - log-cache.((system_domain))
        - "*.log-cache.((system_domain))"
    - name: log_cache_proxy_tls
      type: certificate
      options:
        ca: log_cache_ca
        common_name: localhost
    - name: router_ca
      type: certificate
      options:
        is_ca: true
        common_name: routerCA
    - name: router_ssl
      type: certificate
      options:
        ca: router_ca
        common_name: routerSSL
        alternative_names:
        - "((system_domain))"
        - "*.((system_domain))"
    - name: routing_api_ca
      type: certificate
      options:
        common_name: routing_api
        is_ca: true
    - name: routing_api_tls
      type: certificate
      options:
        ca: routing_api_ca
        common_name: routing-api.service.cf.internal
        extended_key_usage:
          - server_auth
    - name: routing_api_tls_client
      type: certificate
      options:
        ca: routing_api_ca
        common_name: routing-api-client
        extended_key_usage:
          - client_auth
    - name: uaa_ca
      type: certificate
      options:
        is_ca: true
        common_name: uaaCA
    - name: uaa_ssl
      type: certificate
      options:
        ca: uaa_ca
        common_name: uaa.service.cf.internal
        alternative_names:
        - uaa.service.cf.internal
    - name: uaa_login_saml
      type: certificate
      options:
        ca: uaa_ca
        common_name: uaa_login_saml
    - name: cc_tls
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: cloud-controller-ng.service.cf.internal
        extended_key_usage:
        - client_auth
        - server_auth
    - name: cc_public_tls
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: "api.((system_domain))"
        alternative_names:
        - "api.((system_domain))"
        - cloud-controller-ng.service.cf.internal
    - name: cc_bridge_tps
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: tps_watcher
        extended_key_usage:
        - client_auth
    - name: cc_bridge_cc_uploader
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: cc_uploader
        extended_key_usage:
        - client_auth
    - name: cc_bridge_cc_uploader_server
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: cc-uploader.service.cf.internal
        extended_key_usage:
        - server_auth
    - name: diego_locket_server
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: locket.service.cf.internal
        extended_key_usage:
        - server_auth
        alternative_names:
        - "*.locket.service.cf.internal"
        - locket.service.cf.internal
    - name: diego_locket_client
      type: certificate
      options:
        ca: service_cf_internal_ca
        common_name: locket client
        extended_key_usage:
        - client_auth
    - name: locket_database_password
      type: password
    - name: application_ca
      type: certificate
      options:
        common_name: appRootCA
        is_ca: true
    - name: diego_instance_identity_ca
      type: certificate
      options:
        ca: application_ca
        common_name: instanceIdentityCA
        is_ca: true
    - name: gorouter_backend_tls
      type: certificate
      options:
        ca: service_cf_internal_ca
        extended_key_usage:
        - client_auth
        common_name: gorouter_backend_tls
        alternative_names:
        - gorouter.service.cf.internal
    - name: credhub_ca
      type: certificate
      options:
        common_name: credhubServerCa
        is_ca: true
    - name: credhub_tls
      type: certificate
      options:
        ca: credhub_ca
        common_name: credhub.((system_domain))
        alternative_names:
        - credhub.service.cf.internal
        - credhub.((system_domain))
    - name: ssh_proxy_backends_tls
      type: certificate
      options:
        ca: service_cf_internal_ca
        extended_key_usage:
        - client_auth
        common_name: ssh_proxy_backends_tls
        alternative_names:
        - ssh-proxy.service.cf.internal
    - name: pxc_galera_ca
      type: certificate
      options:
        common_name: pxc_galera_ca
        is_ca: true
    - name: pxc_server_ca
      type: certificate
      options:
        common_name: pxc_server_ca
        is_ca: true
    - name: galera_server_certificate
      type: certificate
      options:
        ca: pxc_galera_ca
        common_name: galera_server_certificate
        extended_key_usage:
        - server_auth
        - client_auth
    - name: mysql_server_certificate
      type: certificate
      options:
        ca: pxc_server_ca
        common_name: sql-db.service.cf.internal

    - name: loggregator_rlp_gateway_tls
      type: certificate
      options:
        alternative_names:
        - log-stream.((system_domain))
        - log-api.service.cf.internal
        ca: service_cf_internal_ca
        common_name: log-stream.((system_domain))

    - name: loggregator_trafficcontroller_tls
      type: certificate
      options:
        alternative_names:
        - doppler.((system_domain))
        - log-api.service.cf.internal
        ca: service_cf_internal_ca
        common_name: doppler.((system_domain))

    - name: metric_scraper_ca
      type: certificate
      options:
        is_ca: true
        common_name: metricScraperCA

    - name: log_cache_metrics_tls
      type: certificate
      options:
        ca: metric_scraper_ca
        common_name: log_cache_metrics
        extended_key_usage:
        - server_auth

    - name: log_cache_cf_auth_proxy_metrics_tls
      type: certificate
      options:
        ca: metric_scraper_ca
        common_name: log_cache_cf_auth_proxy_metrics
        extended_key_usage:
        - server_auth

    - name: log_cache_gateway_metrics_tls
      type: certificate
      options:
        ca: metric_scraper_ca
        common_name: log_cache_gateway_metrics
        extended_key_usage:
        - server_auth

    - name: forwarder_agent_metrics_tls
      type: certificate
      options:
        ca: metric_scraper_ca
        common_name: forwarder_agent_metrics
        extended_key_usage:
        - server_auth

    - name: loggregator_agent_metrics_tls
      type: certificate
      options:
        ca: metric_scraper_ca
        common_name: loggregator_agent_metrics
        extended_key_usage:
        - server_auth

    - name: loggr_udp_forwarder_tls
      type: certificate
      options:
        ca: metric_scraper_ca
        common_name: loggr_udp_forwarder_metrics
        extended_key_usage:
        - server_auth

    - name: prom_scraper_scrape_tls
      type: certificate
      options:
        ca: metric_scraper_ca
        common_name: prom_scraper
        extended_key_usage:
        - client_auth

    - name: prom_scraper_metrics_tls
      type: certificate
      options:
        ca: metric_scraper_ca
        common_name: prom_scraper_metrics
        extended_key_usage:
        - server_auth

    - name: rlp_gateway_metrics_tls
      type: certificate
      options:
        ca: metric_scraper_ca
        common_name: rlp_gateway_metrics
        extended_key_usage:
        - server_auth

    releases:
    - name: binary-buildpack
      url: https://bosh.io/d/github.com/cloudfoundry/binary-buildpack-release?v=1.0.36
      version: 1.0.36
      sha1: 0269a613be68f988682bbf56504b78477965b1c4
    - name: bpm
      url: https://bosh.io/d/github.com/cloudfoundry/bpm-release?v=1.1.7
      version: 1.1.7
      sha1: e7d474c3a205fa4438ea5a60d3b59479939163aa
    - name: capi
      url: https://bosh.io/d/github.com/cloudfoundry/capi-release?v=1.91.0
      version: 1.91.0
      sha1: a1c6175eaf98e27d70562976cf82d09dec493e53
    - name: cf-networking
      url: https://bosh.io/d/github.com/cloudfoundry/cf-networking-release?v=2.28.0
      version: 2.28.0
      sha1: 683397ede8132bd343f83a4d8c85e943537151a8
    - name: cf-smoke-tests
      url: https://bosh.io/d/github.com/cloudfoundry/cf-smoke-tests-release?v=40.0.127
      version: 40.0.127
      sha1: fa710d4fefd2bb03657cf6d889298a2b9dc682b4
    - name: cf-syslog-drain
      url: https://bosh.io/d/github.com/cloudfoundry/cf-syslog-drain-release?v=10.2.11
      version: 10.2.11
      sha1: 4f4c86ff6e4ab8d398f3e9705c9e01d1cd46b896
    - name: cflinuxfs3
      url: https://bosh.io/d/github.com/cloudfoundry/cflinuxfs3-release?v=0.167.0
      version: 0.167.0
      sha1: 83ef2b1cf57a3c66d60b62828991506f71cc41f9
    - name: credhub
      url: https://bosh.io/d/github.com/pivotal-cf/credhub-release?v=2.5.11
      version: 2.5.11
      sha1: f1810c1e662a1c76f40911cffd1d159204c9a661
    - name: diego
      url: https://bosh.io/d/github.com/cloudfoundry/diego-release?v=2.44.0
      version: 2.44.0
      sha1: 53f3688ca65cd21fd4334b32c38b362bdaf4a2ca
    - name: dotnet-core-buildpack
      url: https://bosh.io/d/github.com/cloudfoundry/dotnet-core-buildpack-release?v=2.3.6
      version: 2.3.6
      sha1: 7f2b3c932b60fbe696abffc138aa28069b69059a
    - name: garden-runc
      url: https://bosh.io/d/github.com/cloudfoundry/garden-runc-release?v=1.19.10
      version: 1.19.10
      sha1: 6d3a30a5d90b0ab7bd89fddf7fa22b9a4cc08b0d
    - name: go-buildpack
      url: https://bosh.io/d/github.com/cloudfoundry/go-buildpack-release?v=1.9.7
      version: 1.9.7
      sha1: 3fbd1a0411ae1520c9166f3c0df2baa7b093f112
    - name: java-buildpack
      url: https://bosh.io/d/github.com/cloudfoundry/java-buildpack-release?v=4.26
      version: "4.26"
      sha1: 24e8c51cbf364fc38f40d0e261dd3bb663e145e3
    - name: loggregator
      url: https://bosh.io/d/github.com/cloudfoundry/loggregator-release?v=106.3.8
      version: 106.3.8
      sha1: 2a3a526ee17b8f3994e396ef81ef79cbb378358d
    - name: nats
      url: https://bosh.io/d/github.com/cloudfoundry/nats-release?v=33
      version: "33"
      sha1: 48074b227e5950fbc06694b20ae9ef7b34a0a09e
    - name: nginx-buildpack
      url: https://bosh.io/d/github.com/cloudfoundry/nginx-buildpack-release?v=1.1.5
      version: 1.1.5
      sha1: 37f34ecd9a1e120f4479990e31e87c8da85f7fdb
    - name: r-buildpack
      url: https://bosh.io/d/github.com/cloudfoundry/r-buildpack-release?v=1.1.1
      version: 1.1.1
      sha1: eb406b09e0cafb176c8ab52752da41e736b81cea
    - name: nodejs-buildpack
      url: https://bosh.io/d/github.com/cloudfoundry/nodejs-buildpack-release?v=1.7.13
      version: 1.7.13
      sha1: f6bf4499abbc5180d5ead246d2e2fa0607befd57
    - name: php-buildpack
      url: https://bosh.io/d/github.com/cloudfoundry/php-buildpack-release?v=4.4.8
      version: 4.4.8
      sha1: 7818cf2f5d11164bb600a7077a808385a06294de
    - name: pxc
      url: https://bosh.io/d/github.com/cloudfoundry-incubator/pxc-release?v=0.24.0
      version: 0.24.0
      sha1: f2a479ab18e7327d67705bb54111b9a0f8ffcba3
    - name: python-buildpack
      url: https://bosh.io/d/github.com/cloudfoundry/python-buildpack-release?v=1.7.8
      version: 1.7.8
      sha1: 27667ee2486e2baa3e7ef9a6d8963a6841d19204
    - name: routing
      url: https://bosh.io/d/github.com/cloudfoundry/routing-release?v=0.198.0
      version: 0.198.0
      sha1: 90de2a9bcc5fea7ece8da8b8675949c1772e33be
    - name: ruby-buildpack
      url: https://bosh.io/d/github.com/cloudfoundry/ruby-buildpack-release?v=1.8.11
      version: 1.8.11
      sha1: 8ef27c470dcaef844ec606d0b200947befd566da
    - name: silk
      url: https://bosh.io/d/github.com/cloudfoundry/silk-release?v=2.28.0
      version: 2.28.0
      sha1: 43bad37b29a5eb16ac0ce79bc6045c41479d761b
    - name: staticfile-buildpack
      url: https://bosh.io/d/github.com/cloudfoundry/staticfile-buildpack-release?v=1.5.4
      version: 1.5.4
      sha1: 04e887342a28d58bb8ba927af381d9d4d9c70ae2
    - name: statsd-injector
      url: https://bosh.io/d/github.com/cloudfoundry/statsd-injector-release?v=1.11.15
      version: 1.11.15
      sha1: a0a2d33c6ab7d8fec8c017ea6f2c5a344af1407c
    - name: uaa
      url: https://bosh.io/d/github.com/cloudfoundry/uaa-release?v=74.15.0
      version: 74.15.0
      sha1: 27cf2309644d7c3b6b41c5ee32775b63707062a9
    - name: loggregator-agent
      url: https://bosh.io/d/github.com/cloudfoundry/loggregator-agent-release?v=5.3.7
      version: 5.3.7
      sha1: de7eaea3a286489fd979bddabdc5949ae21c1277
    - name: log-cache
      url: https://bosh.io/d/github.com/cloudfoundry/log-cache-release?v=2.6.8
      version: 2.6.8
      sha1: 3a5bcd3162387cd2fd2deb6e15a29bfff398cdae
    - name: bosh-dns-aliases
      url: https://bosh.io/d/github.com/cloudfoundry/bosh-dns-aliases-release?v=0.0.3
      version: 0.0.3
      sha1: b0d0a0350ed87f1ded58b2ebb469acea0e026ccc
    - name: cf-cli
      url: https://bosh.io/d/github.com/bosh-packages/cf-cli-release?v=1.25.0
      version: 1.25.0
      sha1: 6ef0373f50061d3483b451820af87358b7591d80
    stemcells:
    - alias: default
      os: ubuntu-xenial
      version: "621.59"
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: database-config-files
  namespace: default
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ssl.cnf: |
    [mysqld]
    ssl-ca=/etc/mysql/tls/certs/ca
    ssl-cert=/etc/mysql/tls/certs/certificate
    ssl-key=/etc/mysql/tls/certs/private_key
  node.cnf: |
    [mysqld]
    datadir=/var/lib/mysql
    default_storage_engine=InnoDB
    binlog_format=ROW
    innodb_flush_log_at_trx_commit  = 0
    innodb_flush_method             = O_DIRECT
    innodb_file_per_table           = 1
    innodb_autoinc_lock_mode=2
    bind_address = 0.0.0.0
    wsrep_slave_threads=2
    wsrep_cluster_address=gcomm://
    wsrep_provider=/usr/lib/galera3/libgalera_smm.so
    wsrep_sst_method=xtrabackup-v2
  charset.cnf: |-
    [client]
    default_character_set           = utf8

    [mysql]
    default_character_set           = utf8

    [mysqld]
    # Ignore the client information and use the default server character set.
    character_set_client_handshake  = false

    character_set_server            = utf8
    collation_server                = utf8_unicode_ci

    [mysqld_safe]
    default_character_set           = utf8
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: database-startup-scripts
  namespace: default
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  entrypoint.sh: |
    #!/bin/bash

    #    Copyright The Helm Authors.
    #
    #    Licensed under the Apache License, Version 2.0 (the "License");
    #    you may not use this file except in compliance with the License.
    #    You may obtain a copy of the License at
    #
    #        http://www.apache.org/licenses/LICENSE-2.0
    #
    #    Unless required by applicable law or agreed to in writing, software
    #    distributed under the License is distributed on an "AS IS" BASIS,
    #    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    #    See the License for the specific language governing permissions and
    #    limitations under the License.
    #
    # This file was obtained from:
    # https://github.com/helm/charts/blob/7ccc9f99d7ab6b9554624985d9c9b9723b7253c0/stable/percona-xtradb-cluster/files/entrypoint.sh

    set -e

    if [[ -n "${DEBUG}" ]]; then
        set -x
    fi

    # shellcheck disable=SC1091
    . /startup-scripts/functions.sh

    ipaddr=$(hostname -i | awk ' { print $1 } ')
    hostname=$(hostname)
    echo "I AM $hostname - $ipaddr"

    # if command starts with an option, prepend mysqld
    if [ "${1:0:1}" = '-' ]; then
        CMDARG=( "$@" )
    fi

    cluster_join=$(resolveip -s "${K8S_SERVICE_NAME}" || echo "")
    if [[ -z "${cluster_join}" ]]; then
        echo "I am the Primary Node"
        init_mysql
        write_password_file
        exec mysqld --user=mysql --wsrep_cluster_name="$SHORT_CLUSTER_NAME" --wsrep_node_name="$hostname" \
        --wsrep_cluster_address=gcomm:// --wsrep_sst_method=xtrabackup-v2 \
        --wsrep_sst_auth="xtrabackup:$XTRABACKUP_PASSWORD" \
        --wsrep_node_address="$ipaddr" --pxc_strict_mode="$PXC_STRICT_MODE" "${CMDARG[@]}"
    else
        echo "I am not the Primary Node"
        chown -R mysql:mysql /var/lib/mysql || true # default is root:root 777
        touch /var/log/mysqld.log
        chown mysql:mysql /var/log/mysqld.log
        write_password_file
        exec mysqld --user=mysql --wsrep_cluster_name="$SHORT_CLUSTER_NAME" --wsrep_node_name="$hostname" \
        --wsrep_cluster_address="gcomm://$cluster_join" --wsrep_sst_method=xtrabackup-v2 \
        --wsrep_sst_auth="xtrabackup:$XTRABACKUP_PASSWORD" \
        --wsrep_node_address="$ipaddr" --pxc_strict_mode="$PXC_STRICT_MODE" "${CMDARG[@]}"
    fi
  functions.sh: |
    #!/bin/bash

    #    Copyright The Helm Authors.
    #
    #    Licensed under the Apache License, Version 2.0 (the "License");
    #    you may not use this file except in compliance with the License.
    #    You may obtain a copy of the License at
    #
    #        http://www.apache.org/licenses/LICENSE-2.0
    #
    #    Unless required by applicable law or agreed to in writing, software
    #    distributed under the License is distributed on an "AS IS" BASIS,
    #    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    #    See the License for the specific language governing permissions and
    #    limitations under the License.
    #
    # This file was obtained from:
    # https://github.com/helm/charts/blob/7ccc9f99d7ab6b9554624985d9c9b9723b7253c0/stable/percona-xtradb-cluster/files/functions.sh

    write_password_file() {
        if [[ -n "${MYSQL_ROOT_PASSWORD}" ]]; then
            cat <<EOF > /root/.my.cnf
            [client]
            user=root
            password=${MYSQL_ROOT_PASSWORD}
    EOF
        fi
    }

    init_mysql() {
        SENTINEL=INIT_MYSQL_DONE
        DATADIR=/var/lib/mysql
        # if we have CLUSTER_JOIN - then we do not need to perform datadir initialize
        # the data will be copied from another node
        if [ ! -e "$DATADIR/$SENTINEL" ]; then
            echo "Removing pending files in $DATADIR, because sentinel was not reached"
            rm -rf "${DATADIR:?}"/*
            if [ -z "$MYSQL_ROOT_PASSWORD" ] && [ -z "$MYSQL_ALLOW_EMPTY_PASSWORD" ] && [ -z "$MYSQL_RANDOM_ROOT_PASSWORD" ] && [ -z "$MYSQL_ROOT_PASSWORD_FILE" ]; then
                echo >&2 'error: database is uninitialized and password option is not specified '
                echo >&2 '  You need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ROOT_PASSWORD_FILE,  MYSQL_ALLOW_EMPTY_PASSWORD or MYSQL_RANDOM_ROOT_PASSWORD'
                exit 1
            fi

            if [ -n "$MYSQL_ROOT_PASSWORD_FILE" ] && [ -z "$MYSQL_ROOT_PASSWORD" ]; then
                MYSQL_ROOT_PASSWORD=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
            fi
            mkdir -p "$DATADIR"

            echo "Running --initialize-insecure on $DATADIR"
            ls -lah $DATADIR
            if [ "$PERCONA_MAJOR" = "5.6" ]; then
                mysql_install_db --user=mysql --datadir="$DATADIR"
            else
                mysqld --user=mysql --datadir="$DATADIR" --initialize-insecure
            fi
            chown -R mysql:mysql "$DATADIR" || true # default is root:root 777
            if [ -f /var/log/mysqld.log ]; then
                chown mysql:mysql /var/log/mysqld.log
            fi
            echo 'Finished --initialize-insecure'

            mysqld --user=mysql --datadir="$DATADIR" --skip-networking &
            pid="$!"

            mysql=( mysql "--protocol=socket" -uroot )

            for i in {30..0}; do
                if echo 'SELECT 1' | "${mysql[@]}" &> /dev/null; then
                    break
                fi
                echo 'MySQL init process in progress...'
                sleep 1
            done

            if [ "$i" = 0 ]; then
                echo >&2 'MySQL init process failed.'
                exit 1
            fi

            # sed is for https://bugs.mysql.com/bug.php?id=20545
            mysql_tzinfo_to_sql /usr/share/zoneinfo | sed 's/Local time zone must be set--see zic manual page/FCTY/' | "${mysql[@]}" mysql
            "${mysql[@]}" <<-EOSQL
            -- What's done in this file shouldn't be replicated
            --  or products like mysql-fabric won't work
            SET @@SESSION.SQL_LOG_BIN=0;
            CREATE USER 'root'@'${ALLOW_ROOT_FROM}' IDENTIFIED BY '${MYSQL_ROOT_PASSWORD}' ;
            GRANT ALL ON *.* TO 'root'@'${ALLOW_ROOT_FROM}' WITH GRANT OPTION ;
            GRANT ALL ON *.* TO 'root'@'localhost' WITH GRANT OPTION ;
            CREATE USER 'xtrabackup'@'localhost' IDENTIFIED BY '$XTRABACKUP_PASSWORD';
            GRANT RELOAD,PROCESS,LOCK TABLES,REPLICATION CLIENT ON *.* TO 'xtrabackup'@'localhost';
            GRANT REPLICATION CLIENT ON *.* TO monitor@'%' IDENTIFIED BY 'monitor';
            GRANT PROCESS ON *.* TO monitor@localhost IDENTIFIED BY 'monitor';
            CREATE USER 'mysql'@'localhost' IDENTIFIED BY '' ;
            DROP DATABASE IF EXISTS test ;
            FLUSH PRIVILEGES ;
    EOSQL

            if [ "$PERCONA_MAJOR" = "5.6" ]; then
                echo "SET PASSWORD FOR 'root'@'localhost' = PASSWORD('${MYSQL_ROOT_PASSWORD}'); FLUSH PRIVILEGES;" | "${mysql[@]}"
            else
                echo "ALTER USER 'root'@'localhost' IDENTIFIED BY '${MYSQL_ROOT_PASSWORD}'; FLUSH PRIVILEGES;" | "${mysql[@]}"
            fi

            if [ -n "$MYSQL_ROOT_PASSWORD" ]; then
                mysql+=( -p"${MYSQL_ROOT_PASSWORD}" )
            fi

            if [ "$MYSQL_DATABASE" ]; then
                echo "CREATE DATABASE IF NOT EXISTS \`$MYSQL_DATABASE\` ;" | "${mysql[@]}"
                mysql+=( "$MYSQL_DATABASE" )
            fi

            if [ "$MYSQL_USER" ] && [ "$MYSQL_PASSWORD" ]; then
                echo "CREATE USER '""$MYSQL_USER""'@'%' IDENTIFIED BY '""$MYSQL_PASSWORD""' ;" | "${mysql[@]}"

                if [ "$MYSQL_DATABASE" ]; then
                    echo "GRANT ALL ON \`""$MYSQL_DATABASE""\`.* TO '""$MYSQL_USER""'@'%' ;" | "${mysql[@]}"
                fi

                echo 'FLUSH PRIVILEGES ;' | "${mysql[@]}"
            fi

            if [ -n "$MYSQL_ONETIME_PASSWORD" ]; then
                "${mysql[@]}" <<-EOSQL
                ALTER USER 'root'@'%' PASSWORD EXPIRE;
    EOSQL
            fi
            if ! kill -s TERM "$pid" || ! wait "$pid"; then
                echo >&2 'MySQL init process failed.'
                exit 1
            fi

            echo
            echo 'MySQL init process done. Ready for start up.'
            echo
            touch "$DATADIR/$SENTINEL"
        fi
    }
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-move-auctioneer
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    - type: remove
      path: /instance_groups/name=scheduler/jobs/name=auctioneer
    - type: replace
      path: /instance_groups/name=scheduler:after
      value:
        name: auctioneer
        instances: 2
        stemcell: default
        update:
          serial: true
        jobs:
        -
          name: auctioneer
          properties:
            bpm:
              enabled: true
            diego:
              auctioneer:
                bbs:
                  ca_cert: ((diego_bbs_client.ca))
                  client_cert: ((diego_bbs_client.certificate))
                  client_key: ((diego_bbs_client.private_key))
                ca_cert: ((diego_auctioneer_server.ca))
                rep:
                  ca_cert: ((diego_rep_client.ca))
                  client_cert: ((diego_rep_client.certificate))
                  client_key: ((diego_rep_client.private_key))
                  require_tls: true
                server_cert: ((diego_auctioneer_server.certificate))
                server_key: ((diego_auctioneer_server.private_key))
                skip_consul_lock: true
            enable_consul_service_registration: false
            logging:
              format:
                timestamp: rfc3339
            loggregator:
              ca_cert: ((loggregator_tls_agent.ca))
              cert: ((loggregator_tls_agent.certificate))
              key: ((loggregator_tls_agent.private_key))
              use_v2_api: true
          release: diego
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-move-log-cache
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: "- type: remove\n  path: /instance_groups/name=doppler/jobs/name=log-cache\n- type: remove\n  path: /instance_groups/name=doppler/jobs/name=log-cache-gateway\n- type: remove\n  path: /instance_groups/name=doppler/jobs/name=log-cache-nozzle\n- type: remove\n  path: /instance_groups/name=doppler/jobs/name=log-cache-cf-auth-proxy\n- type: remove\n  path: /instance_groups/name=doppler/jobs/name=route_registrar\n- type: replace\n  path: /instance_groups/name=log-api:after\n  value:\n    name: log-cache\n    instances: 1\n    stemcell: default\n    update:\n      serial: false\n    jobs:\n    -\n      name: log-cache-cf-auth-proxy\n      properties:\n        cc:\n          ca_cert: ((cc_tls.ca))\n          common_name: cloud-controller-ng.service.cf.internal\n        external_cert: ((logcache_ssl.certificate))\n        external_key: ((logcache_ssl.private_key))\n        metrics:\n          ca_cert: ((log_cache_cf_auth_proxy_metrics_tls.ca))\n          cert: ((log_cache_cf_auth_proxy_metrics_tls.certificate))\n          key: ((log_cache_cf_auth_proxy_metrics_tls.private_key))\n          server_name: log_cache_cf_auth_proxy_metrics\n        proxy_ca_cert: ((log_cache.ca))\n        proxy_port: 8083\n        uaa:\n          ca_cert: ((uaa_ssl.ca))\n          client_id: doppler\n          client_secret: ((uaa_clients_doppler_secret))\n          internal_addr: https://uaa.service.cf.internal:8443\n      release: log-cache\n      \n    -\n      name: log-cache-gateway\n      properties:\n        gateway_addr: localhost:8081\n        metrics:\n          ca_cert: ((log_cache_gateway_metrics_tls.ca))\n          cert: ((log_cache_gateway_metrics_tls.certificate))\n          key: ((log_cache_gateway_metrics_tls.private_key))\n          server_name: log_cache_gateway_metrics\n        proxy_cert: ((log_cache_proxy_tls.certificate))\n        proxy_key: ((log_cache_proxy_tls.private_key))\n      release: log-cache\n      \n    -\n      name: log-cache\n      properties:\n        health_addr: localhost:6060\n        metrics:\n          ca_cert: ((log_cache_metrics_tls.ca))\n          cert: ((log_cache_metrics_tls.certificate))\n          key: ((log_cache_metrics_tls.private_key))\n          server_name: log_cache_metrics\n        tls:\n          ca_cert: ((log_cache.ca))\n          cert: ((log_cache.certificate))\n          key: ((log_cache.private_key))\n      provides:\n        log-cache:\n          shared: true\n      release: log-cache\n      \n    -\n      consumes:\n        reverse_log_proxy:\n          from: reverse_log_proxy\n      name: log-cache-nozzle\n      properties:\n        logs_provider:\n          tls:\n            ca_cert: ((logs_provider.ca))\n            cert: ((logs_provider.certificate))\n            key: ((logs_provider.private_key))\n      release: log-cache\n      \n    -\n      name: route_registrar\n      properties:\n        route_registrar:\n          routes:\n          - name: log-cache-reverse-proxy\n            port: 8083\n            registration_interval: 20s\n            server_cert_domain_san: log-cache.((system_domain))\n            tls_port: 8083\n            uris:\n            - log-cache.((system_domain))\n            - '*.log-cache.((system_domain))'\n      release: routing"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-move-routing-api
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: ""
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-set-suse-buildpacks
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    - type: replace
      path: /releases/-
      value:
        name: sle15
        version: "10.93"
        url: docker.io/cfcontainerization
        stemcell: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/-
      value:
        name: suse-staticfile-buildpack
        version: "1.5.6.1"
        url: registry.suse.com/cap-staging
        stemcell: {"os":"SLE_15_SP1","version":"26.7-7.0.0_374.gb8e8e6af"}

    - type: replace
      path: /instance_groups/name=api/jobs/-
      value:
        name: suse-staticfile-buildpack
        release: suse-staticfile-buildpack
        properties:
          quarks:
            bpm:
              processes: []
    - type: replace
      path: /releases/-
      value:
        name: suse-java-buildpack
        version: "4.31.0.1"
        url: registry.suse.com/cap-staging
        stemcell: {"os":"SLE_15_SP1","version":"26.7-7.0.0_374.gb8e8e6af"}

    - type: replace
      path: /instance_groups/name=api/jobs/-
      value:
        name: suse-java-buildpack
        release: suse-java-buildpack
        properties:
          quarks:
            bpm:
              processes: []
    - type: replace
      path: /releases/-
      value:
        name: suse-ruby-buildpack
        version: "1.8.19.1"
        url: registry.suse.com/cap-staging
        stemcell: {"os":"SLE_15_SP1","version":"26.7-7.0.0_374.gb8e8e6af"}

    - type: replace
      path: /instance_groups/name=api/jobs/-
      value:
        name: suse-ruby-buildpack
        release: suse-ruby-buildpack
        properties:
          quarks:
            bpm:
              processes: []
    - type: replace
      path: /releases/-
      value:
        name: suse-dotnet-core-buildpack
        version: "2.3.9.1"
        url: registry.suse.com/cap-staging
        stemcell: {"os":"SLE_15_SP1","version":"26.7-7.0.0_374.gb8e8e6af"}

    - type: replace
      path: /instance_groups/name=api/jobs/-
      value:
        name: suse-dotnet-core-buildpack
        release: suse-dotnet-core-buildpack
        properties:
          quarks:
            bpm:
              processes: []
    - type: replace
      path: /releases/-
      value:
        name: suse-nodejs-buildpack
        version: "1.7.18.1"
        url: registry.suse.com/cap-staging
        stemcell: {"os":"SLE_15_SP1","version":"26.7-7.0.0_374.gb8e8e6af"}

    - type: replace
      path: /instance_groups/name=api/jobs/-
      value:
        name: suse-nodejs-buildpack
        release: suse-nodejs-buildpack
        properties:
          quarks:
            bpm:
              processes: []
    - type: replace
      path: /releases/-
      value:
        name: suse-go-buildpack
        version: "1.9.13.1"
        url: registry.suse.com/cap-staging
        stemcell: {"os":"SLE_15_SP1","version":"26.7-7.0.0_374.gb8e8e6af"}

    - type: replace
      path: /instance_groups/name=api/jobs/-
      value:
        name: suse-go-buildpack
        release: suse-go-buildpack
        properties:
          quarks:
            bpm:
              processes: []
    - type: replace
      path: /releases/-
      value:
        name: suse-python-buildpack
        version: "1.7.14.1"
        url: registry.suse.com/cap-staging
        stemcell: {"os":"SLE_15_SP1","version":"26.7-7.0.0_374.gb8e8e6af"}

    - type: replace
      path: /instance_groups/name=api/jobs/-
      value:
        name: suse-python-buildpack
        release: suse-python-buildpack
        properties:
          quarks:
            bpm:
              processes: []
    - type: replace
      path: /releases/-
      value:
        name: suse-php-buildpack
        version: "4.4.14.1"
        url: registry.suse.com/cap-staging
        stemcell: {"os":"SLE_15_SP1","version":"26.7-7.0.0_374.gb8e8e6af"}

    - type: replace
      path: /instance_groups/name=api/jobs/-
      value:
        name: suse-php-buildpack
        release: suse-php-buildpack
        properties:
          quarks:
            bpm:
              processes: []
    - type: replace
      path: /releases/-
      value:
        name: suse-nginx-buildpack
        version: "1.1.7.1"
        url: registry.suse.com/cap-staging
        stemcell: {"os":"SLE_15_SP1","version":"26.7-7.0.0_374.gb8e8e6af"}

    - type: replace
      path: /instance_groups/name=api/jobs/-
      value:
        name: suse-nginx-buildpack
        release: suse-nginx-buildpack
        properties:
          quarks:
            bpm:
              processes: []
    - type: replace
      path: /releases/-
      value:
        name: suse-binary-buildpack
        version: "1.0.36.1"
        url: registry.suse.com/cap-staging
        stemcell: {"os":"SLE_15_SP1","version":"26.7-7.0.0_374.gb8e8e6af"}

    - type: replace
      path: /instance_groups/name=api/jobs/-
      value:
        name: suse-binary-buildpack
        release: suse-binary-buildpack
        properties:
          quarks:
            bpm:
              processes: []
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-acceptance-tests
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: ""
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-adapter
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    # Add quarks properties.
    - type: replace
      path: /instance_groups/name=adapter/jobs/name=adapter/properties/quarks?
      value:
        ports:
        - name: adapter
          protocol: TCP
          internal: 4443
        run:
          healthcheck:
            adapter:
              readiness:
                # The healthcheck port is bound to localhost only
                exec:
                  command: ["curl", "--fail", "--head", "--silent", "http://localhost:8080/health"]
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-api
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: "# TODO(jandubois) Starting with v2.3.0 enabling eirini will no longer automatically switch to the suse stack.\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/default_stack?\n  value: sle15\n- type: replace\n  path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/cc/default_stack?\n  value: sle15\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/cc/default_stack?\n  value: sle15\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cc_deployment_updater/properties/cc/default_stack?\n  value: sle15\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/stacks/-\n  value:\n    name: sle15\n    description: \"SUSE Linux Enterprise-based filesystem\"\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/diego/lifecycle_bundles?\n  value:\n    buildpack/sle15: buildpack_app_lifecycle/buildpack_app_lifecycle.tgz\n    buildpack/cflinuxfs3: buildpack_app_lifecycle/buildpack_app_lifecycle.tgz\n    docker: docker_app_lifecycle/docker_app_lifecycle.tgz\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/install_buildpacks\n  value:\n  ## Order is important here\n  - name: staticfile_buildpack\n    file: /var/vcap/data/shared-packages/staticfile-buildpack/packages/staticfile-buildpack-cflinuxfs3/staticfile_buildpack-cflinuxfs3-v1.5.4.zip\n  - name: java_buildpack\n    file: /var/vcap/data/shared-packages/java-buildpack/packages/java-buildpack-cflinuxfs3/java-buildpack-cflinuxfs3-v4.26.zip\n  - name: ruby_buildpack\n    file: /var/vcap/data/shared-packages/ruby-buildpack/packages/ruby-buildpack-cflinuxfs3/ruby_buildpack-cflinuxfs3-v1.8.11.zip\n  - name: dotnet-core_buildpack\n    file: /var/vcap/data/shared-packages/dotnet-core-buildpack/packages/dotnet-core-buildpack-cflinuxfs3/dotnet-core_buildpack-cflinuxfs3-v2.3.6.zip\n  - name: nodejs_buildpack\n    file: /var/vcap/data/shared-packages/nodejs-buildpack/packages/nodejs-buildpack-cflinuxfs3/nodejs_buildpack-cflinuxfs3-v1.7.13.zip\n  - name: go_buildpack\n    file: /var/vcap/data/shared-packages/go-buildpack/packages/go-buildpack-cflinuxfs3/go_buildpack-cflinuxfs3-v1.9.7.zip\n  - name: python_buildpack\n    file: /var/vcap/data/shared-packages/python-buildpack/packages/python-buildpack-cflinuxfs3/python_buildpack-cflinuxfs3-v1.7.8.zip\n  - name: php_buildpack\n    file: /var/vcap/data/shared-packages/php-buildpack/packages/php-buildpack-cflinuxfs3/php_buildpack-cflinuxfs3-v4.4.8.zip\n  - name: nginx_buildpack\n    file: /var/vcap/data/shared-packages/nginx-buildpack/packages/nginx-buildpack-cflinuxfs3/nginx_buildpack-cflinuxfs3-v1.1.5.zip\n  - name: r_buildpack\n    file: /var/vcap/data/shared-packages/r-buildpack/packages/r-buildpack-cflinuxfs3/r_buildpack-cflinuxfs3-v1.1.1.zip\n  - name: binary_buildpack\n    file: /var/vcap/data/shared-packages/binary-buildpack/packages/binary-buildpack-cflinuxfs3/binary_buildpack-cflinuxfs3-v1.0.36.zip\n  - name: staticfile_buildpack\n    file: /var/vcap/data/shared-packages/suse-staticfile-buildpack/packages/staticfile-buildpack-sle15/staticfile-buildpack-sle15-v1.5.6.1-2.1-a74620df.zip\n  - name: java_buildpack\n    file: /var/vcap/data/shared-packages/suse-java-buildpack/packages/java-buildpack-sle15/java-buildpack-sle15-v4.31.0.1-a4991aaa.zip\n  - name: ruby_buildpack\n    file: /var/vcap/data/shared-packages/suse-ruby-buildpack/packages/ruby-buildpack-sle15/ruby-buildpack-sle15-v1.8.19.1-1.1-63c0bb58.zip\n  - name: dotnet-core_buildpack\n    file: /var/vcap/data/shared-packages/suse-dotnet-core-buildpack/packages/dotnet-core-buildpack-sle15/dotnet-core-buildpack-sle15-v2.3.9.1-1.1-e74bd89e.zip\n  - name: nodejs_buildpack\n    file: /var/vcap/data/shared-packages/suse-nodejs-buildpack/packages/nodejs-buildpack-sle15/nodejs-buildpack-sle15-v1.7.18.1-1.1-d2f6559e.zip\n  - name: go_buildpack\n    file: /var/vcap/data/shared-packages/suse-go-buildpack/packages/go-buildpack-sle15/go-buildpack-sle15-v1.9.13.1-1.1-826f794c.zip\n  - name: python_buildpack\n    file: /var/vcap/data/shared-packages/suse-python-buildpack/packages/python-buildpack-sle15/python-buildpack-sle15-v1.7.14.1-1.1-a828fa62.zip\n  - name: php_buildpack\n    file: /var/vcap/data/shared-packages/suse-php-buildpack/packages/php-buildpack-sle15/php-buildpack-sle15-v4.4.14.1-1.1-bf878677.zip\n  - name: nginx_buildpack\n    file: /var/vcap/data/shared-packages/suse-nginx-buildpack/packages/nginx-buildpack-sle15/nginx-buildpack-sle15-v1.1.7.1-1.1-fbf90d1f.zip\n  - name: binary_buildpack\n    file: /var/vcap/data/shared-packages/suse-binary-buildpack/packages/binary-buildpack-sle15/binary-buildpack-sle15-v1.0.36.1-1.1-37ec2cbf.zip\n\n# These operations set the default values explicitly in order to get the rotate-cc-database-key\n# errand working. Without them, the rotate-cc-database-key errand is not able to resolve the CC BOSH\n# link correctly.\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/ccdb/max_connections?\n  value: 25\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/ccdb/pool_timeout?\n  value: 10\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/ccdb/ssl_verify_hostname?\n  value: true\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/ccdb/read_timeout?\n  value: 3600\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/ccdb/connection_validation_timeout?\n  value: 3600\n- type: replace\n  path: /variables/-\n  value:\n    name: ccdb_key_label_encryption_key_0\n    type: password\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/database_encryption?\n  value: &encryption_info\n    keys:\n      \"encryption_key_0\": \"((ccdb_key_label_encryption_key_0))\"\n    current_key_label: \"encryption_key_0\"\n- type: replace\n  path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/cc/database_encryption?\n  value: *encryption_info\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/cc/database_encryption?\n  value: *encryption_info\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cc_deployment_updater/properties/cc/database_encryption?\n  value: *encryption_info\n# XXX encryption_info is also used by cc_route_syncer (see CF-K8s-Networking)\n#- type: replace\n#  path: /instance_groups/name=?????/jobs/name=cc_route_syncer/properties/cc/database_encryption?\n#  value: *encryption_info\n\n# core_file_pattern should be disabled as CC is not running on a VM.\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/core_file_pattern?\n  value: false\n\n# Disable tuning /proc/sys kernel parameters as file_server is running on a container.\n- type: replace\n  path: /instance_groups/name=api/jobs/name=file_server/properties/set_kernel_parameters?\n  value: false\n\n# We don't have a /var/vcap/job/*/packages directory, so we point to all the packages.\n- type: replace\n  path: /instance_groups/name=api/jobs/name=file_server/properties/diego?/file_server/static_directory\n  value: \"/var/vcap/packages/\"\n\n# Enable volume services\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/volume_services_enabled?\n  value: true\n\n# Add empty BPM processes to buildpacks.\n- type: replace\n  path: /instance_groups/name=api/jobs/name=binary-buildpack/properties?/quarks/bpm/processes\n  value: []\n- type: replace\n  path: /instance_groups/name=api/jobs/name=dotnet-core-buildpack/properties?/quarks/bpm/processes\n  value: []\n- type: replace\n  path: /instance_groups/name=api/jobs/name=go-buildpack/properties?/quarks/bpm/processes\n  value: []\n- type: replace\n  path: /instance_groups/name=api/jobs/name=java-buildpack/properties?/quarks/bpm/processes\n  value: []\n- type: replace\n  path: /instance_groups/name=api/jobs/name=nodejs-buildpack/properties?/quarks/bpm/processes\n  value: []\n- type: replace\n  path: /instance_groups/name=api/jobs/name=nginx-buildpack/properties?/quarks/bpm/processes\n  value: []\n- type: replace\n  path: /instance_groups/name=api/jobs/name=r-buildpack/properties?/quarks/bpm/processes\n  value: []\n- type: replace\n  path: /instance_groups/name=api/jobs/name=php-buildpack/properties?/quarks/bpm/processes\n  value: []\n- type: replace\n  path: /instance_groups/name=api/jobs/name=python-buildpack/properties?/quarks/bpm/processes\n  value: []\n- type: replace\n  path: /instance_groups/name=api/jobs/name=ruby-buildpack/properties?/quarks/bpm/processes\n  value: []\n- type: replace\n  path: /instance_groups/name=api/jobs/name=staticfile-buildpack/properties?/quarks/bpm/processes\n  value: []\n\n# Add quarks properties for cloud_controller_ng.\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/quarks?\n  value:\n    ports:\n    - name: api\n      protocol: TCP\n      internal: 9022\n    - name: api-tls\n      protocol: TCP\n      internal: 9023\n    - name: api-mutual-tls\n      protocol: TCP\n      internal: 9024\n    run:\n      healthcheck:\n        ccng_monit_http_healthcheck:\n          readiness:\n            # This job exists just to healthcheck cloud_controller_ng; we're\n            # already doing that separately.\n            exec:\n              command: [/bin/true]\n          liveness:\n            exec:\n              command:\n              - /usr/bin/pgrep\n              - --full\n              - --exact\n              - bash /var/vcap/jobs/cloud_controller_ng/bin/ccng_monit_http_healthcheck\n        cloud_controller_ng:\n          readiness: &cloud_controller_ng_readiness\n            exec:\n              command:\n              - curl\n              - --fail\n              - --head\n              - --silent\n              - --unix-socket\n              - /var/vcap/data/cloud_controller_ng/cloud_controller.sock\n              - http:/healthz\n          # We don't want a liveness probe here as we do migration here, and we\n          # do not want to interrupt that.  We may want to consider using a\n          # startupProbe in the future (once that feature stabilizes).\n          liveness: ~\n        local_worker_1:\n          readiness: &cc_local_worker_readiness\n            exec:\n              command: [/usr/bin/pgrep, --full, cc_api_worker]\n        local_worker_2:\n          readiness: *cc_local_worker_readiness\n        nginx:\n          readiness:\n            httpGet:\n              httpHeaders:\n              - name: Host\n                value: api\n              path: /healthz\n              port: 9024\n              scheme: HTTPS\n          liveness:\n            exec:\n              command: [/usr/bin/pgrep, --full, \"nginx: master process\"]\n    post_start:\n      condition:\n        exec:\n          command: [\"curl\", \"--fail\", \"--head\", \"--silent\", \"http://127.0.0.1:9022/healthz\"]\n\n# Add quarks properties for cc_uploader.\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cc_uploader/properties/quarks?\n  value:\n    ports:\n    - name: http\n      protocol: TCP\n      internal: 9090\n    - name: https\n      protocol: TCP\n      internal: 9091\n    run:\n      healthcheck:\n        cc_uploader:\n          readiness:\n            # cc-uploader does not have a health check endpoint; just use a TCP\n            # socket.\n            tcpSocket:\n              port: 9091\n\n# Add quarks properties for file_server.\n- type: replace\n  path: /instance_groups/name=api/jobs/name=file_server/properties/quarks?\n  value:\n    ports:\n    - name: file-server\n      protocol: TCP\n      internal: &file-server-port 8080\n    run:\n      healthcheck:\n        file_server:\n          readiness:\n            httpGet:\n              path: /v1/static/file_server/bin/file-server\n              port: *file-server-port\n\n# Add quarks properties for statsd_injector.\n- type: replace\n  path: /instance_groups/name=api/jobs/name=statsd_injector/properties/quarks?/run/healthcheck/statsd_injector/readiness/exec/command\n  value:\n  - /bin/sh\n  - -c\n  - ss -nlu src localhost:8125 | grep :8125\n\n# Add quarks properties for policy-server.\n- type: replace\n  path: /instance_groups/name=api/jobs/name=policy-server/properties/quarks?\n  value:\n    ports:\n    - name: policy-server\n      protocol: TCP\n      internal: 4002\n    run:\n      healthcheck:\n        policy-server:\n          readiness:\n            httpGet:\n              port: 4002\n              scheme: HTTPS\n    post_start:\n      condition:\n        exec:\n          # policy-server doesn't support HTTP HEAD requests\n          command: [\"curl\", \"--insecure\", \"--fail\", \"--silent\", \"https://localhost:4002/\"]\n\n# Add quarks properties for policy-server-internal.\n- type: replace\n  path: /instance_groups/name=api/jobs/name=policy-server-internal/properties/quarks?\n  value:\n    run:\n      healthcheck:\n        policy-server-internal:\n          readiness: &policy_server_internal_readiness\n            httpGet:\n              port: 31946\n    post_start:\n      condition:\n        exec:\n          # policy-server-internal doesn't support HTTP HEAD requests\n          command: [\"curl\", \"--fail\", \"--silent\", \"http://localhost:31946/\"]\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=route_registrar/properties/quarks?/run/healthcheck/route_registrar\n  value:\n    readiness: ~\n      # The route registrar doesn't expose anything to indicate if the\n      # routes are healthy\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=loggr-udp-forwarder/properties/quarks?/run/healthcheck/loggr-udp-forwarder/readiness/exec/command\n  value: [\"sh\", \"-c\", \"ss -nlu sport = 3457 | grep :3457\"]\n- type: replace\n  path: /instance_groups/name=api/jobs/name=binary-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"binary-buildpack\"\n    buildpack=\"binary-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=binary-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"binary-buildpack\"\n    job=\"binary-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cc_uploader/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n    target=\"/var/vcap/all-releases/jobs-src/capi/cc_uploader/templates/pre-start.erb\"\n    sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n      if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch, re-patching\"\n    fi\n    \n    # Remove sysctl calls as we are running in containers.\n    # cc_uploader_ctl in https://github.com/cloudfoundry/capi-release/blob/master/jobs/cc_uploader/templates/cc_uploader_ctl.erb#L26\n    # also skips setting those parameters.\n    patch --verbose \"${target}\" <<'EOT'\n    @@ -6,6 +6,3 @@\n         /var/vcap/jobs/bosh-dns/bin/wait\n       fi\n     fi\n    -\n    -sysctl -e -w net.ipv4.tcp_fin_timeout=10\n    -sysctl -e -w net.ipv4.tcp_tw_reuse=1\n    EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\"\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/quarks?/pre_render_scripts/bpm/-\n  value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n    target=\"/var/vcap/all-releases/jobs-src/capi/cloud_controller_ng/templates/bpm.yml.erb\"\n    sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n      if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch, re-patching\"\n    fi\n    \n    # Patch a few things on the BPM:\n    #   - DYNO environment variable is not needed.\n    #   - We don't enable New Relic.\n    #   - NGINX maintenance shouldn't run.\n    patch --verbose \"${target}\" <<'EOT'\n    @@ -20,7 +20,6 @@\n         \"BUNDLE_GEMFILE\" => \"/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/Gemfile\",\n         \"CLOUD_CONTROLLER_NG_CONFIG\" => \"/var/vcap/jobs/cloud_controller_ng/config/cloud_controller_ng.yml\",\n         \"C_INCLUDE_PATH\" => \"/var/vcap/packages/libpq/include\",\n    -    \"DYNO\" => \"#{spec.job.name}-#{spec.index}\",\n         \"HOME\" => \"/home/vcap\",\n         \"LANG\" => \"en_US.UTF-8\",\n         \"LIBRARY_PATH\" => \"/var/vcap/packages/libpq/lib\",\n    @@ -79,8 +78,6 @@\n       \"processes\" => [\n         cloud_controller_ng_config,\n         nginx_config,\n    -    nginx_newrelic_plugin_config,\n    -    nginx_maintenance_config,\n         ccng_monit_http_healthcheck_config,\n       ]\n     }\n    EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\"\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=dotnet-core-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"dotnet-core-buildpack\"\n    buildpack=\"dotnet-core-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=dotnet-core-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"dotnet-core-buildpack\"\n    job=\"dotnet-core-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=go-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"go-buildpack\"\n    buildpack=\"go-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=go-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"go-buildpack\"\n    job=\"go-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=java-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"java-buildpack\"\n    buildpack=\"java-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=java-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"java-buildpack\"\n    job=\"java-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=nginx-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"nginx-buildpack\"\n    buildpack=\"nginx-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=nginx-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"nginx-buildpack\"\n    job=\"nginx-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=nodejs-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"nodejs-buildpack\"\n    buildpack=\"nodejs-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=nodejs-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"nodejs-buildpack\"\n    job=\"nodejs-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=php-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"php-buildpack\"\n    buildpack=\"php-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=php-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"php-buildpack\"\n    job=\"php-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=python-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"python-buildpack\"\n    buildpack=\"python-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=python-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"python-buildpack\"\n    job=\"python-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=r-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"r-buildpack\"\n    buildpack=\"r-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=r-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"r-buildpack\"\n    job=\"r-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=ruby-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"ruby-buildpack\"\n    buildpack=\"ruby-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=ruby-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"ruby-buildpack\"\n    job=\"ruby-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=staticfile-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"staticfile-buildpack\"\n    buildpack=\"staticfile-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=staticfile-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"staticfile-buildpack\"\n    job=\"staticfile-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-binary-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-binary-buildpack\"\n    buildpack=\"suse-binary-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-binary-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-binary-buildpack\"\n    job=\"suse-binary-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-dotnet-core-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-dotnet-core-buildpack\"\n    buildpack=\"suse-dotnet-core-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-dotnet-core-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-dotnet-core-buildpack\"\n    job=\"suse-dotnet-core-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-go-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-go-buildpack\"\n    buildpack=\"suse-go-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-go-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-go-buildpack\"\n    job=\"suse-go-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-java-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-java-buildpack\"\n    buildpack=\"suse-java-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-java-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-java-buildpack\"\n    job=\"suse-java-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-nginx-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-nginx-buildpack\"\n    buildpack=\"suse-nginx-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-nginx-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-nginx-buildpack\"\n    job=\"suse-nginx-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-nodejs-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-nodejs-buildpack\"\n    buildpack=\"suse-nodejs-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-nodejs-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-nodejs-buildpack\"\n    job=\"suse-nodejs-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-php-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-php-buildpack\"\n    buildpack=\"suse-php-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-php-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-php-buildpack\"\n    job=\"suse-php-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-python-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-python-buildpack\"\n    buildpack=\"suse-python-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-python-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-python-buildpack\"\n    job=\"suse-python-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-ruby-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-ruby-buildpack\"\n    buildpack=\"suse-ruby-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-ruby-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-ruby-buildpack\"\n    job=\"suse-ruby-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-staticfile-buildpack/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-staticfile-buildpack\"\n    buildpack=\"suse-staticfile-buildpack\"\n    \n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start\"\n    copy_dst=\"/var/vcap/data/shared-packages/${buildpack}/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst}\"\n    cp -r /var/vcap/packages \"${copy_dst}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=suse-staticfile-buildpack/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Add bin/pre-start to the buildpack job templates.\n    \n    set -o errexit -o nounset\n    \n    release=\"suse-staticfile-buildpack\"\n    job=\"suse-staticfile-buildpack\"\n    \n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    \n    sed -i 's|templates: {}||' \"${job_mf}\"\n    cat <<EOT > \"${job_mf}\"\n    templates:\n      bin/pre-start: bin/pre-start\n    EOT"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-app-autoscaler
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: ""
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-auctioneer
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    # Add quarks properties for the auctioneer job.
    - type: replace
      path: /instance_groups/name=auctioneer/jobs/name=auctioneer/properties/quarks?
      value:
        ports:
        - name: auctioneer
          protocol: TCP
          internal: 9016
        activePassiveProbes:
          auctioneer-auctioneer:
            exec:
              command:
              - bash
              - -ce
              - "head -c0 </dev/tcp/${HOSTNAME}/9016"
    # Set the alias auctioneer.service.cf.internal instance group to auctioneer.
    - type: replace
      path: /addons/name=bosh-dns-aliases/jobs/name=bosh-dns-aliases/properties/aliases/domain=auctioneer.service.cf.internal/targets/0/instance_group
      value: auctioneer
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-brain-tests
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: ""
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-cc-worker
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    # Add quarks properties for cloud_controller_worker
    - type: replace
      path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/quarks?/run/healthcheck/worker_1
      value:
        readiness: &cloud_controller_worker_readiness
          exec:
            command: [/usr/bin/pgrep, --full, cc-worker-cloud_controller_worker]
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-credhub
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |2-


    # Remove directly from the cf-deployment.yml YAML file.
    - path: /addons/name=bosh-dns-aliases/jobs/name=bosh-dns-aliases/properties/aliases/domain=credhub.service.cf.internal?
      type: remove
    - path: /instance_groups/name=database/jobs/name=pxc-mysql/properties/seeded_databases/name=credhub?
      type: remove
    - path: /instance_groups/name=uaa/jobs/name=uaa/properties/uaa/clients/cc_service_key_client?
      type: remove
    - path: /instance_groups/name=uaa/jobs/name=uaa/properties/uaa/clients/credhub_admin_client?
      type: remove
    - path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/credhub_api?
      type: remove
    - path: /instance_groups/name=diego-cell/jobs/name=cflinuxfs3-rootfs-setup/properties/cflinuxfs3-rootfs/trusted_certs/1
      type: remove
    - path: /instance_groups/name=diego-cell/jobs/name=rep/properties/containers/trusted_ca_certificates/1
      type: remove
    - path: /instance_groups/name=credhub?
      type: remove
    - path: /variables/name=credhub_encryption_password?
      type: remove
    - path: /variables/name=credhub_admin_client_secret?
      type: remove
    - path: /variables/name=credhub_database_password?
      type: remove
    - path: /variables/name=credhub_ca?
      type: remove
    - path: /variables/name=credhub_tls?
      type: remove
    - path: /releases/name=credhub?
      type: remove
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-database
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    # Remove database instance group and variables.
    - type: remove
      path: /instance_groups/name=database

    # Remove the PXC release and related variables.
    - type: remove
      path: /releases/name=pxc
    - type: remove
      path: /variables/name=pxc_galera_ca
    - type: remove
      path: /variables/name=pxc_server_ca
    - type: remove
      path: /variables/name=galera_server_certificate
    - type: remove
      path: /variables/name=mysql_server_certificate

    # Add certificates for PXC.
    - type: replace
      path: /variables/-
      value:
        name: pxc_ca
        type: certificate
        options:
          common_name: pxc_ca
          is_ca: true
    - type: replace
      path: /variables/-
      value:
        name: pxc_tls
        type: certificate
        options:
          ca: pxc_ca
          common_name: &pxc-cluster-address database.default

    # Override the address and CA cert with the one from PXC.
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=bbs/properties/diego/bbs/sql/db_host?
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=bbs/properties/diego/bbs/sql/ca_cert?
      value: &pxc-cluster-ca ((pxc_tls.ca))

    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=locket/properties/diego/locket/sql/db_host?
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=locket/properties/diego/locket/sql/ca_cert?
      value: *pxc-cluster-ca

    - type: replace
      path: /instance_groups/name=uaa/jobs/name=uaa/properties/uaadb/address?
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=uaa/jobs/name=uaa/properties/uaa?/ca_certs
      value:
      - *pxc-cluster-ca

    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/ccdb/address?
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/ccdb/ca_cert?
      value: *pxc-cluster-ca

    - type: replace
      path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/ccdb/address?
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/ccdb/ca_cert?
      value: *pxc-cluster-ca

    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/ccdb/address?
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/ccdb/ca_cert?
      value: *pxc-cluster-ca

    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=cc_deployment_updater/properties/ccdb/address?
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=cc_deployment_updater/properties/ccdb/ca_cert?
      value: *pxc-cluster-ca

    - type: replace
      path: /instance_groups/name=api/jobs/name=policy-server/properties/database/host
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=api/jobs/name=policy-server/properties/database/ca_cert?
      value: *pxc-cluster-ca

    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=silk-controller/properties/database/host
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=silk-controller/properties/database/ca_cert?
      value: *pxc-cluster-ca
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-diego-api
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: "# Selectively remove jobs temporarily.\n- type: remove\n  path: /instance_groups/name=diego-api/jobs/name=silk-controller\n\n# Override the addresses for the jobs under the diego-api instance group.\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=bbs/properties/diego/bbs/locket?/api_location\n  value: 127.0.0.1:8891\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=bbs/properties/diego/bbs/health_addr?\n  value: 0.0.0.0:8890\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=cfdot/properties/bbs?/hostname\n  value: 127.0.0.1\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=cfdot/properties/locket?/hostname\n  value: 127.0.0.1\n- type: replace\n  path: /variables/name=diego_bbs_server/options?/alternative_names?/-\n  value: '127.0.0.1'\n- type: replace\n  path: /variables/name=diego_locket_server/options?/alternative_names?/-\n  value: '127.0.0.1'\n\n# Disable tuning /proc/sys kernel parameters as locket and bbs are running on containers.\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=locket/properties/set_kernel_parameters?\n  value: false\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=bbs/properties/set_kernel_parameters?\n  value: false\n\n# Add quarks properties for locket.\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=locket/properties/quarks?\n  value:\n    ports:\n    - name: locket\n      protocol: TCP\n      internal: 8891\n    run:\n      healthcheck:\n        locket:\n          readiness:\n            exec:\n              command:\n              - /var/vcap/packages/cfdot/bin/cfdot\n              - locks\n              - --locketAPILocation=localhost:8891\n              # We need to both give --skipCertVerify and provide a CA cert;\n              # skipping the former ends up with context deadline exceeded, and\n              # skipping the latter errors out failing to read file \"\"\n              - --skipCertVerify\n              - --caCertFile=/var/vcap/jobs/locket/config/certs/ca.crt\n              - --clientCertFile=/var/vcap/jobs/locket/config/certs/server.crt\n              - --clientKeyFile=/var/vcap/jobs/locket/config/certs/server.key\n\n# Add quarks properties for bbs.\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=bbs/properties/quarks?\n  value:\n    ports:\n    - name: cell-bbs-api\n      protocol: TCP\n      internal: 8889 # If you change this values, change the probe below too\n    activePassiveProbes:\n      bbs-bbs:\n        exec:\n          command:\n          - bash\n          - -ce\n          - \"head -c0 </dev/tcp/${HOSTNAME}/8889\"\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=cfdot/properties/quarks?/bpm/processes\n  value: []\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=bbs/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n    target=\"/var/vcap/all-releases/jobs-src/diego/bbs/templates/bbs.json.erb\"\n    sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n      if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch, re-patching\"\n    fi\n    \n    # Advertise our spec address.\n    patch --verbose \"${target}\" <<'EOT'\n    62c62\n    <     \"#{scheme}://#{name.gsub('_', '-')}-#{spec.index}.#{base}:#{port}\"\n    ---\n    >     \"#{scheme}://#{spec.address}:#{port}\"\n    EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\"\n\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=cfdot/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    ##\n    # ATTENTION: This is part of a set of six interconnected patches.\n    #            Two files spread over three instance groups.\n    # See\n    # - bosh/releases/pre_render_scripts/diego-cell/cfdot/jobs\n    # - bosh/releases/pre_render_scripts/diego-api/cfdot/jobs\n    # - bosh/releases/pre_render_scripts/scheduler/cfdot/jobs\n    \n    set -o errexit -o nounset\n    \n    target=\"/var/vcap/all-releases/jobs-src/diego/cfdot/templates/pre-start.erb\"\n    sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n      if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch, re-patching\"\n    fi\n    \n    # Place the cfdot related things into a location shared by all the jobs.\n    ##\n    # Implementation note: Given the small size of the pre-start script a\n    # patch is likely as large, or even larger than just the replacement\n    # script, we simply do the latter.\n    cat > \"${target}\" <<'EOT'\n    #!/bin/bash -e\n    \n    DEST=/var/vcap/data/cfdot/bin\n    mkdir -p \"${DEST}\"\n    \n    cp /var/vcap/jobs/cfdot/bin/setup     \"${DEST}/cfdot.sh\"\n    cp /var/vcap/packages/cfdot/bin/cfdot \"${DEST}/cfdot\"\n    chown root:vcap \"${DEST}/cfdot.sh\"\n    EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\"\n\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=cfdot/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    ##\n    # ATTENTION: This is part of a set of six interconnected patches.\n    #            Two files spread over three instance groups.\n    # See\n    # - bosh/releases/pre_render_scripts/diego-cell/cfdot/jobs\n    # - bosh/releases/pre_render_scripts/diego-api/cfdot/jobs\n    # - bosh/releases/pre_render_scripts/scheduler/cfdot/jobs\n    \n    set -o errexit -o nounset\n    \n    target=\"/var/vcap/all-releases/jobs-src/diego/cfdot/templates/setup.erb\"\n    sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n      if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch, re-patching\"\n    fi\n    \n    # Look for cfdot in the new, shared location.\n    sed -i \"s|PATH=/var/vcap/packages|PATH=/var/vcap/data|g\" \"${target}\"\n    \n    sha256sum \"${target}\" > \"${sentinel}\""
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-diego-cell
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: ""
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-doppler
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    # Add quarks properties for doppler.
    - type: replace
      path: /instance_groups/name=doppler/jobs/name=doppler/properties/quarks?
      value:
        ports:
        - name: doppler-grpc
          protocol: TCP
          internal: 8082
        run:
          healthcheck:
            doppler:
              readiness:
                exec:
                  command: [sh, -c, 'ss -nlt sport = 8082 | grep "LISTEN.*:8082"']
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-eirini-helm
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: "\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/opi?/enabled?\n  value: true\n\n# TODO(jandubois) This check is disabled until v2.3.0 for backwards compatibility reasons.\n## TODO(jandubois) This check becomes redundant once the previous check is enabled because\n# TODO(jandubois) the api.yaml file already checks that suse_buildpacks are enabled when\n# TODO(jandubois) suse_default_stack is set.\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/opi?/url?\n  value: \"https://eirini-opi.default.svc.cluster.local:8085\"\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/packages?/webdav_config?/private_endpoint?\n  value: \"https://singleton-blobstore.default.svc.cluster.local:4443\"\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/buildpacks?/webdav_config?/private_endpoint?\n  value: \"https://singleton-blobstore.default.svc.cluster.local:4443\"\n- type: replace\n  path: /variables/name=blobstore_tls/options/alternative_names?/-\n  value: \"singleton-blobstore.default.svc.cluster.local\"\n- type: replace\n  path: /variables/name=cc_bridge_cc_uploader_server/options/alternative_names?/-\n  value: \"api.default.svc.cluster.local\"\n- type: replace\n  path: /variables/name=cc_bridge_cc_uploader/options/copies?/-\n  value:\n    namespace: eirini\n    name: var-cc-bridge-cc-uploader\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cc_uploader/properties/internal_hostname?\n  value: \"api.default.svc.cluster.local\"\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/internal_service_hostname?\n  value: \"api.default.svc.cluster.local\"\n- type: replace\n  path: /variables/name=cc_public_tls/options/alternative_names?/-\n  value: \"api.default.svc.cluster.local\"\n- type: replace\n  path: /variables/name=cc_tls/options/alternative_names?/-\n  value: \"api.default.svc.cluster.local\"\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/opi?/opi_staging?\n  value: true\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/opi?/client_cert?\n  value: ((eirini_tls_client_cert.certificate))\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/opi?/client_key?\n  value: ((eirini_tls_client_cert.private_key))\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/opi?/ca_cert?\n  value: ((eirini_tls_server_cert.ca))\n\n- type: replace\n  path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/cc/opi?/enabled?\n  value: true\n- type: replace\n  path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/cc/opi?/url?\n  value: \"https://eirini-opi.default.svc.cluster.local:8085\" \n- type: replace\n  path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/cc/opi?/opi_staging?\n  value: true\n- type: replace\n  path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/cc/opi?/client_cert?\n  value: ((eirini_tls_client_cert.certificate))\n- type: replace\n  path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/cc/opi?/client_key?\n  value: ((eirini_tls_client_cert.private_key))\n- type: replace\n  path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/cc/opi?/ca_cert?\n  value: ((eirini_tls_server_cert.ca))\n\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/cc/opi?/enabled?\n  value: true\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/cc/opi?/url?\n  value: \"https://eirini-opi.default.svc.cluster.local:8085\"\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/cc/opi?/opi_staging?\n  value: true\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/cc/opi?/client_cert?\n  value: ((eirini_tls_client_cert.certificate))\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/cc/opi?/client_key?\n  value: ((eirini_tls_client_cert.private_key))\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/cc/opi?/ca_cert?\n  value: ((eirini_tls_server_cert.ca))\n\n- type: replace\n  path: /variables/name=loggregator_tls_agent/options/alternative_names?\n  value:\n    - localhost\n    - metron\n- type: replace\n  path: /variables/name=loggregator_tls_doppler/options/alternative_names?/-\n  value: metron\n\n- type: replace\n  path: /variables/name=eirini_tls_server_cert?\n  value:\n    name: eirini_tls_server_cert\n    type: certificate\n    options:\n      ca: service_cf_internal_ca\n      common_name: \"eirini-opi\"\n      alternative_names:\n      - \"eirini-opi.default.svc.cluster.local\"\n      extended_key_usage:\n      - server_auth\n\n- type: replace\n  path: /variables/name=eirini_tls_client_cert?\n  value:\n    name: eirini_tls_client_cert\n    type: certificate\n    options:\n      ca: service_cf_internal_ca\n      common_name: cloud_controller\n      extended_key_usage:\n      - client_auth\n      copies:\n      - namespace: eirini\n        name: var-eirini-tls-client-cert\n\n# Remove the whole diego-cell instance group.\n- type: remove\n  path: /instance_groups/name=diego-cell\n\n# Remove bbs from diego-api.\n# TODO: remove bbs in the future - when the clock and worker no longer need it.\n# - type: remove\n#   path: /instance_groups/name=diego-api/jobs/name=bbs\n\n# Remove auctioneer, tps and ssh_proxy from scheduler.\n- type: remove\n  path: /instance_groups/name=auctioneer\n- type: remove\n  path: /instance_groups/name=scheduler/jobs/name=tps\n- type: remove\n  path: /instance_groups/name=scheduler/jobs/name=ssh_proxy\n\n# For eirini-extensions: Add hostname-only names to the CC/UAA certificates\n- type: replace\n  path: /variables/name=cc_public_tls/options/alternative_names/-\n  value: api\n- type: replace\n  path: /variables/name=uaa_ssl/options/alternative_names/-\n  value: uaa\n- type: replace\n  path: /instance_groups/name=uaa/jobs/name=uaa/properties/uaa/zones/internal/hostnames/-\n  value: uaa"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-log-api
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    # Add quarks properties for loggregator_trafficcontroller.
    - type: replace
      path: /instance_groups/name=log-api/jobs/name=loggregator_trafficcontroller/properties/quarks?
      value:
        envs:
        - name: TRAFFIC_CONTROLLER_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        run:
          healthcheck:
            loggregator_trafficcontroller:
              # The traffic controller doesn't expose anything to indicate its healthiness.
              readiness: ~

    # Add quarks properties for reverse_log_proxy.
    - type: replace
      path: /instance_groups/name=log-api/jobs/name=reverse_log_proxy/properties/reverse_log_proxy?/pprof/port
      value: "33047"
    - type: replace
      path: /instance_groups/name=log-api/jobs/name=reverse_log_proxy/properties/quarks?
      value:
        ports:
        - name: grpc-egress
          protocol: TCP
          internal: 8082
        run:
          healthcheck:
            reverse_log_proxy:
              readiness:
                exec:
                  command:
                  - curl
                  - --fail
                  - --head
                  - --silent
                  - http://localhost:33047/debug/pprof/cmdline

    - type: replace
      path: /instance_groups/name=log-api/jobs/name=reverse_log_proxy_gateway/properties/quarks?/bpm/processes/name=reverse_log_proxy_gateway/env/PPROF_PORT?
      value: "33045"
    - type: replace
      path: /instance_groups/name=log-api/jobs/name=reverse_log_proxy_gateway/properties/quarks?/run/healthcheck/reverse_log_proxy_gateway/readiness/exec/command
      value: ["curl", "--fail", "--head", "--silent", "http://localhost:33045/debug/pprof/cmdline"]

    - type: replace
      path: /instance_groups/name=log-api/jobs/name=route_registrar/properties/quarks?/run/healthcheck/route_registrar
      value:
        # The route registrar doesn't expose anything to indicate if the
        # routes are healthy.
        readiness: ~
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-log-cache
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    # Change the log_cache_ca CN to avoid clashing with the other log-cache certificate CNs.
    - type: replace
      path: /variables/name=log_cache_ca/options/common_name
      value: log-cache-ca

    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache/properties/health_addr?
      value: "::6060"

    # Add quarks properties for log-cache.
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache/properties/quarks?
      value:
        ports:
        - name: log-cache-metrics
          protocol: TCP
          internal: 6060
        - name: log-cache
          protocol: TCP
          internal: 8080
        run:
          healthcheck:
            log-cache:
              readiness:
                exec:
                  command:
                  - curl
                  - --insecure
                  - --fail
                  - --head
                  - --silent
                  - --cert
                  - /var/vcap/jobs/log-cache/config/certs/metrics.crt
                  - --key
                  - /var/vcap/jobs/log-cache/config/certs/metrics.key
                  - https://localhost:6060/debug/pprof/cmdline

    # Add quarks properties for log-cache-gateway.
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache-gateway/properties/quarks?
      value:
        ports:
        - name: log-cache-gateway-metrics
          protocol: TCP
          internal: 6063
        run:
          healthcheck:
            log-cache-gateway:
              readiness:
                # Unfortunately, by default the health port listens on localhost only
                # and isn't easily configurable.
                exec:
                  command:
                  - curl
                  - --insecure
                  - --fail
                  - --head
                  - --silent
                  - --cert
                  - /var/vcap/jobs/log-cache/config/certs/metrics.crt
                  - --key
                  - /var/vcap/jobs/log-cache/config/certs/metrics.key
                  - https://localhost:6063/debug/pprof/cmdline

    # Add quarks properties for log-cache-nozzle.
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache-nozzle/properties/quarks?
      value:
        run:
          healthcheck:
            log-cache-nozzle:
              readiness:
                # Unfortunately, by default the health port listens on localhost only
                # and isn't easily configurable.
                exec:
                  command: [curl, --fail, --head, --silent, http://localhost:6061/debug/pprof/cmdline]

    # Add quarks properties for log-cache-cf-auth-proxy.
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache-cf-auth-proxy/properties/quarks?
      value:
        ports:
        - name: log-cache-cf-auth-proxy-metrics
          protocol: TCP
          internal: 6065
        - name: log-cache-cf-auth-proxy
          protocol: TCP
          internal: 8083
        run:
          healthcheck:
            log-cache-cf-auth-proxy:
              readiness:
                # Unfortunately, by default the health port listens on localhost only
                # and isn't easily configurable.
                exec:
                  command:
                  - curl
                  - --insecure
                  - --fail
                  - --head
                  - --silent
                  - --cert
                  - /var/vcap/jobs/log-cache/config/certs/metrics.crt
                  - --key
                  - /var/vcap/jobs/log-cache/config/certs/metrics.key
                  - https://localhost:6065/debug/pprof/cmdline

    # Add quarks properties for route_registrar.
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=route_registrar/properties/quarks?
      value:
        run:
          healthcheck:
            route_registrar:
              # The route registrar doesn't expose anything to indicate if the
              # routes are healthy.
              readiness: ~

    # Add log-cache.service.cf.internal DNS alias to be able to point CC to it.
    - type: replace
      path: /addons/name=bosh-dns-aliases/jobs/name=bosh-dns-aliases/properties/aliases/-
      value:
        domain: log-cache.service.cf.internal
        targets:
        - deployment: cf
          domain: bosh
          instance_group: log-cache
          network: default
          query: '*'

    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/cc/logcache/host
      value: log-cache.service.cf.internal
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-nats
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    # Add the nats BOSH DNS alias.
    - type: replace
      path: /addons/name=bosh-dns-aliases/jobs/name=bosh-dns-aliases/properties/aliases/-
      value:
        domain: nats.service.cf.internal
        targets:
        - deployment: cf
          domain: bosh
          instance_group: nats
          network: default
          query: '*'

    # Add quarks properties.
    - type: replace
      path: /instance_groups/name=nats/jobs/name=nats/properties/quarks?
      value:
        ports:
        - name: nats
          protocol: TCP
          internal: 4222
        - name: nats-routes
          protocol: TCP
          internal: 4223
        run:
          healthcheck:
            nats:
              readiness:
                exec:
                  command:
                  - sh
                  - -c
                  - ss -nlt sport = 4222 | grep "LISTEN.*:4222" && ss -nlt sport = 4223 | grep "LISTEN.*:4223"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-router
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    # Add quarks properties for the gorouter job.
    - type: replace
      path: /instance_groups/name=router/jobs/name=gorouter/properties/quarks?
      value:
        ports:
        - name: router
          protocol: TCP
          internal: 80
        - name: router-ssl
          protocol: TCP
          internal: 443
        run:
          healthcheck:
            gorouter:
              readiness:
                httpGet:
                  port: 8080
                  path: /health
        post_start:
          condition:
            exec:
              command: ["curl", "--fail", "--head", "http://127.0.0.1:8080/health"]

    # Disable tuning /proc/sys kernel parameters as things are running on a container.
    - type: replace
      path: /instance_groups/name=router/jobs/name=gorouter/properties/router?/set_kernel_parameters
      value: false

    - type: replace
      path: /instance_groups/name=router/jobs/name=loggr-udp-forwarder/properties/quarks?/run/healthcheck/loggr-udp-forwarder
      value:
        readiness:
          exec:
            command: ["sh", "-c", "ss -nlu sport = 3457 | grep :3457"]

    # Add necessary labels to the router instance group so that the service can select it to create the
    # endpoint.
    - type: replace
      path: /instance_groups/name=router/env?/bosh/agent/settings/labels/app.kubernetes.io~1component
      value: "router"
    - type: replace
      path: /instance_groups/name=router/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=router/env?/bosh/agent/settings/labels/app.kubernetes.io~1version
      value: "v2.3.0"

    # Update cipher suites based on https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations
    - type: replace
      path: /instance_groups/name=router/jobs/name=gorouter/properties/router/cipher_suites?
      value: "\
        ECDHE-ECDSA-CHACHA20-POLY1305:\
        ECDHE-RSA-CHACHA20-POLY1305:\
        ECDHE-ECDSA-AES128-GCM-SHA256:\
        ECDHE-RSA-AES128-GCM-SHA256:\
        ECDHE-ECDSA-AES256-GCM-SHA384:\
        ECDHE-RSA-AES256-GCM-SHA384:\
        AES128-GCM-SHA256:\
        AES256-GCM-SHA384"

    # Trust the diego_instance_identity_ca certificate - trusting its CA
    # is insufficient with the cf-operator
    # Also add the CredHub CA
    - type: replace
      path: /instance_groups/name=router/jobs/name=gorouter/properties/router/ca_certs
      value: |
        ((diego_instance_identity_ca.ca))
        ((cc_tls.ca))
        ((uaa_ssl.ca))
        ((network_policy_server_external.ca))

    # If the router certificate is provided via Helm, don't generate the router_ca and router_ssl
    # certificates. router_ssl becomes an implicit variable.
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-routing-api
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |2-

    # Disable routing-api.
    - type: remove
      path: /instance_groups/name=api/jobs/name=routing-api

    - type: remove
      path: /instance_groups/name=database?/jobs/name=pxc-mysql/properties/seeded_databases/name=routing-api

    - type: replace
      path: /instance_groups/name=router/jobs/name=gorouter/properties/routing_api/enabled
      value: false

    - type: remove
      path: /addons/name=bosh-dns-aliases/jobs/name=bosh-dns-aliases/properties/aliases/domain=routing-api.service.cf.internal

    - type: remove
      path: /variables/name=routing_api_tls

    - type: remove
      path: /variables/name=routing_api_tls_client
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-scheduler
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    # Add quarks properties for the scheduler job.
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=scheduler/properties/quarks?/run/healthcheck/scheduler
      value:
        readiness:
          exec:
            command: ["curl", "--fail", "--head", "--silent", "http://127.0.0.1:8080/health"]

    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/quarks?/run/healthcheck/cloud_controller_clock
      value:
        readiness:
          # There is no good readiness check for the scheduled tasks
          exec:
            command: ["pgrep", "--full", "clock:start"]

    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=cc_deployment_updater/properties/quarks?/run/healthcheck/cc_deployment_updater
      value:
        readiness:
          exec:
            command:
            # We should sleep about once every 5 seconds; check that the last entry was no more than 2 cycles ago
            - sh
            - -c
            - tac /var/vcap/sys/log/cc_deployment_updater/cc_deployment_updater.log | grep --max-count=1 Sleeping | jq -r '(now - .timestamp) < 10'
        liveness:
          exec:
            command: ["pgrep", "--full", "deployment_updater:start"]

    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=statsd_injector/properties/quarks?/run/healthcheck/statsd_injector/readiness/exec/command
      value: ["/bin/sh", "-c", "ss -nlu src localhost:8125 | grep :8125"]

    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=cfdot/properties/quarks?/bpm/processes
      value: []
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-singleton-blobstore
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: "\n- type: replace\n  path: /instance_groups/name=singleton-blobstore/persistent_disk?\n  value: 102400 # 100GB\n- type: remove\n  path: /instance_groups/name=singleton-blobstore/persistent_disk_type\n\n- type: replace\n  path: /instance_groups/name=singleton-blobstore/jobs/name=blobstore/properties/blobstore/internal_access_rules?\n  value: [ \"allow 10.0.0.0/8;\",\"allow 172.16.0.0/12;\", \"allow 192.168.0.0/16;\" , \"allow 100.64.0.0/10;\"]\n- type: replace\n  path: /instance_groups/name=singleton-blobstore/jobs/name=blobstore/properties/quarks?\n  value:\n    ports:\n    - name: http\n      protocol: TCP\n      internal: 8080\n    - name: https\n      protocol: TCP\n      internal: 4443\n    run:\n      security_context:\n        runAsUser: 1000 # vcap\n      healthcheck:\n        nginx:\n          readiness:\n            tcpSocket:\n              port: 8080\n          liveness:\n            exec:\n              command: [pgrep, --full, 'nginx: master process']\n        url_signer:\n          readiness:\n            exec:\n              command: [test, -S, /var/vcap/data/blobstore/signer.sock]\n\n- type: replace\n  path: /instance_groups/name=singleton-blobstore/jobs/name=route_registrar/properties/quarks?/run/healthcheck/route_registrar\n  value:\n    readiness: ~\n      # The route registrar doesn't expose anything to indicate if the\n      # routes are healthy.\n- type: replace\n  path: /instance_groups/name=singleton-blobstore/jobs/name=blobstore/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n    # Remove /var/vcap/packages from chowing.\n    \n    target=\"/var/vcap/all-releases/jobs-src/capi/blobstore/templates/pre-start.sh.erb\"\n    sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n      if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch, re-patching\"\n    fi\n    \n    patch --verbose \"${target}\" <<'EOT'\n    @@ -9,7 +9,6 @@\n       local data_dir=/var/vcap/data/blobstore\n       local store_tmp_dir=$store_dir/tmp/uploads\n       local data_tmp_dir=$data_dir/tmp/uploads\n    -  local nginx_webdav_dir=/var/vcap/packages/nginx_webdav\n    \n       mkdir -p $run_dir\n       mkdir -p $log_dir\n    @@ -19,7 +18,7 @@\n       mkdir -p $data_tmp_dir\n    \n       chown vcap:vcap $store_dir\n    -  local dirs=\"$run_dir $log_dir $store_tmp_dir $data_dir $data_tmp_dir $nginx_webdav_dir ${nginx_webdav_dir}/..\"\n    +  local dirs=\"$run_dir $log_dir $store_tmp_dir $data_dir $data_tmp_dir\"\n       local num_needing_chown=$(find $dirs -not -user vcap -or -not -group vcap | wc -l)\n    \n       if [ $num_needing_chown -gt 0 ]; then\n    EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\""
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-smoke-tests
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: "\n\n- type: replace\n  path: /instance_groups/name=smoke-tests/jobs/name=cf-cli-6-linux/properties?/quarks/bpm/processes\n  value: []\n\n- type: replace\n  path: /instance_groups/name=smoke-tests/env?/bosh/agent/settings/disable_log_sidecar\n  value: true\n\n- type: replace\n  path: /instance_groups/name=smoke-tests/env?/bosh/agent/settings/jobBackoffLimit\n  value: 1\n\n# The smoke-tests require the cf-cli package that is provided by the cf-cli-6-linux job from another\n# BOSH release. Since the /var/vcap/packages directory is not shared between the containers of different\n# releases, smoke-tests won't have access # to the cf-cli package. \n# As part of the pre-render scripts added here will patch the cf-cli-6-linux\n# job in order to add a BOSH pre-start script. This script will then copy the cf-cli package into a\n# shared volume that can be accessed by the smoke-tests job.\n- type: replace\n  path: /instance_groups/name=smoke-tests/jobs/name=cf-cli-6-linux/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    # Create the pre-start script that copies the cf-cli packages to /var/vcap/data/shared-packages/.\n    \n    set -o errexit -o nounset\n    \n    release=\"cf-cli\"\n    job=\"cf-cli-6-linux\"\n    pre_start=\"/var/vcap/all-releases/jobs-src/${release}/${job}/templates/bin/pre-start\"\n    copy_dst_dir=\"/var/vcap/data/shared-packages/\"\n    mkdir -p \"$(dirname \"${pre_start}\")\"\n    cat <<EOT > \"${pre_start}\"\n    #!/usr/bin/env bash\n    set -o errexit\n    mkdir -p \"${copy_dst_dir}\"\n    cp -r /var/vcap/packages/* \"${copy_dst_dir}\"\n    EOT\n\n- type: replace\n  path: /instance_groups/name=smoke-tests/jobs/name=cf-cli-6-linux/properties/quarks?/pre_render_scripts/ig_resolver/-\n  value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n    release=\"cf-cli\"\n    job=\"cf-cli-6-linux\"\n    job_mf=\"/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF\"\n    mkdir -p \"$(dirname \"${job_mf}\")\"\n    cat <<EOT > \"${job_mf}\"\n    ---\n    name: ${job}\n    \n    packages: []\n    \n    templates:\n      bin/pre-start: bin/pre-start\n    \n    properties: {}\n    EOT\n\n- type: replace\n  path: /instance_groups/name=smoke-tests/jobs/name=smoke_tests/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    target=\"/var/vcap/all-releases/jobs-src/cf-smoke-tests/smoke_tests/templates/test.erb\"\n    sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n      if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch, re-patching\"\n    fi\n    \n    # Patch test.erb to add the correct cf-cli path to $PATH.\n    patch --verbose \"${target}\" <<'EOT'\n    8c8\n    < export PATH=/var/vcap/packages/cf-cli-6-linux/bin:${PATH} # put the cli on the path\n    ---\n    > export PATH=/var/vcap/data/shared-packages/cf-cli-6-linux/bin:${PATH} # put the cli on the path\n    EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\""
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-sync-integration-tests
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    # SITS only makes sense when using Diego, for this reason, we only enable it if Eirini is not
    # enabled.
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-tcp-router
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |2-

    # Disable the tcp-router ig, as there will be no routing-api SSE events to read from.
    - type: remove
      path: /instance_groups/name=tcp-router

    - type: remove
      path: /variables/name=uaa_clients_tcp_router_secret

    - type: remove
      path: /instance_groups/name=uaa/jobs/name=uaa/properties/uaa/clients/tcp_router
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-uaa
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: "# Add quarks properties.\n- type: replace\n  path: /instance_groups/name=uaa/jobs/name=uaa/properties/quarks?\n  value:\n    ports:\n    - name: http\n      protocol: TCP\n      internal: 8080\n    - name: https\n      protocol: TCP\n      internal: &uaa_https_port 8443\n    run:\n      healthcheck:\n        uaa:\n          # UAA has a long period of cert import, so we can't set up a liveness\n          # check without killing it accidentally.\n          readiness: &uaa_readiness\n            exec:\n              command: ['sh', '-c', '/var/vcap/jobs/uaa/bin/health_check']\n    post_start:\n      condition: *uaa_readiness\n\n- type: replace\n  path: /instance_groups/name=uaa/jobs/name=route_registrar/properties/quarks?/run/healthcheck/route_registrar\n  value:\n    readiness: ~\n      # The route registrar doesn't expose anything to indicate if the\n      # routes are healthy.\n\n- type: replace\n  path: /instance_groups/name=uaa/jobs/name=statsd_injector/properties/quarks?/run/healthcheck/statsd_injector/readiness/exec/command\n  value: [\"/bin/sh\", \"-c\", \"ss -nlu src localhost:8125 | grep :8125\"]\n- type: replace\n  path: /instance_groups/name=uaa/jobs/name=uaa/properties/quarks?/pre_render_scripts/jobs/-\n  value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n    target=\"/var/vcap/all-releases/jobs-src/uaa/uaa/templates/bin/pre-start.erb\"\n    sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n      if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch, re-patching\"\n    fi\n    \n    # Patch bin/pre-start.erb for the certificates to work with SUSE.\n    patch --verbose \"${target}\" <<'EOT'\n    --- pre-start.erb  2019-12-04 08:37:51.046503943 +0100\n    +++ - 2019-12-04 08:41:36.055142488 +0100\n    @@ -32,9 +32,24 @@\n         <% end %>\n    \n         log \"Trying to run update-ca-certificates...\"\n    -    # --certbundle is an undocumented flag in the update-ca-certificates script\n    -    # https://salsa.debian.org/debian/ca-certificates/blob/master/sbin/update-ca-certificates#L53\n    -    timeout --signal=KILL 180s /usr/sbin/update-ca-certificates -f -v --certbundle \"$(basename \"${OS_CERTS_FILE}\")\"\n    +    source /etc/os-release\n    +    case \"${ID}\" in\n    +      *ubuntu*)\n    +        # --certbundle is an undocumented flag in the update-ca-certificates script\n    +        # https://salsa.debian.org/debian/ca-certificates/blob/master/sbin/update-ca-certificates#L53\n    +        timeout --signal=KILL 180s /usr/sbin/update-ca-certificates -f -v --certbundle \"$(basename \"${OS_CERTS_FILE}\")\"\n    +      ;;\n    +\n    +      *suse|sles*)\n    +        timeout --signal=KILL 180s /usr/sbin/update-ca-certificates -f -v\n    +        mv /var/lib/ca-certificates/ca-bundle.pem /etc/ssl/certs/\"$(basename \"${OS_CERTS_FILE}\")\"\n    +      ;;\n    +\n    +      *)\n    +        echo \"Unsupported operating system: ${PRETTY_NAME}\"\n    +        exit 42\n    +      ;;\n    +    esac\n     }\n    \n     function new_cache_files_are_identical {\n    EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\""
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-addons
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    - type: remove
      path: /addons/name=bpm

    - type: replace
      path: /addons/name=loggregator_agent/jobs/name=loggregator_agent/properties/quarks?/envs?
      value:
      - name: AGENT_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: AGENT_INDEX
        value: "0"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-azs
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |2-

    - type: remove
      path: /instance_groups/name=nats/azs?
    - type: remove
      path: /instance_groups/name=adapter/azs?

    - type: remove
      path: /instance_groups/name=diego-api/azs?
    - type: remove
      path: /instance_groups/name=uaa/azs?
    - type: remove
      path: /instance_groups/name=singleton-blobstore/azs?
    - type: remove
      path: /instance_groups/name=api/azs?
    - type: remove
      path: /instance_groups/name=cc-worker/azs?
    - type: remove
      path: /instance_groups/name=scheduler/azs?
    - type: remove
      path: /instance_groups/name=router/azs?
    - type: remove
      path: /instance_groups/name=doppler/azs?
    - type: remove
      path: /instance_groups/name=log-api/azs?
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-releases
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    # Remove unused releases from cf-deployment.
    - type: remove
      path: /releases/name=bosh-dns-aliases
    - type: remove
      path: /releases/name=bpm

    # Create a list with the current cf-deployment releases for replacing their info.

    # Add the cf-acceptance-tests-release.

    # SITS only makes sense when using Diego, for this reason, we only enable it if Eirini is not
    # enabled.

    # Add app-autoscaler related releases.

    # Loop over the releases, replacing the url and stemcell, and the version when appropriate.
    - type: replace
      path: /releases/name=binary-buildpack/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=binary-buildpack/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=binary-buildpack/sha1?
    - type: replace
      path: /releases/name=capi/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=capi/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=capi/sha1?
    - type: replace
      path: /releases/name=cf-networking/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=cf-networking/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=cf-networking/sha1?
    - type: replace
      path: /releases/name=cf-smoke-tests/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=cf-smoke-tests/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=cf-smoke-tests/version?
      value: "40.0.128"
    - type: remove
      path: /releases/name=cf-smoke-tests/sha1?
    - type: replace
      path: /releases/name=cf-syslog-drain/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=cf-syslog-drain/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=cf-syslog-drain/sha1?
    - type: replace
      path: /releases/name=cflinuxfs3/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=cflinuxfs3/stemcell?
      value: {"os":"SLE_15_SP1","version":"26.6-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=cflinuxfs3/version?
      value: "0.197.0"
    - type: remove
      path: /releases/name=cflinuxfs3/sha1?
    - type: replace
      path: /releases/name=diego/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=diego/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=diego/sha1?
    - type: replace
      path: /releases/name=dotnet-core-buildpack/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=dotnet-core-buildpack/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=dotnet-core-buildpack/sha1?
    - type: replace
      path: /releases/name=garden-runc/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=garden-runc/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=garden-runc/sha1?
    - type: replace
      path: /releases/name=go-buildpack/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=go-buildpack/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=go-buildpack/sha1?
    - type: replace
      path: /releases/name=java-buildpack/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=java-buildpack/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=java-buildpack/sha1?
    - type: replace
      path: /releases/name=loggregator/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=loggregator/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=loggregator/sha1?
    - type: replace
      path: /releases/name=nats/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=nats/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=nats/sha1?
    - type: replace
      path: /releases/name=nginx-buildpack/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=nginx-buildpack/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=nginx-buildpack/sha1?
    - type: replace
      path: /releases/name=r-buildpack/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=r-buildpack/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=r-buildpack/sha1?
    - type: replace
      path: /releases/name=nodejs-buildpack/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=nodejs-buildpack/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=nodejs-buildpack/sha1?
    - type: replace
      path: /releases/name=php-buildpack/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=php-buildpack/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=php-buildpack/sha1?
    - type: replace
      path: /releases/name=python-buildpack/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=python-buildpack/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=python-buildpack/sha1?
    - type: replace
      path: /releases/name=routing/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=routing/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=routing/sha1?
    - type: replace
      path: /releases/name=ruby-buildpack/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=ruby-buildpack/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=ruby-buildpack/sha1?
    - type: replace
      path: /releases/name=silk/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=silk/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=silk/sha1?
    - type: replace
      path: /releases/name=staticfile-buildpack/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=staticfile-buildpack/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=staticfile-buildpack/sha1?
    - type: replace
      path: /releases/name=statsd-injector/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=statsd-injector/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=statsd-injector/sha1?
    - type: replace
      path: /releases/name=uaa/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=uaa/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=uaa/sha1?
    - type: replace
      path: /releases/name=loggregator-agent/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=loggregator-agent/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=loggregator-agent/sha1?
    - type: replace
      path: /releases/name=log-cache/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=log-cache/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=log-cache/sha1?
    - type: replace
      path: /releases/name=cf-cli/url?
      value: docker.io/cfcontainerization
    - type: replace
      path: /releases/name=cf-cli/stemcell?
      value: {"os":"SLE_15_SP1","version":"25.2-7.0.0_374.gb8e8e6af"}
    - type: remove
      path: /releases/name=cf-cli/sha1?
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-set-deployment-name
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    - type: replace
      path: /name
      value: kubecf
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-set-opensuse-stemcells
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |-
    # This ops file sets the openSUSE stemcells.

    - type: replace
      path: /stemcells/alias=default
      value:
        alias: default
        os: "SLE_15_SP1"
        version: "25.2-7.0.0_374.gb8e8e6af"

    - type: replace
      path: /addons/name=loggregator_agent/include/stemcell/os=ubuntu-xenial/os
      value: "SLE_15_SP1"

    - type: replace
      path: /addons/name=forwarder_agent/include/stemcell/os=ubuntu-xenial/os
      value: "SLE_15_SP1"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ops-sizing
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: |2-

    - type: replace
      path: /instance_groups/name=adapter/instances
      value: 1
    - type: replace
      path: /instance_groups/name=api/instances
      value: 1
    - type: replace
      path: /instance_groups/name=cc-worker/instances
      value: 1
    - type: replace
      path: /instance_groups/name=diego-api/instances
      value: 1
    - type: replace
      path: /instance_groups/name=doppler/instances
      value: 1
    - type: replace
      path: /instance_groups/name=log-api/instances
      value: 1
    - type: replace
      path: /instance_groups/name=nats/instances
      value: 1
    - type: replace
      path: /instance_groups/name=router/instances
      value: 1
    - type: replace
      path: /instance_groups/name=scheduler/instances
      value: 1
    - type: replace
      path: /instance_groups/name=uaa/instances
      value: 1
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-provided-properties
  namespace: default
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
data:
  ops: ""
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: eirini-nodes-policy
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  - nodes/proxy
  verbs:
  - get
  - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: default:eirini-webhook
rules:
- apiGroups:
  - admissionregistration.k8s.io
  resources:
  - mutatingwebhookconfigurations
  verbs:
  - '*'
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: eirini-metrics
subjects:
- kind: ServiceAccount
  name: eirini-metrics
  namespace: default
roleRef:
  kind: ClusterRole
  name: eirini-nodes-policy
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: default:eirini-webhook
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: default:eirini-webhook
subjects:
- kind: ServiceAccount
  name: eirinix
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: bits-service-psp
  namespace: default
rules:
- apiGroups:
  - policy
  resources:
  - podsecuritypolicies
  verbs:
  - use
  resourceNames:
  - bits-service
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: eirini-app-role
  namespace: eirini
rules:
- apiGroups:
  - policy
  resources:
  - podsecuritypolicies
  verbs:
  - use
  resourceNames:
  - eirini-app-psp
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: eirini-role
  namespace: eirini
rules:
- apiGroups:
  - batch
  resources:
  - jobs
  verbs:
  - create
  - delete
  - list
- apiGroups:
  - apps
  resources:
  - statefulsets
  verbs:
  - create
  - update
  - delete
  - list
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - list
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - delete
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - create
  - delete
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - create
  - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: eirini-role
  namespace: default
rules:
- apiGroups:
  - policy
  resources:
  - podsecuritypolicies
  verbs:
  - use
  resourceNames:
  - eirini
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: eirini-events
  namespace: eirini
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: eirini-events-psp
  namespace: default
rules:
- apiGroups:
  - policy
  resources:
  - podsecuritypolicies
  verbs:
  - use
  resourceNames:
  - eirini-events
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: eirini-metrics
  namespace: eirini
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
- apiGroups:
  - metrics.k8s.io
  resources:
  - pods
  verbs:
  - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: eirini-metrics-psp
  namespace: default
rules:
- apiGroups:
  - policy
  resources:
  - podsecuritypolicies
  verbs:
  - use
  resourceNames:
  - eirini-metrics
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: eirini-routing
  namespace: eirini
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - statefulsets
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: eirini-routing-psp
  namespace: default
rules:
- apiGroups:
  - policy
  resources:
  - podsecuritypolicies
  verbs:
  - use
  resourceNames:
  - eirini-routing
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: eirini-staging-reporter
  namespace: eirini
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: eirini-staging-reporter-psp
  namespace: default
rules:
- apiGroups:
  - policy
  resources:
  - podsecuritypolicies
  verbs:
  - use
  resourceNames:
  - eirini-staging-reporter
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: eirinix:all
  namespace: eirini
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kubecf-default-psp
  namespace: default
rules:
- apiGroups:
  - policy
  resourceNames:
  - kubecf-default
  resources:
  - podsecuritypolicies
  verbs:
  - use
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: bits-service-psp
  namespace: default
roleRef:
  kind: Role
  name: bits-service-psp
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: bits-service
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: eirini-app-rolebinding
  namespace: eirini
roleRef:
  kind: Role
  name: eirini-app-role
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: eirini
  namespace: eirini
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: eirini-rolebinding
  namespace: eirini
roleRef:
  kind: Role
  name: eirini-role
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: opi
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: eirini-rolebinding
  namespace: default
roleRef:
  kind: Role
  name: eirini-role
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: opi
  namespace: default
- kind: ServiceAccount
  name: eirini-rootfs-patcher
  namespace: default
- kind: ServiceAccount
  name: eirini-secret-smuggler
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: eirini-events
  namespace: eirini
roleRef:
  kind: Role
  name: eirini-events
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: eirini-events
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: eirini-events-psp
  namespace: default
roleRef:
  kind: Role
  name: eirini-events-psp
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: eirini-events
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: eirini-metrics
  namespace: eirini
roleRef:
  kind: Role
  name: eirini-metrics
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: eirini-metrics
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: eirini-metrics-psp
  namespace: default
roleRef:
  kind: Role
  name: eirini-metrics-psp
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: eirini-metrics
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: eirini-routing
  namespace: eirini
roleRef:
  kind: Role
  name: eirini-routing
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: eirini-routing
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: eirini-routing-psp
  namespace: default
roleRef:
  kind: Role
  name: eirini-routing-psp
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: eirini-routing
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: eirini-staging-reporter
  namespace: eirini
roleRef:
  kind: Role
  name: eirini-staging-reporter
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: eirini-staging-reporter
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: eirini-staging-reporter-psp
  namespace: default
roleRef:
  kind: Role
  name: eirini-staging-reporter-psp
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: eirini-staging-reporter
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: eirinix:all
  namespace: eirini
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: eirinix:all
subjects:
- kind: ServiceAccount
  name: eirinix
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubecf-default-psp
  namespace: default
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
roleRef:
  kind: Role
  name: kubecf-default-psp
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: default
  namespace: default
---
apiVersion: v1
kind: Service
metadata:
  name: bits
  annotations: null
spec:
  ports:
  - port: 8888
    protocol: TCP
    targetPort: 8888
    name: bits
  selector:
    name: bits
---
apiVersion: v1
kind: Service
metadata:
  name: eirini-opi
  namespace: default
spec:
  ports:
  - port: 8085
    protocol: TCP
    name: https
  selector:
    name: eirini
---
apiVersion: v1
kind: Service
metadata:
  name: eirini-persi-broker
  namespace: default
  labels:
    app.kubernetes.io/component: persi-broker
    app.kubernetes.io/name: eirinix
    helm.sh/chart: eirinix-0.0.0-10_g3df5bb4
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: eirinix
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/component: persi-broker
  ports:
  - protocol: TCP
    name: http
    port: 8999
    targetPort: 8999
---
apiVersion: v1
kind: Service
metadata:
  name: eirini-persi
  namespace: default
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: eirinix
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/component: persi
  ports:
  - protocol: TCP
    name: https
    port: 443
    targetPort: 8443
---
apiVersion: v1
kind: Service
metadata:
  name: eirinix-ssh-proxy
  namespace: default
  labels:
    app.kubernetes.io/component: ssh-proxy
    app.kubernetes.io/name: eirinix
    helm.sh/chart: eirinix-0.0.0-10_g3df5bb4
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    app.kubernetes.io/name: eirinix
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/component: ssh-proxy
  ports:
  - name: ssh
    protocol: TCP
    port: 2222
    targetPort: 2222
  type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  name: eirinix-ssh
  namespace: default
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: eirinix
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/component: ssh
  ports:
  - protocol: TCP
    name: https
    port: 443
    targetPort: 2999
---
apiVersion: v1
kind: Service
metadata:
  name: database
  namespace: default
  labels:
    app: database
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
spec:
  ports:
  - name: mysql
    port: 3306
    targetPort: mysql
  selector:
    app: database
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    quarks.cloudfoundry.org/pod-active: active
---
apiVersion: v1
kind: Service
metadata:
  name: database-repl
  namespace: default
  labels:
    app: database
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
spec:
  clusterIP: None
  ports:
  - name: galera
    port: 4567
  - name: state-xfer
    port: 4568
  - name: state-snap
    port: 4444
  selector:
    app: database
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
---
apiVersion: v1
kind: Service
metadata:
  name: eirini-registry
  namespace: default
spec:
  type: NodePort
  selector:
    name: bits
  ports:
  - protocol: TCP
    port: 6666
    targetPort: 6666
    nodePort: 31666
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bits
spec:
  replicas: 1
  selector:
    matchLabels:
      name: bits
  template:
    metadata:
      labels:
        name: bits
    spec:
      dnsPolicy: ClusterFirst
      volumes:
      - name: bits-config
        secret:
          secretName: bits
          items:
          - key: bits-config-key
            path: bits-service.yml
      - name: bits-cert
        secret:
          secretName: bits-service-ssl
      - name: bits-assets
        emptyDir: {}
      containers:
      - name: bits
        image: registry.suse.com/cap-staging/bits-service:bits-1.0.17-15.1.6.2.251-24.20
        imagePullPolicy: Always
        ports:
        - containerPort: 8888
        env:
        - name: BITS_BLOBSTORE_PASSWORD
          valueFrom:
            secretKeyRef:
              key: password
              name: var-blobstore-admin-users-password
        volumeMounts:
        - name: bits-config
          mountPath: /workspace/jobs/bits-service/config
        - name: bits-cert
          mountPath: /workspace/jobs/bits-service/certs
        resources:
          requests:
            cpu: 800m
            memory: 150Mi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: eirini
  namespace: default
spec:
  selector:
    matchLabels:
      name: eirini
  template:
    metadata:
      labels:
        name: eirini
        eirinifs_version: v75.0.0
    spec:
      dnsPolicy: ClusterFirst
      serviceAccountName: opi
      volumes:
      - name: config-map-volume
        configMap:
          name: eirini
          items:
          - key: opi.yml
            path: opi.yml
      - name: cf-secrets
        projected:
          sources:
          - secret:
              name: var-eirini-tls-client-cert
              items:
              - key: certificate
                path: cc.crt
              - key: private_key
                path: cc.key
          - secret:
              name: var-eirini-tls-server-cert
              items:
              - key: ca
                path: cc.ca
          - secret:
              name: var-eirini-tls-server-cert
              items:
              - key: certificate
                path: eirini-server.crt
              - key: private_key
                path: eirini-server.key
          - secret:
              name: var-eirini-tls-server-cert
              items:
              - key: ca
                path: eirini.ca
      securityContext:
        runAsNonRoot: true
      containers:
      - name: opi
        image: registry.suse.com/cap-staging/opi:1.5.0
        imagePullPolicy: Always
        volumeMounts:
        - name: config-map-volume
          mountPath: /workspace/jobs/opi/config
        - name: cf-secrets
          mountPath: /workspace/jobs/opi/secrets
        ports:
        - containerPort: 8085
          name: https
        resources:
          requests:
            cpu: 20m
            memory: 20Mi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: eirini-events
  namespace: default
spec:
  selector:
    matchLabels:
      name: eirini-events
  template:
    metadata:
      labels:
        name: eirini-events
    spec:
      dnsPolicy: ClusterFirst
      serviceAccountName: eirini-events
      volumes:
      - name: config-map-volume
        configMap:
          name: eirini
          items:
          - key: events.yml
            path: events.yml
      - name: cf-secrets
        projected:
          sources:
          - secret:
              name: var-cc-tls
              items:
              - key: certificate
                path: cc.crt
              - key: private_key
                path: cc.key
          - secret:
              name: var-cc-tls
              items:
              - key: ca
                path: cc.ca
      securityContext:
        runAsNonRoot: true
      containers:
      - name: event-reporter
        image: registry.suse.com/cap-staging/event-reporter:1.5.0
        imagePullPolicy: Always
        volumeMounts:
        - name: config-map-volume
          mountPath: /etc/eirini/config
        - name: cf-secrets
          mountPath: /etc/eirini/secrets
        resources:
          requests:
            cpu: 15m
            memory: 15Mi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: eirini-metrics
  namespace: default
spec:
  selector:
    matchLabels:
      name: eirini-metrics
  template:
    metadata:
      labels:
        name: eirini-metrics
    spec:
      dnsPolicy: ClusterFirst
      serviceAccountName: eirini-metrics
      volumes:
      - name: config-map-volume
        configMap:
          name: eirini
          items:
          - key: metrics.yml
            path: metrics.yml
      - name: cf-secrets
        projected:
          sources:
          - secret:
              name: var-loggregator-tls-doppler
              items:
              - key: certificate
                path: doppler.crt
              - key: private_key
                path: doppler.key
          - secret:
              name: var-loggregator-tls-doppler
              items:
              - key: ca
                path: doppler.ca
      securityContext:
        runAsNonRoot: true
      containers:
      - name: metrics-collector
        image: registry.suse.com/cap-staging/metrics-collector:1.5.0
        imagePullPolicy: Always
        volumeMounts:
        - name: config-map-volume
          mountPath: /etc/eirini/config
        - name: cf-secrets
          mountPath: /etc/eirini/secrets
        resources:
          requests:
            cpu: 15m
            memory: 15Mi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: eirini-routing
  namespace: default
spec:
  selector:
    matchLabels:
      name: eirini-routing
  template:
    metadata:
      labels:
        name: eirini-routing
    spec:
      dnsPolicy: ClusterFirst
      serviceAccountName: eirini-routing
      volumes:
      - name: config-map-volume
        configMap:
          name: eirini
          items:
          - key: routing.yml
            path: routing.yml
      securityContext:
        runAsNonRoot: true
      containers:
      - name: route-collector
        image: registry.suse.com/cap-staging/route-collector:1.5.0
        imagePullPolicy: Always
        env:
        - name: NATS_PASSWORD
          valueFrom:
            secretKeyRef:
              key: password
              name: var-nats-password
        volumeMounts:
        - name: config-map-volume
          mountPath: /etc/eirini/
        resources:
          requests:
            cpu: 30m
            memory: 45Mi
      - name: route-pod-informer
        image: registry.suse.com/cap-staging/route-pod-informer:1.5.0
        imagePullPolicy: Always
        env:
        - name: NATS_PASSWORD
          valueFrom:
            secretKeyRef:
              key: password
              name: var-nats-password
        volumeMounts:
        - name: config-map-volume
          mountPath: /etc/eirini/
        resources:
          requests:
            cpu: 30m
            memory: 45Mi
      - name: route-statefulset-informer
        image: registry.suse.com/cap-staging/route-statefulset-informer:1.5.0
        imagePullPolicy: Always
        env:
        - name: NATS_PASSWORD
          valueFrom:
            secretKeyRef:
              key: password
              name: var-nats-password
        volumeMounts:
        - name: config-map-volume
          mountPath: /etc/eirini/
        resources:
          requests:
            cpu: 30m
            memory: 45Mi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: eirini-staging-reporter
  namespace: default
spec:
  selector:
    matchLabels:
      name: eirini-staging-reporter
  template:
    metadata:
      labels:
        name: eirini-staging-reporter
    spec:
      dnsPolicy: ClusterFirst
      serviceAccountName: eirini-staging-reporter
      securityContext:
        runAsNonRoot: true
      containers:
      - name: staging-reporter
        image: registry.suse.com/cap-staging/staging-reporter:1.5.0
        imagePullPolicy: Always
        resources:
          requests:
            cpu: 15m
            memory: 15Mi
        volumeMounts:
        - name: config-map-volume
          mountPath: /etc/eirini/config
        - name: cf-secrets
          mountPath: /etc/eirini/secrets
      volumes:
      - name: config-map-volume
        configMap:
          name: eirini
          items:
          - key: staging-reporter.yml
            path: staging-reporter.yml
      - name: cf-secrets
        projected:
          sources:
          - secret:
              name: var-eirini-tls-client-cert
              items:
              - key: certificate
                path: eirini-client.crt
              - key: private_key
                path: eirini-client.key
              - key: ca
                path: eirini-client.ca
          - secret:
              name: var-cc-tls
              items:
              - key: ca
                path: cc.ca
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: loggregator-bridge
  labels:
    app.kubernetes.io/component: loggregator-bridge
    app.kubernetes.io/name: eirinix
    helm.sh/chart: eirinix-0.0.0-10_g3df5bb4
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: eirinix
      app.kubernetes.io/instance: kubecf
      app.kubernetes.io/component: loggregator-bridge
  template:
    metadata:
      labels:
        app.kubernetes.io/name: eirinix
        app.kubernetes.io/instance: kubecf
        app.kubernetes.io/component: loggregator-bridge
    spec:
      serviceAccountName: eirinix
      containers:
      - name: loggregator-bridge
        image: splatform/eirinix-loggregator-bridge:v0.0.0-0.gfd70cac
        imagePullPolicy: IfNotPresent
        env:
        - name: EIRINI_LOGGREGATOR_BRIDGE_LOGLEVEL
          value: DEBUG
        - name: LOGGREGATOR_CERT_PATH
          value: /run/secrets/loggregator-cert/certificate
        - name: LOGGREGATOR_KEY_PATH
          value: /run/secrets/loggregator-cert/private_key
        - name: LOGGREGATOR_CA_PATH
          value: /run/secrets/loggregator-ca/certificate
        - name: NAMESPACE
          value: eirini
        - name: LOGGREGATOR_ENDPOINT
          value: doppler:8082
        volumeMounts:
        - name: loggregator-ca
          mountPath: /run/secrets/loggregator-ca
        - name: loggregator-cert
          mountPath: /run/secrets/loggregator-cert
        resources: {}
      volumes:
      - name: config
        emptyDir: {}
      - name: loggregator-ca
        secret:
          secretName: var-loggregator-ca
          items:
          - key: certificate
            path: certificate
      - name: loggregator-cert
        secret:
          secretName: var-loggregator-tls-agent
          items:
          - key: certificate
            path: certificate
          - key: private_key
            path: private_key
      nodeSelector: {}
      affinity: {}
      tolerations: []
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: persi-broker
  labels:
    app.kubernetes.io/component: persi-broker
    app.kubernetes.io/name: eirinix
    helm.sh/chart: eirinix-0.0.0-10_g3df5bb4
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: eirinix
      app.kubernetes.io/instance: kubecf
      app.kubernetes.io/component: persi-broker
  template:
    metadata:
      labels:
        app.kubernetes.io/name: eirinix
        app.kubernetes.io/instance: kubecf
        app.kubernetes.io/component: persi-broker
    spec:
      serviceAccountName: eirinix
      initContainers:
      - name: generate-config
        image: splatform/eirinix-persi-broker-setup:v0.0.0-ge2799d8
        imagePullPolicy: IfNotPresent
        env:
        - name: SERVICE_CONFIG
          value: |
            service:
              service_name: eirini-persi
              service_id: eirini-persi
              plans:
                - plan_id: default
                  plan_name: default
                  description: Existing default storage class
                  kube_storage_class: default
                  free: true
                  deault_size: 1Gi
              description: Eirini persistence broker
              long_description: Eirini persistence broker to provide Kubernete storage classes
              provider_display_name: Eirini broker
              documentation_url: https://github.com/SUSE/eirini-persi-broker
              support_url: https://github.com/SUSE/eirini-persi-broker/issues
              display_name: Eirini broker
              icon_image: Eirini broker
        - name: NAMESPACE
          value: eirini
        volumeMounts:
        - name: config
          mountPath: /run/secrets/config
        - name: auth-password
          mountPath: /run/secrets/auth-password
      containers:
      - name: persi-broker
        image: splatform/eirinix-persi-broker:v0.0.0-g47355df
        imagePullPolicy: IfNotPresent
        env:
        - name: BROKER_CONFIG_PATH
          value: /run/secrets/config/eirini-persi-broker.yml
        volumeMounts:
        - name: config
          mountPath: /run/secrets/config
        resources: {}
      volumes:
      - name: config
        emptyDir: {}
      - name: auth-password
        secret:
          secretName: persi-broker-auth-password
      nodeSelector: {}
      affinity: {}
      tolerations: []
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: persi
  labels:
    app.kubernetes.io/component: persi
    app.kubernetes.io/name: eirinix
    helm.sh/chart: eirinix-0.0.0-10_g3df5bb4
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: eirinix
      app.kubernetes.io/instance: kubecf
      app.kubernetes.io/component: persi
  template:
    metadata:
      labels:
        app.kubernetes.io/name: eirinix
        app.kubernetes.io/instance: kubecf
        app.kubernetes.io/component: persi
    spec:
      serviceAccountName: eirinix
      containers:
      - name: persi
        image: splatform/eirinix-persi:v0.0.0-67.ge68b3d3
        imagePullPolicy: IfNotPresent
        args:
        - start
        env:
        - name: OPERATOR_SERVICE_NAME
          value: eirini-persi
        - name: OPERATOR_WEBHOOK_NAMESPACE
          value: default
        - name: OPERATOR_WEBHOOK_HOST
          value: 0.0.0.0
        - name: OPERATOR_WEBHOOK_PORT
          value: "8443"
        - name: NAMESPACE
          value: eirini
        resources: {}
      nodeSelector: {}
      affinity: {}
      tolerations: []
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ssh-proxy
  labels:
    app.kubernetes.io/component: ssh-proxy
    app.kubernetes.io/name: eirinix
    helm.sh/chart: eirinix-0.0.0-10_g3df5bb4
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: eirinix
      app.kubernetes.io/instance: kubecf
      app.kubernetes.io/component: ssh-proxy
  template:
    metadata:
      labels:
        app.kubernetes.io/name: eirinix
        app.kubernetes.io/instance: kubecf
        app.kubernetes.io/component: ssh-proxy
    spec:
      serviceAccountName: eirinix
      initContainers:
      - name: generate-config
        image: splatform/eirinix-ssh-proxy-setup:v0.0.0-ge2799d8
        imagePullPolicy: IfNotPresent
        env:
        - name: CC_API_URL
          value: https://api:9024
        - name: UAA_TOKEN_URL
          value: https://uaa:8443/oauth/token
        volumeMounts:
        - name: config
          mountPath: /run/secrets/config
        - name: uaa-client
          mountPath: /run/secrets/uaa-client-password
          subPath: password
        - name: ssh-proxy-host-key
          mountPath: /run/secrets/ssh-proxy-host-key.key
          subPath: proxy.key
      containers:
      - name: ssh-proxy
        image: splatform/eirinix-ssh:v0.0.0-0.ge2ed1ab
        imagePullPolicy: IfNotPresent
        command:
        - /bin/ssh-proxy
        args:
        - --config
        - /run/secrets/config/eirini-ssh-proxy.json
        env:
        - name: SSH_PROXY_KUBERNETES_NAMESPACE
          value: eirini
        - name: SSH_PROXY_DAEMON_PORT
          value: "2222"
        volumeMounts:
        - name: config
          mountPath: /run/secrets/config
        - name: uaa-ca-cert
          mountPath: /run/secrets/uaa-ca.crt
          subPath: ca.crt
        - name: cc-api-ca-cert
          mountPath: /run/secrets/cc-api-ca.crt
          subPath: ca.crt
        resources: {}
      volumes:
      - name: config
        emptyDir: {}
      - name: uaa-client
        secret:
          secretName: var-uaa-clients-ssh-proxy-secret
          items:
          - key: password
            path: password
      - name: ssh-proxy-host-key
        secret:
          secretName: var-diego-ssh-proxy-host-key
          items:
          - key: private_key
            path: proxy.key
      - name: uaa-ca-cert
        secret:
          secretName: var-uaa-ca
          items:
          - key: certificate
            path: ca.crt
      - name: cc-api-ca-cert
        secret:
          secretName: var-cc-public-tls
          items:
          - key: certificate
            path: ca.crt
      nodeSelector: {}
      affinity: {}
      tolerations: []
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ssh
  labels:
    app.kubernetes.io/component: ssh
    app.kubernetes.io/name: eirinix
    helm.sh/chart: eirinix-0.0.0-10_g3df5bb4
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: eirinix
      app.kubernetes.io/instance: kubecf
      app.kubernetes.io/component: ssh
  template:
    metadata:
      labels:
        app.kubernetes.io/name: eirinix
        app.kubernetes.io/instance: kubecf
        app.kubernetes.io/component: ssh
    spec:
      serviceAccountName: eirinix
      containers:
      - name: ssh
        image: splatform/eirinix-ssh:v0.0.0-0.g04a3871
        imagePullPolicy: IfNotPresent
        command:
        - /bin/ssh-extension
        args:
        - start
        env:
        - name: OPERATOR_WEBHOOK_HOST
          value: 0.0.0.0
        - name: OPERATOR_WEBHOOK_PORT
          value: "2999"
        - name: EIRINI_EXTENSION_NAMESPACE
          value: eirini
        - name: OPERATOR_SERVICE_NAME
          value: eirinix-ssh
        - name: OPERATOR_WEBHOOK_NAMESPACE
          value: default
        resources: {}
      nodeSelector: {}
      affinity: {}
      tolerations: []
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: bits-registry
  labels:
    release: kubecf
    heritage: Helm
  annotations: {}
spec:
  tls:
  - hosts:
    - registry.vcap.me
    secretName: bits-service-ssl
  rules:
  - host: registry.vcap.me
    http:
      paths:
      - path: /
        backend:
          serviceName: bits
          servicePort: 8888
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubecf
  namespace: default
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/backend-protocol: HTTPS
    nginx.ingress.kubernetes.io/proxy-body-size: 64m
    nginx.ingress.kubernetes.io/secure-backends: "true"
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.org/websocket-services: router
  labels:
    app.kubernetes.io/component: ingress
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
spec:
  tls:
  - secretName: kubecf-ingress-tls
    hosts:
    - '*.kubecf.vcap.me'
    - kubecf.vcap.me
  rules:
  - host: '*.kubecf.vcap.me'
    http:
      paths:
      - path: /
        backend:
          serviceName: router
          servicePort: 443
  - host: kubecf.vcap.me
    http:
      paths:
      - path: /
        backend:
          serviceName: router
          servicePort: 443
---
apiVersion: quarks.cloudfoundry.org/v1alpha1
kind: BOSHDeployment
metadata:
  name: kubecf
  namespace: default
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
spec:
  manifest:
    name: cf-deployment
    type: configmap
  vars: []
  ops:
  - name: ops-move-auctioneer
    type: configmap
  - name: ops-move-log-cache
    type: configmap
  - name: ops-move-routing-api
    type: configmap
  - name: ops-set-suse-buildpacks
    type: configmap
  - name: ops-acceptance-tests
    type: configmap
  - name: ops-adapter
    type: configmap
  - name: ops-api
    type: configmap
  - name: ops-app-autoscaler
    type: configmap
  - name: ops-auctioneer
    type: configmap
  - name: ops-brain-tests
    type: configmap
  - name: ops-cc-worker
    type: configmap
  - name: ops-credhub
    type: configmap
  - name: ops-database
    type: configmap
  - name: ops-diego-api
    type: configmap
  - name: ops-diego-cell
    type: configmap
  - name: ops-doppler
    type: configmap
  - name: ops-eirini-helm
    type: configmap
  - name: ops-log-api
    type: configmap
  - name: ops-log-cache
    type: configmap
  - name: ops-nats
    type: configmap
  - name: ops-router
    type: configmap
  - name: ops-routing-api
    type: configmap
  - name: ops-scheduler
    type: configmap
  - name: ops-singleton-blobstore
    type: configmap
  - name: ops-smoke-tests
    type: configmap
  - name: ops-sync-integration-tests
    type: configmap
  - name: ops-tcp-router
    type: configmap
  - name: ops-uaa
    type: configmap
  - name: ops-sizing
    type: configmap
  - name: ops-azs
    type: configmap
  - name: ops-addons
    type: configmap
  - name: ops-azs
    type: configmap
  - name: ops-releases
    type: configmap
  - name: ops-set-deployment-name
    type: configmap
  - name: ops-set-opensuse-stemcells
    type: configmap
  - name: ops-sizing
    type: configmap
  - name: user-provided-properties
    type: configmap
---
apiVersion: quarks.cloudfoundry.org/v1alpha1
kind: QuarksJob
metadata:
  name: database-seeder
  namespace: default
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
spec:
  trigger:
    strategy: once
  updateOnConfigChange: true
  template:
    spec:
      template:
        spec:
          containers:
          - name: seeder
            image: docker.io/cfcontainerization/pxc:0.9.4
            imagePullPolicy: null
            env:
            - name: DATABASE_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: var-pxc-root-password
                  key: password
            - name: DATABASE_HOST
              value: database.default
            - name: CHARACTER_SET
              value: utf8
            - name: COLLATE
              value: utf8_unicode_ci
            - name: DATABASES
              value: |-
                cloud_controller
                diego
                network_connectivity
                network_policy
                uaa
                locket
            volumeMounts:
            - name: cloud-controller-database-password
              mountPath: /passwords/cloud_controller
              readOnly: true
            - name: diego-database-password
              mountPath: /passwords/diego
              readOnly: true
            - name: network-connectivity-database-password
              mountPath: /passwords/network_connectivity
              readOnly: true
            - name: network-policy-database-password
              mountPath: /passwords/network_policy
              readOnly: true
            - name: uaa-database-password
              mountPath: /passwords/uaa
              readOnly: true
            - name: locket-database-password
              mountPath: /passwords/locket
              readOnly: true
            command:
            - /bin/bash
            - -c
            - |-
              #!/usr/bin/env bash

              set -o errexit -o nounset -o pipefail

              cat > "${HOME}/.my.cnf" <<EOF
              [mysql]
              host=${DATABASE_HOST}
              user=root
              password=${DATABASE_ROOT_PASSWORD}
              EOF

              echo "Waiting for database to be ready..."
              until echo "SELECT 'Ready!'" | mysql --connect-timeout=3 1> /dev/null 2> /dev/null; do
                sleep 1
              done

              mysql < <(
                echo "START TRANSACTION;"
                echo "\
                  CREATE DATABASE IF NOT EXISTS kubecf
                    DEFAULT CHARACTER SET ${CHARACTER_SET}
                    DEFAULT COLLATE ${COLLATE};
                  USE kubecf;
                  CREATE TABLE IF NOT EXISTS db_leader_election (
                    anchor tinyint(3) unsigned NOT NULL,
                    host varchar(128) NOT NULL,
                    last_seen_active timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    PRIMARY KEY (anchor)
                  );"
                for database in ${DATABASES}; do
                  if [[ -z "${database}" ]]; then continue; fi

                  password=$(</passwords/"${database}"/password)

                  echo "\
                    CREATE USER IF NOT EXISTS \`${database}\`;
                    ALTER USER \`${database}\` IDENTIFIED BY '${password}';

                    CREATE DATABASE IF NOT EXISTS \`${database}\`
                      DEFAULT CHARACTER SET ${CHARACTER_SET}
                      DEFAULT COLLATE ${COLLATE};

                    GRANT ALL ON \`${database}\`.* TO '${database}'@'%';
                  "
                done
                echo "COMMIT;"
              )
              echo "Done!"
          volumes:
          - name: cloud-controller-database-password
            secret:
              secretName: var-cc-database-password
          - name: diego-database-password
            secret:
              secretName: var-diego-database-password
          - name: network-connectivity-database-password
            secret:
              secretName: var-network-connectivity-database-password
          - name: network-policy-database-password
            secret:
              secretName: var-network-policy-database-password
          - name: uaa-database-password
            secret:
              secretName: var-uaa-database-password
          - name: locket-database-password
            secret:
              secretName: var-locket-database-password
          restartPolicy: Never
---
apiVersion: quarks.cloudfoundry.org/v1alpha1
kind: QuarksJob
metadata:
  name: database-migrate-charset
  namespace: default
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
spec:
  trigger:
    strategy: manual
  updateOnConfigChange: true
  template:
    spec:
      template:
        spec:
          containers:
          - name: migrate-charset
            image: docker.io/cfcontainerization/pxc:0.9.4
            imagePullPolicy: null
            env:
            - name: DATABASE_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: var-pxc-root-password
                  key: password
            - name: DATABASE_HOST
              value: database.default
            - name: CHARACTER_SET
              value: utf8
            - name: COLLATE
              value: utf8_unicode_ci
            - name: DATABASES
              value: |-
                cloud_controller
                diego
                network_connectivity
                network_policy
                uaa
                locket
            volumeMounts:
            - name: cloud-controller-database-password
              mountPath: /passwords/cloud_controller
              readOnly: true
            - name: diego-database-password
              mountPath: /passwords/diego
              readOnly: true
            - name: network-connectivity-database-password
              mountPath: /passwords/network_connectivity
              readOnly: true
            - name: network-policy-database-password
              mountPath: /passwords/network_policy
              readOnly: true
            - name: uaa-database-password
              mountPath: /passwords/uaa
              readOnly: true
            - name: locket-database-password
              mountPath: /passwords/locket
              readOnly: true
            command:
            - /bin/bash
            - -c
            - |-
              #!/usr/bin/env bash

              # This script migrates the character set and collate of the KubeCF databases. This was originally
              # introduced to migrate from latin1 to utf8 on existing installations up to v2.1.0.

              set -o errexit -o nounset -o pipefail

              cat > "${HOME}/.my.cnf" <<EOF
              [mysql]
              host=${DATABASE_HOST}
              user=root
              password=${DATABASE_ROOT_PASSWORD}
              EOF

              echo "Waiting for database to be ready..."
              until echo "SELECT 'Ready!'" | mysql --connect-timeout=3 1> /dev/null 2> /dev/null; do
                sleep 1
              done

              function show_tables() {
                local database=$1
                mysql \
                  --database="${database}" \
                  --batch \
                  --skip-column-names \
                  --execute "SHOW TABLES"
              }

              function show_columns() {
                local database=$1
                local table=$2
                mysql \
                  --database="${database}" \
                  --batch \
                  --skip-column-names \
                  --execute "SHOW COLUMNS FROM ${table}"
              }

              function alter_tables() {
                local database=$1

                while read -r table; do
                  >&2 echo "Generating statements for \`${database}\`.\`${table}\`..."

                  echo "ALTER TABLE \`${table}\` DEFAULT CHARACTER SET ${CHARACTER_SET};"

                  while read -r column; do
                    field_type=$(awk --field-separator="\t" '{ print toupper($2) }' <<<"${column}")

                    awk_program='/^(CHAR|VARCHAR|TINYTEXT|TEXT|MEDIUMTEXT|LONGTEXT|ENUM|SET)/ { print "modify" }'
                    if [[ "$(awk "${awk_program}" <<<"${field_type}")" == "modify" ]]; then
                      field_name=$(awk --field-separator="\t" '{ print $1 }' <<<"${column}")
                      null_opt=$(awk --field-separator="\t" '{ print ($3 == "YES") ? "NULL" : "NOT NULL"}' <<<"${column}")
                      field_default=$(awk --field-separator="\t" '{ print $5 }' <<<"${column}")
                      if [[ "${field_default}" == "NULL" ]]; then
                        if [[ "${null_opt}" == "NOT NULL" ]]; then
                          default_opt=""
                        else
                          default_opt="DEFAULT NULL"
                        fi
                      else
                        default_opt="DEFAULT '${field_default}'"
                      fi

                      echo "ALTER TABLE \`${table}\` MODIFY \`${field_name}\` ${field_type} CHARACTER SET ${CHARACTER_SET} ${null_opt} ${default_opt};"
                    fi
                  done < <(show_columns "${database}" "${table}")
                done < <(show_tables "${database}")
              }

              function get_charset() {
                local database=$1
                mysql \
                  --database="${database}" \
                  --batch \
                  --skip-column-names \
                  --execute "SELECT default_character_set_name FROM information_schema.SCHEMATA WHERE schema_name = '${database}';"
              }

              function alter_database() {
                local database=$1

                current_charset=$(get_charset "${database}")
                if [[ "${current_charset}" == "${CHARACTER_SET}" ]]; then
                  return 0
                fi

                echo "ALTER DATABASE \`${database}\` DEFAULT CHARACTER SET ${CHARACTER_SET} DEFAULT COLLATE ${COLLATE};"
                echo "USE \`${database}\`;"
                echo "SET sql_mode = 'NO_AUTO_VALUE_ON_ZERO';"
                echo "SET foreign_key_checks = 0;"

                alter_tables "${database}"

                echo "SET foreign_key_checks = 1;"
                echo -e "\n"
              }

              STATEMENT=$(
              echo "START TRANSACTION;"

              alter_database "kubecf"

              for database in ${DATABASES}; do
                if [[ -z "${database}" ]]; then continue; fi
                alter_database "${database}"
              done

              echo "COMMIT;"
              )

              mysql <<<"${STATEMENT}"
          volumes:
          - name: cloud-controller-database-password
            secret:
              secretName: var-cc-database-password
          - name: diego-database-password
            secret:
              secretName: var-diego-database-password
          - name: network-connectivity-database-password
            secret:
              secretName: var-network-connectivity-database-password
          - name: network-policy-database-password
            secret:
              secretName: var-network-policy-database-password
          - name: uaa-database-password
            secret:
              secretName: var-uaa-database-password
          - name: locket-database-password
            secret:
              secretName: var-locket-database-password
          restartPolicy: Never
---
apiVersion: quarks.cloudfoundry.org/v1alpha1
kind: QuarksSecret
metadata:
  name: var-pxc-root-password
  namespace: default
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
spec:
  type: password
  secretName: var-pxc-root-password
---
apiVersion: quarks.cloudfoundry.org/v1alpha1
kind: QuarksSecret
metadata:
  name: var-pxc-password
  namespace: default
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
spec:
  type: password
  secretName: var-pxc-password
---
apiVersion: quarks.cloudfoundry.org/v1alpha1
kind: QuarksSecret
metadata:
  name: var-xtrabackup-password
  namespace: default
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
spec:
  type: password
  secretName: var-xtrabackup-password
---
apiVersion: quarks.cloudfoundry.org/v1alpha1
kind: QuarksSecret
metadata:
  name: bits-service-ssl
spec:
  request:
    certificate:
      alternativeNames:
      - registry.127.0.0.1.nip.io
      commonName: bits
      isCA: false
      signerType: cluster
  secretName: bits-service-ssl
  type: certificate
---
apiVersion: quarks.cloudfoundry.org/v1alpha1
kind: QuarksStatefulSet
metadata:
  name: database
  namespace: default
  labels:
    app: database
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: v2.3.0
    helm.sh/chart: kubecf-v2.3.0
spec:
  updateOnConfigChange: true
  activePassiveProbes:
    database:
      periodSeconds: 5
      exec:
        command:
        - /bin/bash
        - -c
        - |
          #!/usr/bin/env bash

          leader=$(mysql -sN <<EOF
            USE kubecf;
            insert ignore into db_leader_election ( anchor, host, last_seen_active )
              values ( 1, '${HOSTNAME}', now() ) on duplicate key update host = if(last_seen_active < now() - interval 20 second,
              values(host), host), last_seen_active = if(host = values(host), values(last_seen_active), last_seen_active);
              select host from db_leader_election;
          EOF
          )

          # shellcheck disable=SC2181
          if [[ $? != 0 ]]; then
            # the kubecf database doesn't seem to be ready. make the first node the master
            [[ ${HOSTNAME} == *-0 ]] || exit 2
          else
            [[ "${leader}" == "${HOSTNAME}" ]]
          fi
  template:
    metadata:
      name: database
      namespace: default
      labels:
        app: database
        app.kubernetes.io/instance: kubecf
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kubecf
        app.kubernetes.io/version: v2.3.0
        helm.sh/chart: kubecf-v2.3.0
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: database
          app.kubernetes.io/instance: kubecf
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kubecf
          app.kubernetes.io/version: v2.3.0
          helm.sh/chart: kubecf-v2.3.0
      serviceName: database
      template:
        metadata:
          labels:
            app: database
            app.kubernetes.io/instance: kubecf
            app.kubernetes.io/managed-by: Helm
            app.kubernetes.io/name: kubecf
            app.kubernetes.io/version: v2.3.0
            helm.sh/chart: kubecf-v2.3.0
        spec:
          initContainers:
          - name: remove-lost-found
            image: docker.io/cfcontainerization/pxc:0.9.4
            imagePullPolicy: null
            command:
            - rm
            - -fr
            - /var/lib/mysql/lost+found
            volumeMounts:
            - name: pxc-data
              mountPath: /var/lib/mysql
          containers:
          - name: database
            image: docker.io/cfcontainerization/pxc:0.9.4
            imagePullPolicy: null
            command:
            - /bin/bash
            - /startup-scripts/entrypoint.sh
            env:
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: var-pxc-root-password
                  key: password
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: var-pxc-password
                  key: password
            - name: XTRABACKUP_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: var-xtrabackup-password
                  key: password
            - name: ALLOW_ROOT_FROM
              value: '%'
            - name: CLUSTER_NAME
              value: kubecf-database
            - name: SHORT_CLUSTER_NAME
              value: kubecf-database
            - name: K8S_SERVICE_NAME
              value: database-repl
            - name: PXC_STRICT_MODE
              value: ENFORCING
            ports:
            - name: mysql
              containerPort: 3306
            - name: galera-repl
              containerPort: 4567
            - name: state-transfer
              containerPort: 4568
            - name: state-snapshot
              containerPort: 4444
            livenessProbe:
              exec:
                command:
                - /bin/bash
                - -c
                - mysqladmin ping || test -e /var/lib/mysql/sst_in_progress
              initialDelaySeconds: 30
              timeoutSeconds: 2
            readinessProbe:
              exec:
                command:
                - mysql
                - -h
                - 127.0.0.1
                - -e
                - SELECT 1
              initialDelaySeconds: 30
              timeoutSeconds: 2
            volumeMounts:
            - name: pxc-tls
              mountPath: /etc/mysql/tls/certs
            - name: pxc-data
              mountPath: /var/lib/mysql
            - name: pxc-config-files
              mountPath: /etc/mysql/conf.d
            - name: pxc-startup-scripts
              mountPath: /startup-scripts
            - name: slash-root
              mountPath: /root
            - name: var-log
              mountPath: /var/log
          - name: logs
            image: docker.io/cfcontainerization/pxc:0.9.4
            imagePullPolicy: null
            command:
            - tail
            - -f
            - /var/log/mysqld.log
            volumeMounts:
            - name: var-log
              mountPath: /var/log
          volumes:
          - name: slash-root
            emptyDir: {}
          - name: var-log
            emptyDir: {}
          - name: pxc-config-files
            configMap:
              name: database-config-files
          - name: pxc-startup-scripts
            configMap:
              name: database-startup-scripts
          - name: pxc-tls
            secret:
              secretName: var-pxc-tls
      volumeClaimTemplates:
      - metadata:
          name: pxc-data
        spec:
          accessModes:
          - ReadWriteOnce
          storageClassName: null
          resources:
            requests:
              storage: 20Gi
