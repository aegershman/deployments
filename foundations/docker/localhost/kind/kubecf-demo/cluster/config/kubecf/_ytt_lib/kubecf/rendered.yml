---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
  name: kubecf-default
spec:
  allowPrivilegeEscalation: true
  allowedCapabilities:
  - NET_ADMIN
  - NET_BIND_SERVICE
  - NET_RAW
  - SYS_ADMIN
  - SYS_RESOURCE
  defaultAllowPrivilegeEscalation: true
  fsGroup:
    rule: RunAsAny
  hostPorts:
  - max: 65535
    min: 0
  privileged: true
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  volumes:
  - configMap
  - secret
  - emptyDir
  - downwardAPI
  - projected
  - persistentVolumeClaim
  - nfs
  - rbd
  - cephFS
  - glusterfs
  - fc
  - iscsi
  - cinder
  - gcePersistentDisk
  - awsElasticBlockStore
  - azureDisk
  - azureFile
  - vsphereVolume
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: var-system-domain
  namespace: default
stringData:
  value: kubecf.vcap.me
type: Opaque
---
apiVersion: v1
data:
  Corefile: |-
    . {
      errors
      health

      svcdiscovery {
        tls_ca_path /tls/ca.pem
        tls_client_cert_path /tls/cert.pem
        tls_client_key_path /tls/key.pem
        sdc_host service-discovery-controller.default.svc
        sdc_port 8054
        ttl 300
      }

      forward . /config/forward.conf

      cache 120
      loop
      reload
      loadbalance
    }
kind: ConfigMap
metadata:
  labels:
    app: apps-dns
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: apps-dns
  namespace: default
---
apiVersion: v1
data:
  manifest: |-
    addons:
    - exclude:
        jobs:
        - name: smoke_tests
          release: cf-smoke-tests
      include:
        stemcell:
        - os: ubuntu-xenial
      jobs:
      - name: loggregator_agent
        properties:
          disable_udp: true
          grpc_port: 3459
          loggregator:
            tls:
              agent:
                cert: ((loggregator_tls_agent.certificate))
                key: ((loggregator_tls_agent.private_key))
              ca_cert: ((loggregator_tls_agent.ca))
          metrics:
            ca_cert: ((loggregator_agent_metrics_tls.ca))
            cert: ((loggregator_agent_metrics_tls.certificate))
            key: ((loggregator_agent_metrics_tls.private_key))
            server_name: loggregator_agent_metrics
        release: loggregator-agent
      name: loggregator_agent
    - include:
        stemcell:
        - os: ubuntu-xenial
      jobs:
      - name: loggr-forwarder-agent
        properties:
          metrics:
            ca_cert: ((forwarder_agent_metrics_tls.ca))
            cert: ((forwarder_agent_metrics_tls.certificate))
            key: ((forwarder_agent_metrics_tls.private_key))
            server_name: forwarder_agent_metrics
          tls:
            ca_cert: ((loggregator_tls_agent.ca))
            cert: ((loggregator_tls_agent.certificate))
            key: ((loggregator_tls_agent.private_key))
        release: loggregator-agent
      name: forwarder_agent
    - exclude:
        jobs:
        - name: smoke_tests
          release: cf-smoke-tests
      include:
        stemcell:
        - os: ubuntu-trusty
        - os: ubuntu-xenial
      jobs:
      - name: loggr-syslog-agent
        properties:
          cache:
            tls:
              ca_cert: ((syslog_agent_api_tls.ca))
              cert: ((syslog_agent_api_tls.certificate))
              cn: binding-cache
              key: ((syslog_agent_api_tls.private_key))
          metrics:
            ca_cert: ((syslog_agent_metrics_tls.ca))
            cert: ((syslog_agent_metrics_tls.certificate))
            key: ((syslog_agent_metrics_tls.private_key))
            server_name: syslog_agent_metrics
          port: 3460
          tls:
            ca_cert: ((loggregator_tls_agent.ca))
            cert: ((loggregator_tls_agent.certificate))
            key: ((loggregator_tls_agent.private_key))
        release: loggregator-agent
      name: loggr-syslog-agent
    - exclude:
        jobs:
        - name: smoke_tests
          release: cf-smoke-tests
      include:
        stemcell:
        - os: ubuntu-xenial
      jobs:
      - name: prom_scraper
        properties:
          metrics:
            ca_cert: ((prom_scraper_metrics_tls.ca))
            cert: ((prom_scraper_metrics_tls.certificate))
            key: ((prom_scraper_metrics_tls.private_key))
            server_name: prom_scraper_metrics
          scrape:
            tls:
              ca_cert: ((prom_scraper_scrape_tls.ca))
              cert: ((prom_scraper_scrape_tls.certificate))
              key: ((prom_scraper_scrape_tls.private_key))
        release: loggregator-agent
      name: prom_scraper
    - exclude:
        jobs:
        - name: smoke_tests
          release: cf-smoke-tests
      include:
        stemcell:
        - os: ubuntu-trusty
        - os: ubuntu-xenial
      jobs:
      - name: metrics-discovery-registrar
        properties:
          metrics:
            ca_cert: ((metrics_discovery_metrics_tls.ca))
            cert: ((metrics_discovery_metrics_tls.certificate))
            key: ((metrics_discovery_metrics_tls.private_key))
            server_name: metrics_discovery_metrics
          nats_client:
            cert: ((nats_client_cert.certificate))
            key: ((nats_client_cert.private_key))
        release: metrics-discovery
      name: metrics-discovery-registrar
    - exclude:
        jobs:
        - name: smoke_tests
          release: cf-smoke-tests
      include:
        stemcell:
        - os: ubuntu-xenial
      jobs:
      - name: metrics-agent
        properties:
          grpc:
            ca_cert: ((loggregator_tls_agent.ca))
            cert: ((loggregator_tls_agent.certificate))
            key: ((loggregator_tls_agent.private_key))
          metrics:
            ca_cert: ((metrics_agent_tls.ca))
            cert: ((metrics_agent_tls.certificate))
            key: ((metrics_agent_tls.private_key))
            server_name: metrics_agent
          scrape:
            tls:
              ca_cert: ((prom_scraper_scrape_tls.ca))
              cert: ((prom_scraper_scrape_tls.certificate))
              key: ((prom_scraper_scrape_tls.private_key))
        release: metrics-discovery
      name: metrics-agent
    - include:
        stemcell:
        - os: ubuntu-xenial
      jobs:
      - name: bpm
        release: bpm
      name: bpm
    - jobs:
      - name: bosh-dns-aliases
        properties:
          aliases:
          - domain: _.cell.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: diego-cell
              network: default
              query: _
            - deployment: cf
              domain: bosh
              instance_group: windows2019-cell
              network: default
              query: _
            - deployment: cf
              domain: bosh
              instance_group: isolated-diego-cell
              network: default
              query: _
          - domain: auctioneer.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: scheduler
              network: default
              query: q-s4
          - domain: bbs.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: diego-api
              network: default
              query: q-s4
          - domain: blobstore.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: singleton-blobstore
              network: default
              query: '*'
          - domain: cc-uploader.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: api
              network: default
              query: '*'
          - domain: cloud-controller-ng.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: api
              network: default
              query: '*'
          - domain: credhub.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: credhub
              network: default
              query: '*'
          - domain: doppler.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: doppler
              network: default
              query: '*'
          - domain: file-server.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: api
              network: default
              query: '*'
          - domain: gorouter.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: router
              network: default
              query: '*'
          - domain: locket.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: diego-api
              network: default
              query: '*'
          - domain: loggregator-trafficcontroller.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: log-api
              network: default
              query: '*'
          - domain: policy-server.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: api
              network: default
              query: '*'
          - domain: reverse-log-proxy.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: log-api
              network: default
              query: '*'
          - domain: routing-api.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: api
              network: default
              query: '*'
          - domain: silk-controller.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: diego-api
              network: default
              query: '*'
          - domain: sql-db.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: database
              network: default
              query: '*'
          - domain: ssh-proxy.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: scheduler
              network: default
              query: '*'
          - domain: tps.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: scheduler
              network: default
              query: '*'
          - domain: uaa.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: uaa
              network: default
              query: '*'
          - domain: nats.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: nats
              network: default
              query: '*'
          - domain: _.nats.service.cf.internal
            targets:
            - deployment: cf
              domain: bosh
              instance_group: nats
              network: default
              query: _
        release: bosh-dns-aliases
      name: bosh-dns-aliases
    instance_groups:
    - azs:
      - z1
      instances: 1
      jobs:
      - name: smoke_tests
        properties:
          bpm:
            enabled: true
          smoke_tests:
            api: https://api.((system_domain))
            apps_domain: ((system_domain))
            cf_dial_timeout_in_seconds: 300
            client: cf_smoke_tests
            client_secret: ((uaa_clients_cf_smoke_tests_secret))
            org: cf_smoke_tests_org
            skip_ssl_validation: true
            space: cf_smoke_tests_space
        release: cf-smoke-tests
      - name: cf-cli-7-linux
        release: cf-cli
      lifecycle: errand
      name: smoke-tests
      networks:
      - name: default
      stemcell: default
      vm_type: minimal
    - azs:
      - z1
      - z2
      instances: 2
      jobs:
      - name: nats
        properties:
          nats:
            hostname: nats.service.cf.internal
            internal:
              tls:
                ca: ((nats_internal_cert.ca))
                certificate: ((nats_internal_cert.certificate))
                enabled: true
                private_key: ((nats_internal_cert.private_key))
            password: ((nats_password))
            user: nats
        provides:
          nats:
            as: nats
            shared: true
        release: nats
      - custom_provider_definitions:
        - name: nats-tls-address
          type: address
        name: nats-tls
        properties:
          nats:
            external:
              tls:
                ca: ((nats_client_cert.ca))
                certificate: ((nats_server_cert.certificate))
                private_key: ((nats_server_cert.private_key))
            hostname: nats.service.cf.internal
            internal:
              tls:
                ca: ((nats_internal_cert.ca))
                certificate: ((nats_internal_cert.certificate))
                enabled: true
                private_key: ((nats_internal_cert.private_key))
            password: ((nats_password))
            user: nats
        provides:
          nats-tls:
            as: nats-tls
            shared: true
        release: nats
      name: nats
      networks:
      - name: default
      stemcell: default
      vm_type: minimal
    - azs:
      - z1
      instances: 1
      jobs:
      - name: pxc-mysql
        properties:
          admin_password: ((cf_mysql_mysql_admin_password))
          engine_config:
            binlog:
              enabled: false
            galera:
              enabled: true
          port: 13306
          seeded_databases:
          - name: cloud_controller
            password: ((cc_database_password))
            username: cloud_controller
          - name: diego
            password: ((diego_database_password))
            username: diego
          - name: network_connectivity
            password: ((network_connectivity_database_password))
            username: network_connectivity
          - name: network_policy
            password: ((network_policy_database_password))
            username: network_policy
          - name: routing-api
            password: ((routing_api_database_password))
            username: routing-api
          - name: uaa
            password: ((uaa_database_password))
            username: uaa
          - name: locket
            password: ((locket_database_password))
            username: locket
          - name: credhub
            password: ((credhub_database_password))
            username: credhub
          tls:
            galera: ((galera_server_certificate))
            server: ((mysql_server_certificate))
        release: pxc
      - name: proxy
        properties:
          api_password: ((cf_mysql_proxy_api_password))
          api_port: 8083
          api_uri: proxy.((system_domain))
        release: pxc
      - name: galera-agent
        properties:
          db_password: ((cf_mysql_mysql_galera_healthcheck_password))
          endpoint_password: ((cf_mysql_mysql_galera_healthcheck_endpoint_password))
        release: pxc
      - name: gra-log-purger
        release: pxc
      - name: cluster-health-logger
        properties:
          db_password: ((cf_mysql_mysql_cluster_health_password))
        release: pxc
      - name: route_registrar
        properties:
          route_registrar:
            routes:
            - name: cf-mysql-proxy
              port: 8083
              prepend_instance_index: true
              registration_interval: 10s
              uris:
              - proxy.((system_domain))
            - name: cf-mysql-proxy-aggregator
              port: 8082
              registration_interval: 10s
              uris:
              - proxy.((system_domain))
        release: routing
      - name: bootstrap
        release: pxc
      migrated_from:
      - name: mysql
      - name: singleton-database
      name: database
      networks:
      - name: default
      persistent_disk_type: 10GB
      stemcell: default
      update: {}
      vm_type: small
    - azs:
      - z1
      - z2
      instances: 2
      jobs:
      - name: cfdot
        properties:
          tls:
            ca_certificate: ((diego_rep_client.ca))
            certificate: ((diego_rep_client.certificate))
            private_key: ((diego_rep_client.private_key))
        release: diego
      - name: bbs
        properties:
          bpm:
            enabled: true
          diego:
            bbs:
              active_key_label: key-2016-06
              auctioneer:
                ca_cert: ((diego_auctioneer_client.ca))
                client_cert: ((diego_auctioneer_client.certificate))
                client_key: ((diego_auctioneer_client.private_key))
              ca_cert: ((diego_bbs_server.ca))
              detect_consul_cell_registrations: false
              encryption_keys:
              - label: key-2016-06
                passphrase: ((diego_bbs_encryption_keys_passphrase))
              rep:
                ca_cert: ((diego_rep_client.ca))
                client_cert: ((diego_rep_client.certificate))
                client_key: ((diego_rep_client.private_key))
                require_tls: true
              server_cert: ((diego_bbs_server.certificate))
              server_key: ((diego_bbs_server.private_key))
              skip_consul_lock: true
              sql:
                ca_cert: ((mysql_server_certificate.ca))
                db_driver: mysql
                db_host: sql-db.service.cf.internal
                db_password: ((diego_database_password))
                db_port: 3306
                db_schema: diego
                db_username: diego
                require_ssl: true
          enable_consul_service_registration: false
          logging:
            format:
              timestamp: rfc3339
          loggregator:
            ca_cert: ((loggregator_tls_agent.ca))
            cert: ((loggregator_tls_agent.certificate))
            key: ((loggregator_tls_agent.private_key))
            use_v2_api: true
        release: diego
      - name: silk-controller
        properties:
          ca_cert: ((silk_controller.ca))
          database:
            ca_cert: ((mysql_server_certificate.ca))
            host: sql-db.service.cf.internal
            name: network_connectivity
            password: ((network_connectivity_database_password))
            port: 3306
            require_ssl: true
            type: mysql
            username: network_connectivity
          server_cert: ((silk_controller.certificate))
          server_key: ((silk_controller.private_key))
          silk_daemon:
            ca_cert: ((silk_daemon.ca))
            client_cert: ((silk_daemon.certificate))
            client_key: ((silk_daemon.private_key))
        release: silk
      - name: locket
        properties:
          bpm:
            enabled: true
          diego:
            locket:
              sql:
                ca_cert: ((mysql_server_certificate.ca))
                db_driver: mysql
                db_host: sql-db.service.cf.internal
                db_password: ((locket_database_password))
                db_port: 3306
                db_schema: locket
                db_username: locket
                require_ssl: true
          enable_consul_service_registration: false
          logging:
            format:
              timestamp: rfc3339
          loggregator:
            ca_cert: ((loggregator_tls_agent.ca))
            cert: ((loggregator_tls_agent.certificate))
            key: ((loggregator_tls_agent.private_key))
            use_v2_api: true
          tls:
            ca_cert: ((diego_locket_server.ca))
            cert: ((diego_locket_server.certificate))
            key: ((diego_locket_server.private_key))
        release: diego
      - name: loggr-udp-forwarder
        properties:
          loggregator:
            tls:
              ca: ((loggregator_tls_agent.ca))
              cert: ((loggregator_tls_agent.certificate))
              key: ((loggregator_tls_agent.private_key))
          metrics:
            ca_cert: ((loggr_udp_forwarder_tls.ca))
            cert: ((loggr_udp_forwarder_tls.certificate))
            key: ((loggr_udp_forwarder_tls.private_key))
            server_name: loggr_udp_forwarder_metrics
        release: loggregator-agent
      migrated_from:
      - name: diego-bbs
      name: diego-api
      networks:
      - name: default
      stemcell: default
      vm_type: small
    - azs:
      - z1
      - z2
      instances: 2
      jobs:
      - name: uaa
        properties:
          encryption:
            active_key_label: default_key
            encryption_keys:
            - label: default_key
              passphrase: ((uaa_default_encryption_passphrase))
          login:
            saml:
              activeKeyId: key-1
              keys:
                key-1:
                  certificate: ((uaa_login_saml.certificate))
                  key: ((uaa_login_saml.private_key))
                  passphrase: ""
          uaa:
            admin:
              client_secret: ((uaa_admin_client_secret))
            ca_certs:
            - ((mysql_server_certificate.ca))
            clients:
              cc-service-dashboards:
                authorities: clients.read,clients.write,clients.admin
                authorized-grant-types: client_credentials
                scope: openid,cloud_controller_service_permissions.read
                secret: ((uaa_clients_cc-service-dashboards_secret))
              cc_routing:
                authorities: routing.router_groups.read
                authorized-grant-types: client_credentials
                secret: ((uaa_clients_cc-routing_secret))
              cc_service_key_client:
                authorities: credhub.read,credhub.write
                authorized-grant-types: client_credentials
                secret: ((uaa_clients_cc_service_key_client_secret))
              cf:
                access-token-validity: 600
                authorities: uaa.none
                authorized-grant-types: password,refresh_token
                override: true
                refresh-token-validity: 2592000
                scope: network.admin,network.write,cloud_controller.read,cloud_controller.write,openid,password.write,cloud_controller.admin,scim.read,scim.write,doppler.firehose,uaa.user,routing.router_groups.read,routing.router_groups.write,cloud_controller.admin_read_only,cloud_controller.global_auditor,perm.admin,clients.read
                secret: ""
              cf_smoke_tests:
                authorities: cloud_controller.admin,clients.read
                authorized-grant-types: client_credentials
                secret: ((uaa_clients_cf_smoke_tests_secret))
              cloud_controller_username_lookup:
                authorities: scim.userids
                authorized-grant-types: client_credentials
                secret: ((uaa_clients_cloud_controller_username_lookup_secret))
              credhub_admin_client:
                authorities: credhub.read,credhub.write
                authorized-grant-types: client_credentials
                secret: ((credhub_admin_client_secret))
              doppler:
                authorities: uaa.resource
                authorized-grant-types: client_credentials
                override: true
                secret: ((uaa_clients_doppler_secret))
              gorouter:
                authorities: routing.routes.read
                authorized-grant-types: client_credentials
                secret: ((uaa_clients_gorouter_secret))
              network-policy:
                authorities: uaa.resource,cloud_controller.admin_read_only
                authorized-grant-types: client_credentials
                secret: ((uaa_clients_network_policy_secret))
              routing_api_client:
                authorities: routing.routes.write,routing.routes.read,routing.router_groups.read
                authorized-grant-types: client_credentials
                secret: ((uaa_clients_routing_api_client_secret))
              ssh-proxy:
                authorized-grant-types: authorization_code
                autoapprove: true
                override: true
                redirect-uri: https://uaa.((system_domain))/login
                scope: openid,cloud_controller.read,cloud_controller.write,cloud_controller.admin
                secret: ((uaa_clients_ssh-proxy_secret))
              tcp_emitter:
                authorities: routing.routes.write,routing.routes.read
                authorized-grant-types: client_credentials
                secret: ((uaa_clients_tcp_emitter_secret))
              tcp_router:
                authorities: routing.routes.read
                authorized-grant-types: client_credentials
                secret: ((uaa_clients_tcp_router_secret))
            jwt:
              policy:
                active_key_id: key-1
                keys:
                  key-1:
                    signingKey: ((uaa_jwt_signing_key.private_key))
            logging_level: INFO
            scim:
              users:
              - groups:
                - clients.read
                - cloud_controller.admin
                - doppler.firehose
                - network.admin
                - openid
                - routing.router_groups.read
                - routing.router_groups.write
                - scim.read
                - scim.write
                name: admin
                password: ((cf_admin_password))
            sslCertificate: ((uaa_ssl.certificate))
            sslPrivateKey: ((uaa_ssl.private_key))
            url: https://uaa.((system_domain))
            zones:
              internal:
                hostnames:
                - uaa.service.cf.internal
          uaadb:
            address: sql-db.service.cf.internal
            databases:
            - name: uaa
              tag: uaa
            db_scheme: mysql
            port: 3306
            roles:
            - name: uaa
              password: ((uaa_database_password))
              tag: admin
        release: uaa
      - name: route_registrar
        properties:
          route_registrar:
            routes:
            - health_check:
                name: uaa-healthcheck
                script_path: /var/vcap/jobs/uaa/bin/dns/healthy
              name: uaa
              registration_interval: 10s
              server_cert_domain_san: uaa.service.cf.internal
              tags:
                component: uaa
              tls_port: 8443
              uris:
              - uaa.((system_domain))
              - '*.uaa.((system_domain))'
              - login.((system_domain))
              - '*.login.((system_domain))'
        release: routing
      - name: statsd_injector
        properties:
          loggregator:
            tls:
              ca_cert: ((loggregator_tls_statsdinjector.ca))
              statsd_injector:
                cert: ((loggregator_tls_statsdinjector.certificate))
                key: ((loggregator_tls_statsdinjector.private_key))
        release: statsd-injector
      name: uaa
      networks:
      - name: default
      stemcell: default
      vm_type: minimal
    - azs:
      - z1
      instances: 1
      jobs:
      - name: blobstore
        properties:
          blobstore:
            admin_users:
            - password: ((blobstore_admin_users_password))
              username: blobstore-user
            secure_link:
              secret: ((blobstore_secure_link_secret))
            tls:
              cert: ((blobstore_tls.certificate))
              private_key: ((blobstore_tls.private_key))
          select_directories_to_backup:
          - buildpacks
          - packages
          - droplets
          system_domain: ((system_domain))
        release: capi
      - name: route_registrar
        properties:
          route_registrar:
            routes:
            - name: blobstore
              port: 8080
              registration_interval: 20s
              tags:
                component: blobstore
              uris:
              - blobstore.((system_domain))
        release: routing
      migrated_from:
      - name: blobstore
      name: singleton-blobstore
      networks:
      - name: default
      persistent_disk_type: 100GB
      stemcell: default
      update: {}
      vm_type: small
    - azs:
      - z1
      - z2
      instances: 2
      jobs:
      - name: cloud_controller_ng
        properties:
          app_domains:
          - ((system_domain))
          - internal: true
            name: apps.internal
          app_ssh:
            host_key_fingerprint: ((diego_ssh_proxy_host_key.public_key_fingerprint))
          cc:
            buildpacks:
              blobstore_type: webdav
              webdav_config:
                blobstore_timeout: 5
                ca_cert: ((blobstore_tls.ca))
                password: ((blobstore_admin_users_password))
                private_endpoint: https://blobstore.service.cf.internal:4443
                public_endpoint: https://blobstore.((system_domain))
                username: blobstore-user
            bulk_api_password: ((cc_bulk_api_password))
            database_encryption:
              current_key_label: encryption_key_0
              keys:
                encryption_key_0: ((cc_db_encryption_key))
            db_encryption_key: ((cc_db_encryption_key))
            default_running_security_groups:
            - public_networks
            - dns
            default_staging_security_groups:
            - public_networks
            - dns
            diego:
              docker_staging_stack: cflinuxfs3
            droplets:
              blobstore_type: webdav
              webdav_config:
                blobstore_timeout: 5
                ca_cert: ((blobstore_tls.ca))
                password: ((blobstore_admin_users_password))
                private_endpoint: https://blobstore.service.cf.internal:4443
                public_endpoint: https://blobstore.((system_domain))
                username: blobstore-user
            install_buildpacks:
            - name: staticfile_buildpack
              package: staticfile-buildpack-cflinuxfs3
            - name: java_buildpack
              package: java-buildpack-cflinuxfs3
            - name: ruby_buildpack
              package: ruby-buildpack-cflinuxfs3
            - name: dotnet_core_buildpack
              package: dotnet-core-buildpack-cflinuxfs3
            - name: nodejs_buildpack
              package: nodejs-buildpack-cflinuxfs3
            - name: go_buildpack
              package: go-buildpack-cflinuxfs3
            - name: python_buildpack
              package: python-buildpack-cflinuxfs3
            - name: php_buildpack
              package: php-buildpack-cflinuxfs3
            - name: nginx_buildpack
              package: nginx-buildpack-cflinuxfs3
            - name: r_buildpack
              package: r-buildpack-cflinuxfs3
            - name: binary_buildpack
              package: binary-buildpack-cflinuxfs3
            internal_api_password: ((cc_internal_api_password))
            logcache_tls:
              certificate: ((cc_logcache_tls.certificate))
              private_key: ((cc_logcache_tls.private_key))
            mutual_tls:
              ca_cert: ((cc_tls.ca))
              private_key: ((cc_tls.private_key))
              public_cert: ((cc_tls.certificate))
            packages:
              blobstore_type: webdav
              webdav_config:
                blobstore_timeout: 5
                ca_cert: ((blobstore_tls.ca))
                password: ((blobstore_admin_users_password))
                private_endpoint: https://blobstore.service.cf.internal:4443
                public_endpoint: https://blobstore.((system_domain))
                username: blobstore-user
            public_tls:
              ca_cert: ((cc_public_tls.ca))
              certificate: ((cc_public_tls.certificate))
              private_key: ((cc_public_tls.private_key))
            resource_pool:
              blobstore_type: webdav
              webdav_config:
                blobstore_timeout: 5
                ca_cert: ((blobstore_tls.ca))
                password: ((blobstore_admin_users_password))
                private_endpoint: https://blobstore.service.cf.internal:4443
                public_endpoint: https://blobstore.((system_domain))
                username: blobstore-user
            security_group_definitions:
            - name: public_networks
              rules:
              - destination: 0.0.0.0-9.255.255.255
                protocol: all
              - destination: 11.0.0.0-169.253.255.255
                protocol: all
              - destination: 169.255.0.0-172.15.255.255
                protocol: all
              - destination: 172.32.0.0-192.167.255.255
                protocol: all
              - destination: 192.169.0.0-255.255.255.255
                protocol: all
            - name: dns
              rules:
              - destination: 0.0.0.0/0
                ports: "53"
                protocol: tcp
              - destination: 0.0.0.0/0
                ports: "53"
                protocol: udp
            stacks:
            - description: Cloud Foundry Linux-based filesystem (Ubuntu 18.04)
              name: cflinuxfs3
            staging_upload_password: ((cc_staging_upload_password))
            staging_upload_user: staging_user
            temporary_use_logcache: true
          ccdb:
            address: sql-db.service.cf.internal
            ca_cert: ((mysql_server_certificate.ca))
            databases:
            - name: cloud_controller
              tag: cc
            db_scheme: mysql
            port: 3306
            roles:
            - name: cloud_controller
              password: ((cc_database_password))
              tag: admin
          credhub_api:
            ca_cert: ((credhub_tls.ca))
          router:
            route_services_secret: ((router_route_services_secret))
          routing_api:
            enabled: true
          ssl:
            skip_cert_verify: true
          system_domain: ((system_domain))
          uaa:
            ca_cert: ((uaa_ssl.ca))
            clients:
              cc-service-dashboards:
                secret: ((uaa_clients_cc-service-dashboards_secret))
              cc_routing:
                secret: ((uaa_clients_cc-routing_secret))
              cc_service_key_client:
                secret: ((uaa_clients_cc_service_key_client_secret))
              cloud_controller_username_lookup:
                secret: ((uaa_clients_cloud_controller_username_lookup_secret))
            url: https://uaa.((system_domain))
        provides:
          cloud_controller:
            as: cloud_controller
            shared: true
        release: capi
      - name: binary-buildpack
        release: binary-buildpack
      - name: dotnet-core-buildpack
        release: dotnet-core-buildpack
      - name: go-buildpack
        release: go-buildpack
      - name: java-buildpack
        release: java-buildpack
      - name: nodejs-buildpack
        release: nodejs-buildpack
      - name: nginx-buildpack
        release: nginx-buildpack
      - name: r-buildpack
        release: r-buildpack
      - name: php-buildpack
        release: php-buildpack
      - name: python-buildpack
        release: python-buildpack
      - name: ruby-buildpack
        release: ruby-buildpack
      - name: staticfile-buildpack
        release: staticfile-buildpack
      - name: route_registrar
        properties:
          route_registrar:
            routes:
            - health_check:
                name: api-health-check
                script_path: /var/vcap/jobs/cloud_controller_ng/bin/cloud_controller_ng_health_check
                timeout: 6s
              name: api
              port: 9022
              registration_interval: 10s
              server_cert_domain_san: api.((system_domain))
              tags:
                component: CloudController
              tls_port: 9024
              uris:
              - api.((system_domain))
            - name: policy-server
              registration_interval: 20s
              server_cert_domain_san: api.((system_domain))
              tls_port: 4002
              uris:
              - api.((system_domain))/networking
        release: routing
      - name: statsd_injector
        properties:
          loggregator:
            tls:
              ca_cert: ((loggregator_tls_statsdinjector.ca))
              statsd_injector:
                cert: ((loggregator_tls_statsdinjector.certificate))
                key: ((loggregator_tls_statsdinjector.private_key))
        release: statsd-injector
      - name: file_server
        properties:
          bpm:
            enabled: true
          enable_consul_service_registration: false
          logging:
            format:
              timestamp: rfc3339
          loggregator:
            ca_cert: ((loggregator_tls_agent.ca))
            cert: ((loggregator_tls_agent.certificate))
            key: ((loggregator_tls_agent.private_key))
            use_v2_api: true
        release: diego
      - name: policy-server
        properties:
          database:
            ca_cert: ((mysql_server_certificate.ca))
            host: sql-db.service.cf.internal
            name: network_policy
            password: ((network_policy_database_password))
            port: 3306
            require_ssl: true
            type: mysql
            username: network_policy
          enable_space_developer_self_service: true
          enable_tls: true
          server_cert: ((network_policy_server_external.certificate))
          server_key: ((network_policy_server_external.private_key))
          uaa_ca: ((uaa_ssl.ca))
          uaa_client_secret: ((uaa_clients_network_policy_secret))
        release: cf-networking
      - name: policy-server-internal
        properties:
          ca_cert: ((network_policy_server.ca))
          server_cert: ((network_policy_server.certificate))
          server_key: ((network_policy_server.private_key))
        release: cf-networking
      - name: cc_uploader
        properties:
          capi:
            cc_uploader:
              cc:
                ca_cert: ((cc_bridge_cc_uploader.ca))
                client_cert: ((cc_bridge_cc_uploader.certificate))
                client_key: ((cc_bridge_cc_uploader.private_key))
              mutual_tls:
                ca_cert: ((cc_bridge_cc_uploader_server.ca))
                server_cert: ((cc_bridge_cc_uploader_server.certificate))
                server_key: ((cc_bridge_cc_uploader_server.private_key))
        release: capi
      - name: loggr-udp-forwarder
        properties:
          loggregator:
            tls:
              ca: ((loggregator_tls_agent.ca))
              cert: ((loggregator_tls_agent.certificate))
              key: ((loggregator_tls_agent.private_key))
          metrics:
            ca_cert: ((loggr_udp_forwarder_tls.ca))
            cert: ((loggr_udp_forwarder_tls.certificate))
            key: ((loggr_udp_forwarder_tls.private_key))
            server_name: loggr_udp_forwarder_metrics
        release: loggregator-agent
      name: api
      networks:
      - name: default
      stemcell: default
      vm_extensions:
      - 50GB_ephemeral_disk
      vm_type: small
    - azs:
      - z1
      - z2
      instances: 2
      jobs:
      - name: cloud_controller_worker
        properties:
          cc:
            buildpacks:
              blobstore_type: webdav
              webdav_config:
                blobstore_timeout: 5
                ca_cert: ((blobstore_tls.ca))
                password: ((blobstore_admin_users_password))
                private_endpoint: https://blobstore.service.cf.internal:4443
                public_endpoint: https://blobstore.((system_domain))
                username: blobstore-user
            database_encryption:
              current_key_label: encryption_key_0
              keys:
                encryption_key_0: ((cc_db_encryption_key))
            db_encryption_key: ((cc_db_encryption_key))
            droplets:
              blobstore_type: webdav
              webdav_config:
                blobstore_timeout: 5
                ca_cert: ((blobstore_tls.ca))
                password: ((blobstore_admin_users_password))
                private_endpoint: https://blobstore.service.cf.internal:4443
                public_endpoint: https://blobstore.((system_domain))
                username: blobstore-user
            internal_api_password: ((cc_internal_api_password))
            mutual_tls:
              ca_cert: ((cc_tls.ca))
              private_key: ((cc_tls.private_key))
              public_cert: ((cc_tls.certificate))
            packages:
              blobstore_type: webdav
              webdav_config:
                blobstore_timeout: 5
                ca_cert: ((blobstore_tls.ca))
                password: ((blobstore_admin_users_password))
                private_endpoint: https://blobstore.service.cf.internal:4443
                public_endpoint: https://blobstore.((system_domain))
                username: blobstore-user
            resource_pool:
              blobstore_type: webdav
              webdav_config:
                blobstore_timeout: 5
                ca_cert: ((blobstore_tls.ca))
                password: ((blobstore_admin_users_password))
                private_endpoint: https://blobstore.service.cf.internal:4443
                public_endpoint: https://blobstore.((system_domain))
                username: blobstore-user
            staging_upload_password: ((cc_staging_upload_password))
            staging_upload_user: staging_user
          ccdb:
            address: sql-db.service.cf.internal
            ca_cert: ((mysql_server_certificate.ca))
            databases:
            - name: cloud_controller
              tag: cc
            db_scheme: mysql
            port: 3306
            roles:
            - name: cloud_controller
              password: ((cc_database_password))
              tag: admin
          routing_api:
            enabled: true
          ssl:
            skip_cert_verify: true
          system_domain: ((system_domain))
          uaa:
            ca_cert: ((uaa_ssl.ca))
            clients:
              cc-service-dashboards:
                secret: ((uaa_clients_cc-service-dashboards_secret))
              cc_routing:
                secret: ((uaa_clients_cc-routing_secret))
        release: capi
      name: cc-worker
      networks:
      - name: default
      stemcell: default
      vm_type: minimal
    - azs:
      - z1
      - z2
      instances: 2
      jobs:
      - name: cfdot
        properties:
          tls:
            ca_certificate: ((diego_rep_client.ca))
            certificate: ((diego_rep_client.certificate))
            private_key: ((diego_rep_client.private_key))
        release: diego
      - name: cloud_controller_clock
        properties:
          cc:
            buildpacks:
              blobstore_type: webdav
              webdav_config:
                blobstore_timeout: 5
                ca_cert: ((blobstore_tls.ca))
                password: ((blobstore_admin_users_password))
                private_endpoint: https://blobstore.service.cf.internal:4443
                public_endpoint: https://blobstore.((system_domain))
                username: blobstore-user
            database_encryption:
              current_key_label: encryption_key_0
              keys:
                encryption_key_0: ((cc_db_encryption_key))
            db_encryption_key: ((cc_db_encryption_key))
            droplets:
              blobstore_type: webdav
              webdav_config:
                blobstore_timeout: 5
                ca_cert: ((blobstore_tls.ca))
                password: ((blobstore_admin_users_password))
                private_endpoint: https://blobstore.service.cf.internal:4443
                public_endpoint: https://blobstore.((system_domain))
                username: blobstore-user
            internal_api_password: ((cc_internal_api_password))
            mutual_tls:
              ca_cert: ((cc_tls.ca))
              private_key: ((cc_tls.private_key))
              public_cert: ((cc_tls.certificate))
            packages:
              blobstore_type: webdav
              webdav_config:
                blobstore_timeout: 5
                ca_cert: ((blobstore_tls.ca))
                password: ((blobstore_admin_users_password))
                private_endpoint: https://blobstore.service.cf.internal:4443
                public_endpoint: https://blobstore.((system_domain))
                username: blobstore-user
            resource_pool:
              blobstore_type: webdav
              webdav_config:
                blobstore_timeout: 5
                ca_cert: ((blobstore_tls.ca))
                password: ((blobstore_admin_users_password))
                private_endpoint: https://blobstore.service.cf.internal:4443
                public_endpoint: https://blobstore.((system_domain))
                username: blobstore-user
            staging_upload_password: ((cc_staging_upload_password))
            staging_upload_user: staging_user
          ccdb:
            address: sql-db.service.cf.internal
            ca_cert: ((mysql_server_certificate.ca))
            databases:
            - name: cloud_controller
              tag: cc
            db_scheme: mysql
            port: 3306
            roles:
            - name: cloud_controller
              password: ((cc_database_password))
              tag: admin
          routing_api:
            enabled: true
          ssl:
            skip_cert_verify: true
          system_domain: ((system_domain))
          uaa:
            ca_cert: ((uaa_ssl.ca))
            clients:
              cc-service-dashboards:
                secret: ((uaa_clients_cc-service-dashboards_secret))
              cc_routing:
                secret: ((uaa_clients_cc-routing_secret))
            ssl:
              port: 8443
        release: capi
      - name: cc_deployment_updater
        properties:
          cc:
            db_encryption_key: ((cc_db_encryption_key))
            mutual_tls:
              ca_cert: ((cc_tls.ca))
              private_key: ((cc_tls.private_key))
              public_cert: ((cc_tls.certificate))
          ccdb:
            address: sql-db.service.cf.internal
            ca_cert: ((mysql_server_certificate.ca))
            databases:
            - name: cloud_controller
              tag: cc
            db_scheme: mysql
            port: 3306
            roles:
            - name: cloud_controller
              password: ((cc_database_password))
              tag: admin
        release: capi
      - name: service-discovery-controller
        properties:
          dnshttps:
            client:
              ca: ((cf_app_sd_server_tls.ca))
            server:
              tls: ((cf_app_sd_server_tls))
        release: cf-networking
      - name: statsd_injector
        properties:
          loggregator:
            tls:
              ca_cert: ((loggregator_tls_statsdinjector.ca))
              statsd_injector:
                cert: ((loggregator_tls_statsdinjector.certificate))
                key: ((loggregator_tls_statsdinjector.private_key))
        release: statsd-injector
      - name: tps
        properties:
          capi:
            tps:
              bbs:
                ca_cert: ((diego_bbs_client.ca))
                client_cert: ((diego_bbs_client.certificate))
                client_key: ((diego_bbs_client.private_key))
              cc:
                ca_cert: ((cc_bridge_tps.ca))
                client_cert: ((cc_bridge_tps.certificate))
                client_key: ((cc_bridge_tps.private_key))
              watcher:
                locket:
                  api_location: locket.service.cf.internal:8891
                skip_consul_lock: true
        release: capi
      - name: ssh_proxy
        properties:
          backends:
            tls:
              ca_certificates:
              - ((diego_instance_identity_ca.ca))
              client_certificate: ((ssh_proxy_backends_tls.certificate))
              client_private_key: ((ssh_proxy_backends_tls.private_key))
              enabled: true
          bpm:
            enabled: true
          diego:
            ssh_proxy:
              bbs:
                ca_cert: ((diego_bbs_client.ca))
                client_cert: ((diego_bbs_client.certificate))
                client_key: ((diego_bbs_client.private_key))
              disable_healthcheck_server: true
              enable_cf_auth: true
              host_key: ((diego_ssh_proxy_host_key.private_key))
              uaa:
                ca_cert: ((uaa_ssl.ca))
              uaa_secret: ((uaa_clients_ssh-proxy_secret))
          enable_consul_service_registration: false
          logging:
            format:
              timestamp: rfc3339
          loggregator:
            ca_cert: ((loggregator_tls_agent.ca))
            cert: ((loggregator_tls_agent.certificate))
            key: ((loggregator_tls_agent.private_key))
            use_v2_api: true
        release: diego
      - name: loggr-syslog-binding-cache
        properties:
          api:
            tls:
              ca_cert: ((cc_tls.ca))
              cert: ((binding_cache_api_tls.certificate))
              cn: cloud-controller-ng.service.cf.internal
              key: ((binding_cache_api_tls.private_key))
          external_port: 9000
          metrics:
            ca_cert: ((loggr_syslog_binding_cache_metrics_tls.ca))
            cert: ((loggr_syslog_binding_cache_metrics_tls.certificate))
            key: ((loggr_syslog_binding_cache_metrics_tls.private_key))
            server_name: loggr_syslog_binding_cache_metrics
          tls:
            ca_cert: ((binding_cache_tls.ca))
            cert: ((binding_cache_tls.certificate))
            cn: binding-cache
            key: ((binding_cache_tls.private_key))
        release: loggregator-agent
      - name: loggr-udp-forwarder
        properties:
          loggregator:
            tls:
              ca: ((loggregator_tls_agent.ca))
              cert: ((loggregator_tls_agent.certificate))
              key: ((loggregator_tls_agent.private_key))
          metrics:
            ca_cert: ((loggr_udp_forwarder_tls.ca))
            cert: ((loggr_udp_forwarder_tls.certificate))
            key: ((loggr_udp_forwarder_tls.private_key))
            server_name: loggr_udp_forwarder_metrics
        release: loggregator-agent
      migrated_from:
      - name: cc-bridge
      - name: cc-clock
      - name: diego-brain
      name: scheduler
      networks:
      - name: default
      stemcell: default
      vm_extensions:
      - diego-ssh-proxy-network-properties
      vm_type: minimal
    - azs:
      - z1
      - z2
      instances: 2
      jobs:
      - name: gorouter
        properties:
          router:
            backends:
              cert_chain: ((gorouter_backend_tls.certificate))
              private_key: ((gorouter_backend_tls.private_key))
            ca_certs: |
              ((diego_instance_identity_ca.ca))
              ((cc_tls.ca))
              ((uaa_ssl.ca))
              ((network_policy_server_external.ca))
            enable_ssl: true
            route_services_secret: ((router_route_services_secret))
            status:
              password: ((router_status_password))
              user: router-status
            tls_pem:
            - cert_chain: ((router_ssl.certificate))
              private_key: ((router_ssl.private_key))
            tracing:
              enable_zipkin: true
          routing_api:
            enabled: true
          uaa:
            ca_cert: ((uaa_ssl.ca))
            clients:
              gorouter:
                secret: ((uaa_clients_gorouter_secret))
            ssl:
              port: 8443
        release: routing
      - name: loggr-udp-forwarder
        properties:
          loggregator:
            tls:
              ca: ((loggregator_tls_agent.ca))
              cert: ((loggregator_tls_agent.certificate))
              key: ((loggregator_tls_agent.private_key))
          metrics:
            ca_cert: ((loggr_udp_forwarder_tls.ca))
            cert: ((loggr_udp_forwarder_tls.certificate))
            key: ((loggr_udp_forwarder_tls.private_key))
            server_name: loggr_udp_forwarder_metrics
        release: loggregator-agent
      name: router
      networks:
      - name: default
      stemcell: default
      update: {}
      vm_extensions:
      - cf-router-network-properties
      vm_type: minimal
    - azs:
      - z1
      - z2
      instances: 2
      jobs:
      - name: tcp_router
        properties:
          tcp_router:
            oauth_secret: ((uaa_clients_tcp_router_secret))
            router_group: default-tcp
          uaa:
            ca_cert: ((uaa_ssl.ca))
            tls_port: 8443
        release: routing
      - name: loggr-udp-forwarder
        properties:
          loggregator:
            tls:
              ca: ((loggregator_tls_agent.ca))
              cert: ((loggregator_tls_agent.certificate))
              key: ((loggregator_tls_agent.private_key))
          metrics:
            ca_cert: ((loggr_udp_forwarder_tls.ca))
            cert: ((loggr_udp_forwarder_tls.certificate))
            key: ((loggr_udp_forwarder_tls.private_key))
            server_name: loggr_udp_forwarder_metrics
        release: loggregator-agent
      name: tcp-router
      networks:
      - name: default
      stemcell: default
      vm_extensions:
      - cf-tcp-router-network-properties
      vm_type: minimal
    - azs:
      - z1
      - z2
      instances: 4
      jobs:
      - name: doppler
        properties:
          loggregator:
            tls:
              ca_cert: ((loggregator_tls_doppler.ca))
              doppler:
                cert: ((loggregator_tls_doppler.certificate))
                key: ((loggregator_tls_doppler.private_key))
        provides:
          doppler:
            as: doppler
            shared: true
        release: loggregator
      name: doppler
      networks:
      - name: default
      stemcell: default
      vm_type: minimal
    - azs:
      - z1
      - z2
      instances: 3
      jobs:
      - name: bosh-dns-adapter
        properties:
          dnshttps:
            client:
              tls: ((cf_app_sd_client_tls))
            server:
              ca: ((cf_app_sd_client_tls.ca))
          internal_domains:
          - apps.internal.
        release: cf-networking
      - name: cflinuxfs3-rootfs-setup
        properties:
          cflinuxfs3-rootfs:
            trusted_certs:
            - ((diego_instance_identity_ca.ca))
            - ((credhub_tls.ca))
            - ((uaa_ssl.ca))
        release: cflinuxfs3
      - name: garden
        properties:
          garden:
            cleanup_process_dirs_on_wait: true
            containerd_mode: true
            debug_listen_address: 127.0.0.1:17019
            default_container_grace_time: 0
            deny_networks:
            - 0.0.0.0/0
            destroy_containers_on_start: true
            network_plugin: /var/vcap/packages/runc-cni/bin/garden-external-networker
            network_plugin_extra_args:
            - --configFile=/var/vcap/jobs/garden-cni/config/adapter.json
          logging:
            format:
              timestamp: rfc3339
        release: garden-runc
      - name: rep
        properties:
          bpm:
            enabled: true
          containers:
            proxy:
              enabled: true
              require_and_verify_client_certificates: true
              trusted_ca_certificates:
              - ((gorouter_backend_tls.ca))
              - ((ssh_proxy_backends_tls.ca))
              verify_subject_alt_name:
              - gorouter.service.cf.internal
              - ssh-proxy.service.cf.internal
            trusted_ca_certificates:
            - ((diego_instance_identity_ca.ca))
            - ((credhub_tls.ca))
            - ((uaa_ssl.ca))
          diego:
            executor:
              instance_identity_ca_cert: ((diego_instance_identity_ca.certificate))
              instance_identity_key: ((diego_instance_identity_ca.private_key))
            rep:
              preloaded_rootfses:
              - cflinuxfs3:/var/vcap/packages/cflinuxfs3/rootfs.tar
          enable_consul_service_registration: false
          enable_declarative_healthcheck: true
          logging:
            format:
              timestamp: rfc3339
          loggregator:
            ca_cert: ((loggregator_tls_agent.ca))
            cert: ((loggregator_tls_agent.certificate))
            key: ((loggregator_tls_agent.private_key))
            use_v2_api: true
          tls:
            ca_cert: ((diego_rep_agent_v2.ca))
            cert: ((diego_rep_agent_v2.certificate))
            key: ((diego_rep_agent_v2.private_key))
        release: diego
      - name: cfdot
        properties:
          tls:
            ca_certificate: ((diego_rep_client.ca))
            certificate: ((diego_rep_client.certificate))
            private_key: ((diego_rep_client.private_key))
        release: diego
      - consumes:
          nats:
            ip_addresses: false
          nats-tls:
            ip_addresses: false
        name: route_emitter
        properties:
          bpm:
            enabled: true
          diego:
            route_emitter:
              bbs:
                ca_cert: ((diego_bbs_client.ca))
                client_cert: ((diego_bbs_client.certificate))
                client_key: ((diego_bbs_client.private_key))
              local_mode: true
              nats:
                tls:
                  client_cert: ((nats_client_cert.certificate))
                  client_key: ((nats_client_cert.private_key))
                  enabled: true
          internal_routes:
            enabled: true
          logging:
            format:
              timestamp: rfc3339
          loggregator:
            ca_cert: ((loggregator_tls_agent.ca))
            cert: ((loggregator_tls_agent.certificate))
            key: ((loggregator_tls_agent.private_key))
            use_v2_api: true
          tcp:
            enabled: true
          uaa:
            ca_cert: ((uaa_ssl.ca))
            client_secret: ((uaa_clients_tcp_emitter_secret))
        release: diego
      - name: garden-cni
        properties:
          cni_config_dir: /var/vcap/jobs/silk-cni/config/cni
          cni_plugin_dir: /var/vcap/packages/silk-cni/bin
        release: cf-networking
      - name: netmon
        release: silk
      - name: vxlan-policy-agent
        properties:
          ca_cert: ((network_policy_client.ca))
          client_cert: ((network_policy_client.certificate))
          client_key: ((network_policy_client.private_key))
        release: silk
      - name: silk-daemon
        properties:
          ca_cert: ((silk_daemon.ca))
          client_cert: ((silk_daemon.certificate))
          client_key: ((silk_daemon.private_key))
        release: silk
      - name: silk-cni
        properties:
          dns_servers:
          - 169.254.0.2
        release: silk
      - name: loggr-udp-forwarder
        properties:
          loggregator:
            tls:
              ca: ((loggregator_tls_agent.ca))
              cert: ((loggregator_tls_agent.certificate))
              key: ((loggregator_tls_agent.private_key))
          metrics:
            ca_cert: ((loggr_udp_forwarder_tls.ca))
            cert: ((loggr_udp_forwarder_tls.certificate))
            key: ((loggr_udp_forwarder_tls.private_key))
            server_name: loggr_udp_forwarder_metrics
        release: loggregator-agent
      name: diego-cell
      networks:
      - name: default
      stemcell: default
      vm_extensions:
      - 100GB_ephemeral_disk
      vm_type: small-highmem
    - azs:
      - z1
      - z2
      instances: 2
      jobs:
      - consumes:
          doppler:
            from: doppler
        name: loggregator_trafficcontroller
        properties:
          cc:
            internal_service_hostname: cloud-controller-ng.service.cf.internal
            mutual_tls:
              ca_cert: ((cc_tls.ca))
            tls_port: 9023
          loggregator:
            outgoing_cert: ((loggregator_trafficcontroller_tls.certificate))
            outgoing_key: ((loggregator_trafficcontroller_tls.private_key))
            tls:
              ca_cert: ((loggregator_tls_tc.ca))
              cc_trafficcontroller:
                cert: ((loggregator_tls_cc_tc.certificate))
                key: ((loggregator_tls_cc_tc.private_key))
              trafficcontroller:
                cert: ((loggregator_tls_tc.certificate))
                key: ((loggregator_tls_tc.private_key))
            uaa:
              client_secret: ((uaa_clients_doppler_secret))
          ssl:
            skip_cert_verify: true
          system_domain: ((system_domain))
          uaa:
            ca_cert: ((uaa_ssl.ca))
            internal_url: https://uaa.service.cf.internal:8443
        release: loggregator
      - name: reverse_log_proxy
        properties:
          loggregator:
            tls:
              ca_cert: ((loggregator_tls_rlp.ca))
              reverse_log_proxy:
                cert: ((loggregator_tls_rlp.certificate))
                key: ((loggregator_tls_rlp.private_key))
        provides:
          reverse_log_proxy:
            as: reverse_log_proxy
            shared: true
        release: loggregator
      - name: reverse_log_proxy_gateway
        properties:
          cc:
            ca_cert: ((loggregator_rlp_gateway_tls_cc.ca))
            capi_internal_addr: https://cloud-controller-ng.service.cf.internal:9023
            cert: ((loggregator_rlp_gateway_tls_cc.certificate))
            common_name: cloud-controller-ng.service.cf.internal
            key: ((loggregator_rlp_gateway_tls_cc.private_key))
          http:
            address: 0.0.0.0:8088
            cert: ((loggregator_rlp_gateway_tls.certificate))
            key: ((loggregator_rlp_gateway_tls.private_key))
          logs_provider:
            ca_cert: ((loggregator_rlp_gateway.ca))
            client_cert: ((loggregator_rlp_gateway.certificate))
            client_key: ((loggregator_rlp_gateway.private_key))
          metrics:
            ca_cert: ((rlp_gateway_metrics_tls.ca))
            cert: ((rlp_gateway_metrics_tls.certificate))
            key: ((rlp_gateway_metrics_tls.private_key))
            server_name: rlp_gateway_metrics
          uaa:
            ca_cert: ((uaa_ssl.ca))
            client_id: doppler
            client_secret: ((uaa_clients_doppler_secret))
            internal_addr: https://uaa.service.cf.internal:8443
        release: loggregator
      - name: route_registrar
        properties:
          route_registrar:
            routes:
            - name: doppler
              registration_interval: 20s
              server_cert_domain_san: doppler.((system_domain))
              tls_port: 8081
              uris:
              - doppler.((system_domain))
              - '*.doppler.((system_domain))'
            - name: rlp-gateway
              registration_interval: 20s
              server_cert_domain_san: log-stream.((system_domain))
              tls_port: 8088
              uris:
              - log-stream.((system_domain))
              - '*.log-stream.((system_domain))'
        release: routing
      name: log-api
      networks:
      - name: default
      stemcell: default
      vm_type: minimal
    - azs:
      - z1
      - z2
      instances: 2
      jobs:
      - consumes:
          postgres: null
        name: credhub
        properties:
          credhub:
            authentication:
              mutual_tls:
                trusted_cas:
                - ((diego_instance_identity_ca.ca))
              uaa:
                ca_certs:
                - ((uaa_ssl.ca))
                url: https://uaa.service.cf.internal:8443
            authorization:
              acls:
                enabled: true
              permissions:
              - actors:
                - uaa-client:credhub_admin_client
                operations:
                - read
                - write
                - delete
                - read_acl
                - write_acl
                path: /*
              - actors:
                - uaa-client:cc_service_key_client
                operations:
                - read
                path: /*
            ca_certificate: |
              ((credhub_tls.ca))
            data_storage:
              database: credhub
              host: sql-db.service.cf.internal
              password: ((credhub_database_password))
              port: 3306
              tls_ca: ((mysql_server_certificate.ca))
              type: mysql
              username: credhub
            encryption:
              keys:
              - active: true
                key_properties:
                  encryption_password: ((credhub_encryption_password))
                provider_name: internal-provider
              providers:
              - name: internal-provider
                type: internal
            internal_url: https://credhub.service.cf.internal
            tls: ((credhub_tls))
        release: credhub
      name: credhub
      networks:
      - name: default
      stemcell: default
      vm_type: minimal
    - azs:
      - z1
      instances: 1
      jobs:
      - name: rotate_cc_database_key
        properties: {}
        release: capi
      lifecycle: errand
      name: rotate-cc-database-key
      networks:
      - name: default
      stemcell: default
      vm_type: minimal
    - instances: 2
      jobs:
      - name: routing-api
        properties:
          routing_api:
            enabled_api_endpoints: both
            locket:
              api_location: locket.service.cf.internal:8891
              ca_cert: ((diego_locket_client.ca))
              client_cert: ((diego_locket_client.certificate))
              client_key: ((diego_locket_client.private_key))
            mtls_ca: ((routing_api_tls_client.ca))
            mtls_client_cert: ((routing_api_tls_client.certificate))
            mtls_client_key: ((routing_api_tls_client.private_key))
            mtls_server_cert: ((routing_api_tls.certificate))
            mtls_server_key: ((routing_api_tls.private_key))
            router_groups:
            - name: default-tcp
              reservable_ports: 1024-1033
              type: tcp
            skip_consul_lock: true
            sqldb:
              ca_cert: ((mysql_server_certificate.ca))
              host: sql-db.service.cf.internal
              password: ((routing_api_database_password))
              port: 3306
              schema: routing-api
              type: mysql
              username: routing-api
            system_domain: ((system_domain))
          uaa:
            ca_cert: ((uaa_ssl.ca))
            tls_port: 8443
        release: routing
      name: routing-api
      stemcell: default
    - instances: 4
      jobs:
      - name: log-cache
        properties:
          health_addr: localhost:6060
          metrics:
            ca_cert: ((log_cache_metrics_tls.ca))
            cert: ((log_cache_metrics_tls.certificate))
            key: ((log_cache_metrics_tls.private_key))
            server_name: log_cache_metrics
          tls:
            ca_cert: ((log_cache.ca))
            cert: ((log_cache.certificate))
            key: ((log_cache.private_key))
        provides:
          log-cache:
            shared: true
        release: log-cache
      - name: log-cache-cf-auth-proxy
        properties:
          cc:
            ca_cert: ((cc_tls.ca))
            common_name: cloud-controller-ng.service.cf.internal
          external_cert: ((logcache_ssl.certificate))
          external_key: ((logcache_ssl.private_key))
          metrics:
            ca_cert: ((log_cache_cf_auth_proxy_metrics_tls.ca))
            cert: ((log_cache_cf_auth_proxy_metrics_tls.certificate))
            key: ((log_cache_cf_auth_proxy_metrics_tls.private_key))
            server_name: log_cache_cf_auth_proxy_metrics
          proxy_ca_cert: ((log_cache.ca))
          proxy_port: 8083
          uaa:
            ca_cert: ((uaa_ssl.ca))
            client_id: doppler
            client_secret: ((uaa_clients_doppler_secret))
            internal_addr: https://uaa.service.cf.internal:8443
        release: log-cache
      - name: log-cache-gateway
        properties:
          gateway_addr: localhost:8081
          metrics:
            ca_cert: ((log_cache_gateway_metrics_tls.ca))
            cert: ((log_cache_gateway_metrics_tls.certificate))
            key: ((log_cache_gateway_metrics_tls.private_key))
            server_name: log_cache_gateway_metrics
          proxy_cert: ((log_cache_proxy_tls.certificate))
          proxy_key: ((log_cache_proxy_tls.private_key))
        release: log-cache
      - consumes:
          reverse_log_proxy:
            from: reverse_log_proxy
        name: log-cache-nozzle
        properties:
          logs_provider:
            tls:
              ca_cert: ((logs_provider.ca))
              cert: ((logs_provider.certificate))
              key: ((logs_provider.private_key))
        release: log-cache
      - name: route_registrar
        properties:
          route_registrar:
            routes:
            - name: log-cache-reverse-proxy
              port: 8083
              registration_interval: 20s
              server_cert_domain_san: log-cache.((system_domain))
              tls_port: 8083
              uris:
              - log-cache.((system_domain))
              - '*.log-cache.((system_domain))'
        release: routing
      name: log-cache
      stemcell: default
    - instances: 2
      jobs:
      - name: auctioneer
        properties:
          bpm:
            enabled: true
          diego:
            auctioneer:
              bbs:
                ca_cert: ((diego_bbs_client.ca))
                client_cert: ((diego_bbs_client.certificate))
                client_key: ((diego_bbs_client.private_key))
              ca_cert: ((diego_auctioneer_server.ca))
              rep:
                ca_cert: ((diego_rep_client.ca))
                client_cert: ((diego_rep_client.certificate))
                client_key: ((diego_rep_client.private_key))
                require_tls: true
              server_cert: ((diego_auctioneer_server.certificate))
              server_key: ((diego_auctioneer_server.private_key))
              skip_consul_lock: true
          enable_consul_service_registration: false
          logging:
            format:
              timestamp: rfc3339
          loggregator:
            ca_cert: ((loggregator_tls_agent.ca))
            cert: ((loggregator_tls_agent.certificate))
            key: ((loggregator_tls_agent.private_key))
            use_v2_api: true
        release: diego
      name: auctioneer
      stemcell: default
    manifest_version: v13.17.0
    name: cf
    releases:
    - name: binary-buildpack
      sha1: 0269a613be68f988682bbf56504b78477965b1c4
      url: https://bosh.io/d/github.com/cloudfoundry/binary-buildpack-release?v=1.0.36
      version: 1.0.36
    - name: bpm
      sha1: dcf0582d838a73de29da273552ae79ac3098ee8b
      url: https://bosh.io/d/github.com/cloudfoundry/bpm-release?v=1.1.9
      version: 1.1.9
    - name: capi
      sha1: e4b0b8a1ef10b71da5b09248150c3295197ee0b6
      url: https://bosh.io/d/github.com/cloudfoundry/capi-release?v=1.98.0
      version: 1.98.0
    - name: cf-networking
      sha1: 0497597194cf22fcd474e949a9299336cca78e9e
      url: https://bosh.io/d/github.com/cloudfoundry/cf-networking-release?v=2.33.0
      version: 2.33.0
    - name: cf-smoke-tests
      sha1: 0bbdb6970b594e7e4dde08937c39b80d8414f2ba
      url: https://bosh.io/d/github.com/cloudfoundry/cf-smoke-tests-release?v=41.0.1
      version: 41.0.1
    - name: cflinuxfs3
      sha1: 447a855912fa735685d9fe18bc33682b90e061c7
      url: https://bosh.io/d/github.com/cloudfoundry/cflinuxfs3-release?v=0.203.0
      version: 0.203.0
    - name: credhub
      sha1: 3a9732bab2fa80bb9431941193b6569ef29af491
      url: https://bosh.io/d/github.com/pivotal-cf/credhub-release?v=2.8.0
      version: 2.8.0
    - name: diego
      sha1: fbf8ebfeda1f326f5d6834b9a621b88a548e06e3
      url: https://bosh.io/d/github.com/cloudfoundry/diego-release?v=2.48.0
      version: 2.48.0
    - name: dotnet-core-buildpack
      sha1: 5c1f388a340c9c226d7c4feccf0a34a20e8d6de8
      url: https://bosh.io/d/github.com/cloudfoundry/dotnet-core-buildpack-release?v=2.3.13
      version: 2.3.13
    - name: garden-runc
      sha1: c5bb4211adb7e94750b56e81eda9bb8e09eab3a0
      url: https://bosh.io/d/github.com/cloudfoundry/garden-runc-release?v=1.19.16
      version: 1.19.16
    - name: go-buildpack
      sha1: 2badebead2977b1f220abd0158677c3a55afcdbc
      url: https://bosh.io/d/github.com/cloudfoundry/go-buildpack-release?v=1.9.16
      version: 1.9.16
    - name: java-buildpack
      sha1: 80433833d696c5219be4eadcf6af6abb6a4862a3
      url: https://bosh.io/d/github.com/cloudfoundry/java-buildpack-release?v=4.32.1
      version: 4.32.1
    - name: loggregator
      sha1: 31992561334a0c2b42ea830b3af90d012172e7e5
      url: https://bosh.io/d/github.com/cloudfoundry/loggregator-release?v=106.3.10
      version: 106.3.10
    - name: metrics-discovery
      sha1: 4219b58972d20f6fccf1a4a167d915e34d86ee8a
      url: https://bosh.io/d/github.com/cloudfoundry/metrics-discovery-release?v=3.0.1
      version: 3.0.1
    - name: nats
      sha1: 0d7f4c3203c926e79d85d07b1ac215d73c9268c3
      url: https://bosh.io/d/github.com/cloudfoundry/nats-release?v=34
      version: "34"
    - name: nginx-buildpack
      sha1: dfb0be8e2c0f14e2f43b10cb437b433ab88a53fb
      url: https://bosh.io/d/github.com/cloudfoundry/nginx-buildpack-release?v=1.1.12
      version: 1.1.12
    - name: r-buildpack
      sha1: c7ee8762d9ce8da4882c924cb46e77bef3b2fafc
      url: https://bosh.io/d/github.com/cloudfoundry/r-buildpack-release?v=1.1.7
      version: 1.1.7
    - name: nodejs-buildpack
      sha1: 8dc75e1c310d8cd1c6321c3ea43943e6714e84bd
      url: https://bosh.io/d/github.com/cloudfoundry/nodejs-buildpack-release?v=1.7.25
      version: 1.7.25
    - name: php-buildpack
      sha1: e9f0ef1a1ad27a0d34fe55385ce6b3ef2e947d54
      url: https://bosh.io/d/github.com/cloudfoundry/php-buildpack-release?v=4.4.19
      version: 4.4.19
    - name: pxc
      sha1: b09790e0d9df109e84416ef28393438846af9f9f
      url: https://bosh.io/d/github.com/cloudfoundry-incubator/pxc-release?v=0.28.0
      version: 0.28.0
    - name: python-buildpack
      sha1: 72d8cdd7d8c5bee832de0133568a673dc2641349
      url: https://bosh.io/d/github.com/cloudfoundry/python-buildpack-release?v=1.7.18
      version: 1.7.18
    - name: routing
      sha1: 8188941dcaa06082900acbceae80f9188d96ce0d
      url: https://bosh.io/d/github.com/cloudfoundry/routing-release?v=0.206.0
      version: 0.206.0
    - name: ruby-buildpack
      sha1: e32eac05ad44279b6c5b3de50998f967e31e7f1b
      url: https://bosh.io/d/github.com/cloudfoundry/ruby-buildpack-release?v=1.8.23
      version: 1.8.23
    - name: silk
      sha1: 9a2ed4dec4eff614614d6a66b77cd04182e25122
      url: https://bosh.io/d/github.com/cloudfoundry/silk-release?v=2.33.0
      version: 2.33.0
    - name: staticfile-buildpack
      sha1: 67dfff58d2efc40e632f70db6a602c31ee491f6a
      url: https://bosh.io/d/github.com/cloudfoundry/staticfile-buildpack-release?v=1.5.9
      version: 1.5.9
    - name: statsd-injector
      sha1: a0a2d33c6ab7d8fec8c017ea6f2c5a344af1407c
      url: https://bosh.io/d/github.com/cloudfoundry/statsd-injector-release?v=1.11.15
      version: 1.11.15
    - name: uaa
      sha1: 0de84931c008aa55664a5fc47ace4f694f95d688
      url: https://bosh.io/d/github.com/cloudfoundry/uaa-release?v=74.24.0
      version: 74.24.0
    - name: loggregator-agent
      sha1: c8f15cb137a5106cd83834721ee2709f37715930
      url: https://bosh.io/d/github.com/cloudfoundry/loggregator-agent-release?v=6.1.1
      version: 6.1.1
    - name: log-cache
      sha1: 3b158b60a25bb9e48039012ba124cb419a28d397
      url: https://bosh.io/d/github.com/cloudfoundry/log-cache-release?v=2.8.0
      version: 2.8.0
    - name: bosh-dns-aliases
      sha1: b0d0a0350ed87f1ded58b2ebb469acea0e026ccc
      url: https://bosh.io/d/github.com/cloudfoundry/bosh-dns-aliases-release?v=0.0.3
      version: 0.0.3
    - name: cf-cli
      sha1: 0dd79b169473a95f1ff39e79f2695edf4c97f38d
      url: https://bosh.io/d/github.com/bosh-packages/cf-cli-release?v=1.29.0
      version: 1.29.0
    stemcells:
    - alias: default
      os: ubuntu-xenial
      version: "621.82"
    update:
      canaries: 1
      canary_watch_time: 30000-1200000
      max_in_flight: 1
      serial: false
      update_watch_time: 5000-1200000
    variables:
    - name: blobstore_admin_users_password
      type: password
    - name: blobstore_secure_link_secret
      type: password
    - name: cc_bulk_api_password
      type: password
    - name: cc_db_encryption_key
      type: password
    - name: cc_internal_api_password
      type: password
    - name: cc_staging_upload_password
      type: password
    - name: cf_app_sd_ca
      options:
        common_name: service-discovery-controller.service.cf.internal
        is_ca: true
      type: certificate
    - name: cf_app_sd_client_tls
      options:
        ca: cf_app_sd_ca
        common_name: service-discovery-controller.service.cf.internal
        extended_key_usage:
        - client_auth
      type: certificate
    - name: cf_app_sd_server_tls
      options:
        ca: cf_app_sd_ca
        common_name: service-discovery-controller.service.cf.internal
        extended_key_usage:
        - server_auth
      type: certificate
    - name: cf_mysql_mysql_admin_password
      type: password
    - name: cf_mysql_mysql_cluster_health_password
      type: password
    - name: cf_mysql_mysql_galera_healthcheck_endpoint_password
      type: password
    - name: cf_mysql_mysql_galera_healthcheck_password
      type: password
    - name: cf_mysql_proxy_api_password
      type: password
    - name: cc_database_password
      type: password
    - name: credhub_database_password
      type: password
    - name: diego_database_password
      type: password
    - name: uaa_database_password
      type: password
    - name: routing_api_database_password
      type: password
    - name: network_policy_database_password
      type: password
    - name: network_connectivity_database_password
      type: password
    - name: uaa_default_encryption_passphrase
      type: password
    - name: silk_ca
      options:
        common_name: silk-ca
        is_ca: true
      type: certificate
    - name: silk_controller
      options:
        ca: silk_ca
        common_name: silk-controller.service.cf.internal
        extended_key_usage:
        - server_auth
      type: certificate
    - name: silk_daemon
      options:
        ca: silk_ca
        common_name: silk-daemon
        extended_key_usage:
        - client_auth
      type: certificate
    - name: network_policy_ca
      options:
        common_name: networkPolicyCA
        is_ca: true
      type: certificate
    - name: network_policy_server_external
      options:
        alternative_names:
        - api.((system_domain))
        ca: network_policy_ca
        common_name: api.((system_domain))
        extended_key_usage:
        - server_auth
      type: certificate
    - name: network_policy_server
      options:
        ca: network_policy_ca
        common_name: policy-server.service.cf.internal
        extended_key_usage:
        - server_auth
      type: certificate
    - name: network_policy_client
      options:
        ca: network_policy_ca
        common_name: clientName
        extended_key_usage:
        - client_auth
      type: certificate
    - name: uaa_clients_routing_api_client_secret
      type: password
    - name: uaa_clients_tcp_emitter_secret
      type: password
    - name: nats_password
      type: password
    - name: router_status_password
      type: password
    - name: cf_admin_password
      type: password
    - name: router_route_services_secret
      type: password
    - name: uaa_admin_client_secret
      type: password
    - name: uaa_clients_cc-routing_secret
      type: password
    - name: uaa_clients_cc-service-dashboards_secret
      type: password
    - name: uaa_clients_cc_service_key_client_secret
      type: password
    - name: uaa_clients_cf_smoke_tests_secret
      type: password
    - name: uaa_clients_cloud_controller_username_lookup_secret
      type: password
    - name: uaa_clients_doppler_secret
      type: password
    - name: uaa_clients_gorouter_secret
      type: password
    - name: uaa_clients_network_policy_secret
      type: password
    - name: uaa_clients_ssh-proxy_secret
      type: password
    - name: uaa_clients_tcp_router_secret
      type: password
    - name: diego_bbs_encryption_keys_passphrase
      type: password
    - name: credhub_encryption_password
      type: password
    - name: credhub_admin_client_secret
      type: password
    - name: diego_ssh_proxy_host_key
      type: ssh
    - name: uaa_jwt_signing_key
      type: rsa
    - name: service_cf_internal_ca
      options:
        common_name: internalCA
        is_ca: true
      type: certificate
    - name: blobstore_tls
      options:
        ca: service_cf_internal_ca
        common_name: blobstore.service.cf.internal
      type: certificate
    - name: diego_auctioneer_client
      options:
        ca: service_cf_internal_ca
        common_name: auctioneer client
        extended_key_usage:
        - client_auth
      type: certificate
    - name: diego_auctioneer_server
      options:
        alternative_names:
        - '*.auctioneer.service.cf.internal'
        - auctioneer.service.cf.internal
        ca: service_cf_internal_ca
        common_name: auctioneer.service.cf.internal
        extended_key_usage:
        - server_auth
      type: certificate
    - name: diego_bbs_client
      options:
        ca: service_cf_internal_ca
        common_name: bbs client
        extended_key_usage:
        - client_auth
      type: certificate
    - name: diego_bbs_server
      options:
        alternative_names:
        - '*.bbs.service.cf.internal'
        - bbs.service.cf.internal
        ca: service_cf_internal_ca
        common_name: bbs.service.cf.internal
        extended_key_usage:
        - server_auth
        - client_auth
      type: certificate
    - name: diego_rep_client
      options:
        ca: service_cf_internal_ca
        common_name: rep client
        extended_key_usage:
        - client_auth
      type: certificate
    - name: diego_rep_agent_v2
      options:
        alternative_names:
        - '*.cell.service.cf.internal'
        - cell.service.cf.internal
        - 127.0.0.1
        - localhost
        ca: service_cf_internal_ca
        common_name: cell.service.cf.internal
        extended_key_usage:
        - client_auth
        - server_auth
      type: certificate
    - name: loggregator_ca
      options:
        common_name: loggregatorCA
        is_ca: true
      type: certificate
    - name: loggregator_tls_statsdinjector
      options:
        ca: loggregator_ca
        common_name: statsdinjector
        extended_key_usage:
        - client_auth
      type: certificate
    - name: loggregator_tls_agent
      options:
        ca: loggregator_ca
        common_name: metron
        extended_key_usage:
        - client_auth
        - server_auth
      type: certificate
    - name: loggregator_tls_doppler
      options:
        ca: loggregator_ca
        common_name: doppler
        extended_key_usage:
        - client_auth
        - server_auth
      type: certificate
    - name: loggregator_tls_tc
      options:
        ca: loggregator_ca
        common_name: trafficcontroller
        extended_key_usage:
        - client_auth
        - server_auth
      type: certificate
    - name: loggregator_tls_cc_tc
      options:
        ca: service_cf_internal_ca
        common_name: trafficcontroller
        extended_key_usage:
        - client_auth
      type: certificate
    - name: loggregator_rlp_gateway_tls_cc
      options:
        ca: service_cf_internal_ca
        common_name: rlp-gateway
        extended_key_usage:
        - client_auth
      type: certificate
    - name: loggregator_tls_rlp
      options:
        ca: loggregator_ca
        common_name: reverselogproxy
        extended_key_usage:
        - client_auth
        - server_auth
      type: certificate
    - name: loggregator_rlp_gateway
      options:
        ca: loggregator_ca
        common_name: rlp_gateway
        extended_key_usage:
        - client_auth
      type: certificate
    - name: logs_provider
      options:
        ca: loggregator_ca
        common_name: log-cache
        extended_key_usage:
        - client_auth
        - server_auth
      type: certificate
    - name: log_cache_ca
      options:
        common_name: log-cache
        is_ca: true
      type: certificate
    - name: log_cache
      options:
        alternative_names:
        - log_cache
        - log-cache
        - logcache
        ca: log_cache_ca
        common_name: log-cache
        extended_key_usage:
        - client_auth
        - server_auth
      type: certificate
    - name: log_cache_to_loggregator_agent
      options:
        ca: loggregator_ca
        common_name: log-cache
        extended_key_usage:
        - client_auth
      type: certificate
    - name: cc_logcache_tls
      options:
        alternative_names:
        - api.((system_domain))
        - cloud-controller-ng.service.cf.internal
        ca: log_cache_ca
        common_name: api.((system_domain))
      type: certificate
    - name: logcache_ssl
      options:
        alternative_names:
        - log-cache.((system_domain))
        - '*.log-cache.((system_domain))'
        ca: service_cf_internal_ca
        common_name: log-cache
      type: certificate
    - name: log_cache_proxy_tls
      options:
        ca: log_cache_ca
        common_name: localhost
      type: certificate
    - name: router_ca
      options:
        common_name: routerCA
        is_ca: true
      type: certificate
    - name: router_ssl
      options:
        alternative_names:
        - ((system_domain))
        - '*.((system_domain))'
        ca: router_ca
        common_name: routerSSL
      type: certificate
    - name: routing_api_ca
      options:
        common_name: routing_api
        is_ca: true
      type: certificate
    - name: routing_api_tls
      options:
        ca: routing_api_ca
        common_name: routing-api.service.cf.internal
        extended_key_usage:
        - server_auth
      type: certificate
    - name: routing_api_tls_client
      options:
        ca: routing_api_ca
        common_name: routing-api-client
        extended_key_usage:
        - client_auth
      type: certificate
    - name: uaa_ca
      options:
        common_name: uaaCA
        is_ca: true
      type: certificate
    - name: uaa_ssl
      options:
        alternative_names:
        - uaa.service.cf.internal
        ca: uaa_ca
        common_name: uaa.service.cf.internal
      type: certificate
    - name: uaa_login_saml
      options:
        ca: uaa_ca
        common_name: uaa_login_saml
      type: certificate
    - name: cc_tls
      options:
        ca: service_cf_internal_ca
        common_name: cloud-controller-ng.service.cf.internal
        extended_key_usage:
        - client_auth
        - server_auth
      type: certificate
    - name: cc_public_tls
      options:
        alternative_names:
        - api.((system_domain))
        - cloud-controller-ng.service.cf.internal
        ca: service_cf_internal_ca
        common_name: api.((system_domain))
      type: certificate
    - name: cc_bridge_tps
      options:
        ca: service_cf_internal_ca
        common_name: tps_watcher
        extended_key_usage:
        - client_auth
      type: certificate
    - name: cc_bridge_cc_uploader
      options:
        ca: service_cf_internal_ca
        common_name: cc_uploader
        extended_key_usage:
        - client_auth
      type: certificate
    - name: cc_bridge_cc_uploader_server
      options:
        ca: service_cf_internal_ca
        common_name: cc-uploader.service.cf.internal
        extended_key_usage:
        - server_auth
      type: certificate
    - name: diego_locket_server
      options:
        alternative_names:
        - '*.locket.service.cf.internal'
        - locket.service.cf.internal
        ca: service_cf_internal_ca
        common_name: locket.service.cf.internal
        extended_key_usage:
        - server_auth
      type: certificate
    - name: diego_locket_client
      options:
        ca: service_cf_internal_ca
        common_name: locket client
        extended_key_usage:
        - client_auth
      type: certificate
    - name: locket_database_password
      type: password
    - name: application_ca
      options:
        common_name: appRootCA
        is_ca: true
      type: certificate
    - name: diego_instance_identity_ca
      options:
        ca: application_ca
        common_name: instanceIdentityCA
        is_ca: true
      type: certificate
    - name: gorouter_backend_tls
      options:
        alternative_names:
        - gorouter.service.cf.internal
        ca: service_cf_internal_ca
        common_name: gorouter_backend_tls
        extended_key_usage:
        - client_auth
      type: certificate
    - name: credhub_ca
      options:
        common_name: credhubServerCa
        is_ca: true
      type: certificate
    - name: credhub_tls
      options:
        alternative_names:
        - credhub.service.cf.internal
        - credhub.((system_domain))
        ca: credhub_ca
        common_name: credhub.((system_domain))
      type: certificate
    - name: ssh_proxy_backends_tls
      options:
        alternative_names:
        - ssh-proxy.service.cf.internal
        ca: service_cf_internal_ca
        common_name: ssh_proxy_backends_tls
        extended_key_usage:
        - client_auth
      type: certificate
    - name: pxc_galera_ca
      options:
        common_name: pxc_galera_ca
        is_ca: true
      type: certificate
    - name: pxc_server_ca
      options:
        common_name: pxc_server_ca
        is_ca: true
      type: certificate
    - name: galera_server_certificate
      options:
        ca: pxc_galera_ca
        common_name: galera_server_certificate
        extended_key_usage:
        - server_auth
        - client_auth
      type: certificate
    - name: mysql_server_certificate
      options:
        ca: pxc_server_ca
        common_name: sql-db.service.cf.internal
      type: certificate
    - name: loggregator_rlp_gateway_tls
      options:
        alternative_names:
        - log-stream.((system_domain))
        - log-api.service.cf.internal
        ca: service_cf_internal_ca
        common_name: log-stream.((system_domain))
      type: certificate
    - name: loggregator_trafficcontroller_tls
      options:
        alternative_names:
        - doppler.((system_domain))
        - log-api.service.cf.internal
        ca: service_cf_internal_ca
        common_name: doppler.((system_domain))
      type: certificate
    - name: metric_scraper_ca
      options:
        common_name: metricScraperCA
        is_ca: true
      type: certificate
    - name: metrics_agent_tls
      options:
        ca: metric_scraper_ca
        common_name: metrics_agent
        extended_key_usage:
        - server_auth
      type: certificate
    - name: metrics_discovery_metrics_tls
      options:
        ca: metric_scraper_ca
        common_name: metrics_discovery_metrics
        extended_key_usage:
        - server_auth
      type: certificate
    - name: scrape_config_generator_metrics_tls
      options:
        ca: metric_scraper_ca
        common_name: scrape_config_generator_metrics
        extended_key_usage:
        - server_auth
      type: certificate
    - name: log_cache_metrics_tls
      options:
        ca: metric_scraper_ca
        common_name: log_cache_metrics
        extended_key_usage:
        - server_auth
      type: certificate
    - name: log_cache_cf_auth_proxy_metrics_tls
      options:
        ca: metric_scraper_ca
        common_name: log_cache_cf_auth_proxy_metrics
        extended_key_usage:
        - server_auth
      type: certificate
    - name: log_cache_gateway_metrics_tls
      options:
        ca: metric_scraper_ca
        common_name: log_cache_gateway_metrics
        extended_key_usage:
        - server_auth
      type: certificate
    - name: forwarder_agent_metrics_tls
      options:
        ca: metric_scraper_ca
        common_name: forwarder_agent_metrics
        extended_key_usage:
        - server_auth
      type: certificate
    - name: loggregator_agent_metrics_tls
      options:
        ca: metric_scraper_ca
        common_name: loggregator_agent_metrics
        extended_key_usage:
        - server_auth
      type: certificate
    - name: loggr_udp_forwarder_tls
      options:
        ca: metric_scraper_ca
        common_name: loggr_udp_forwarder_metrics
        extended_key_usage:
        - server_auth
      type: certificate
    - name: syslog_agent_api_tls
      options:
        ca: loggregator_ca
        common_name: syslog-agent
        extended_key_usage:
        - client_auth
      type: certificate
    - name: binding_cache_api_tls
      options:
        ca: service_cf_internal_ca
        common_name: binding-cache
        extended_key_usage:
        - client_auth
      type: certificate
    - name: binding_cache_tls
      options:
        ca: loggregator_ca
        common_name: binding-cache
        extended_key_usage:
        - server_auth
      type: certificate
    - name: syslog_agent_metrics_tls
      options:
        ca: metric_scraper_ca
        common_name: syslog_agent_metrics
        extended_key_usage:
        - server_auth
      type: certificate
    - name: loggr_syslog_binding_cache_metrics_tls
      options:
        ca: metric_scraper_ca
        common_name: loggr_syslog_binding_cache_metrics
        extended_key_usage:
        - server_auth
      type: certificate
    - name: prom_scraper_scrape_tls
      options:
        ca: metric_scraper_ca
        common_name: prom_scraper
        extended_key_usage:
        - client_auth
      type: certificate
    - name: prom_scraper_metrics_tls
      options:
        ca: metric_scraper_ca
        common_name: prom_scraper_metrics
        extended_key_usage:
        - server_auth
      type: certificate
    - name: rlp_gateway_metrics_tls
      options:
        ca: metric_scraper_ca
        common_name: rlp_gateway_metrics
        extended_key_usage:
        - server_auth
      type: certificate
    - name: nats_internal_ca
      options:
        common_name: nats_internal
        is_ca: true
      type: certificate
    - name: nats_internal_cert
      options:
        alternative_names:
        - '*.nats.service.cf.internal'
        - nats.service.cf.internal
        ca: nats_internal_ca
        common_name: '*.nats.service.cf.internal'
        extended_key_usage:
        - client_auth
        - server_auth
      type: certificate
    - name: nats_ca
      options:
        common_name: nats
        is_ca: true
      type: certificate
    - name: nats_client_cert
      options:
        ca: nats_ca
        common_name: nats_client
        extended_key_usage:
        - client_auth
      type: certificate
    - consumes:
        alternative_name:
          from: nats-tls-address
          properties:
            wildcard: true
      name: nats_server_cert
      options:
        ca: nats_ca
        common_name: nats.service.cf.internal
        extended_key_usage:
        - server_auth
      type: certificate
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: cf-deployment
  namespace: default
---
apiVersion: v1
data:
  charset.cnf: |-
    [client]
    default_character_set           = utf8

    [mysql]
    default_character_set           = utf8

    [mysqld]
    # Ignore the client information and use the default server character set.
    character_set_client_handshake  = false

    character_set_server            = utf8
    collation_server                = utf8_unicode_ci

    [mysqld_safe]
    default_character_set           = utf8
  node.cnf: |
    [mysqld]
    datadir=/var/lib/mysql
    default_storage_engine=InnoDB
    binlog_format=ROW
    innodb_flush_log_at_trx_commit  = 0
    innodb_flush_method             = O_DIRECT
    innodb_file_per_table           = 1
    innodb_autoinc_lock_mode=2
    bind_address = 0.0.0.0
    wsrep_on = off
  ssl.cnf: |
    [mysqld]
    ssl-ca=/etc/mysql/tls/certs/ca
    ssl-cert=/etc/mysql/tls/certs/certificate
    ssl-key=/etc/mysql/tls/certs/private_key
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: database
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: database-config-files
  namespace: default
---
apiVersion: v1
data:
  entrypoint.sh: |
    #!/bin/bash

    #    Copyright The Helm Authors.
    #
    #    Licensed under the Apache License, Version 2.0 (the "License");
    #    you may not use this file except in compliance with the License.
    #    You may obtain a copy of the License at
    #
    #        http://www.apache.org/licenses/LICENSE-2.0
    #
    #    Unless required by applicable law or agreed to in writing, software
    #    distributed under the License is distributed on an "AS IS" BASIS,
    #    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    #    See the License for the specific language governing permissions and
    #    limitations under the License.
    #
    # This file was obtained from:
    # https://github.com/helm/charts/blob/7ccc9f99d7ab6b9554624985d9c9b9723b7253c0/stable/percona-xtradb-cluster/files/entrypoint.sh

    set -e

    if [[ -n "${DEBUG}" ]]; then
        set -x
    fi

    # shellcheck disable=SC1091
    . /startup-scripts/functions.sh

    # if command starts with an option, prepend mysqld
    if [ "${1:0:1}" = '-' ]; then
        CMDARG=( "$@" )
    fi

    init_mysql
    write_password_file
    exec mysqld --user=mysql --pxc_strict_mode="$PXC_STRICT_MODE" "${CMDARG[@]}"
  functions.sh: |
    #!/bin/bash

    #    Copyright The Helm Authors.
    #
    #    Licensed under the Apache License, Version 2.0 (the "License");
    #    you may not use this file except in compliance with the License.
    #    You may obtain a copy of the License at
    #
    #        http://www.apache.org/licenses/LICENSE-2.0
    #
    #    Unless required by applicable law or agreed to in writing, software
    #    distributed under the License is distributed on an "AS IS" BASIS,
    #    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    #    See the License for the specific language governing permissions and
    #    limitations under the License.
    #
    # This file was obtained from:
    # https://github.com/helm/charts/blob/7ccc9f99d7ab6b9554624985d9c9b9723b7253c0/stable/percona-xtradb-cluster/files/functions.sh

    write_password_file() {
        if [[ -n "${MYSQL_ROOT_PASSWORD}" ]]; then
            cat <<EOF > /root/.my.cnf
            [client]
            user=root
            password=${MYSQL_ROOT_PASSWORD}
    EOF
        fi
    }

    # Wait for the MySQL instance meant for initialization only.
    # Expects `mysql()` to be set.
    wait_for_init_daemon() {
        for i in {30..0}; do
            if echo 'SELECT 1' | "${mysql[@]}" &> /dev/null; then
                break
            fi
            echo 'MySQL init process in progress...'
            sleep 1
        done

        if [ "$i" = 0 ]; then
            echo >&2 'MySQL init process failed.'
            exit 1
        fi
    }

    reset_root_password_statement() {
        if [ "$PERCONA_MAJOR" = "5.6" ]; then
            echo "SET PASSWORD FOR 'root'@'localhost' = PASSWORD('${MYSQL_ROOT_PASSWORD}');"
            echo "SET PASSWORD FOR 'root'@'${ALLOW_ROOT_FROM}' = PASSWORD('${MYSQL_ROOT_PASSWORD}');"
        else
            echo "ALTER USER 'root'@'localhost' IDENTIFIED BY '${MYSQL_ROOT_PASSWORD}';"
            echo "ALTER USER 'root'@'${ALLOW_ROOT_FROM}' IDENTIFIED BY '${MYSQL_ROOT_PASSWORD}';"
        fi
    }

    init_mysql() {
        SENTINEL=INIT_MYSQL_DONE
        DATADIR=/var/lib/mysql

        if [ -n "$MYSQL_ROOT_PASSWORD_FILE" ] && [ -z "$MYSQL_ROOT_PASSWORD" ]; then
            MYSQL_ROOT_PASSWORD=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
        fi

        if [ ! -e "$DATADIR/$SENTINEL" ]; then
            echo "Removing pending files in $DATADIR, because sentinel was not reached"
            rm -rf "${DATADIR:?}"/*
            if [ -z "$MYSQL_ROOT_PASSWORD" ] && [ -z "$MYSQL_ALLOW_EMPTY_PASSWORD" ] && [ -z "$MYSQL_RANDOM_ROOT_PASSWORD" ] && [ -z "$MYSQL_ROOT_PASSWORD_FILE" ]; then
                echo >&2 'error: database is uninitialized and password option is not specified '
                echo >&2 '  You need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ROOT_PASSWORD_FILE,  MYSQL_ALLOW_EMPTY_PASSWORD or MYSQL_RANDOM_ROOT_PASSWORD'
                exit 1
            fi

            mkdir -p "$DATADIR"

            echo "Running --initialize-insecure on $DATADIR"
            ls -lah $DATADIR
            if [ "$PERCONA_MAJOR" = "5.6" ]; then
                mysql_install_db --user=mysql --datadir="$DATADIR"
            else
                mysqld --user=mysql --datadir="$DATADIR" --initialize-insecure
            fi
            chown -R mysql:mysql "$DATADIR" || true # default is root:root 777
            if [ -f /var/log/mysqld.log ]; then
                chown mysql:mysql /var/log/mysqld.log
            fi
            echo 'Finished --initialize-insecure'

            mysqld --user=mysql --datadir="$DATADIR" --skip-networking &
            pid="$!"

            mysql=( mysql "--protocol=socket" -uroot )

            wait_for_init_daemon


            # sed is for https://bugs.mysql.com/bug.php?id=20545
            mysql_tzinfo_to_sql /usr/share/zoneinfo | sed 's/Local time zone must be set--see zic manual page/FCTY/' | "${mysql[@]}" mysql
            "${mysql[@]}" <<-EOSQL
            -- What's done in this file shouldn't be replicated
            --  or products like mysql-fabric won't work
            SET @@SESSION.SQL_LOG_BIN=0;
            CREATE USER 'root'@'${ALLOW_ROOT_FROM}' IDENTIFIED BY '${MYSQL_ROOT_PASSWORD}' ;
            GRANT ALL ON *.* TO 'root'@'${ALLOW_ROOT_FROM}' WITH GRANT OPTION ;
            GRANT ALL ON *.* TO 'root'@'localhost' WITH GRANT OPTION ;
            CREATE USER 'xtrabackup'@'localhost' IDENTIFIED BY '$XTRABACKUP_PASSWORD';
            GRANT RELOAD,PROCESS,LOCK TABLES,REPLICATION CLIENT ON *.* TO 'xtrabackup'@'localhost';
            GRANT REPLICATION CLIENT ON *.* TO monitor@'%' IDENTIFIED BY 'monitor';
            GRANT PROCESS ON *.* TO monitor@localhost IDENTIFIED BY 'monitor';
            CREATE USER 'mysql'@'localhost' IDENTIFIED BY '' ;
            DROP DATABASE IF EXISTS test ;
            FLUSH PRIVILEGES ;
            $(reset_root_password_statement) ;
            FLUSH PRIVILEGES ;
    EOSQL

            if [ -n "$MYSQL_ROOT_PASSWORD" ]; then
                mysql+=( -p"${MYSQL_ROOT_PASSWORD}" )
            fi

            if [ "$MYSQL_DATABASE" ]; then
                echo "CREATE DATABASE IF NOT EXISTS \`$MYSQL_DATABASE\` ;" | "${mysql[@]}"
                mysql+=( "$MYSQL_DATABASE" )
            fi

            if [ "$MYSQL_USER" ] && [ "$MYSQL_PASSWORD" ]; then
                echo "CREATE USER '""$MYSQL_USER""'@'%' IDENTIFIED BY '""$MYSQL_PASSWORD""' ;" | "${mysql[@]}"

                if [ "$MYSQL_DATABASE" ]; then
                    echo "GRANT ALL ON \`""$MYSQL_DATABASE""\`.* TO '""$MYSQL_USER""'@'%' ;" | "${mysql[@]}"
                fi

                echo 'FLUSH PRIVILEGES ;' | "${mysql[@]}"
            fi

            if [ -n "$MYSQL_ONETIME_PASSWORD" ]; then
                "${mysql[@]}" <<-EOSQL
                ALTER USER 'root'@'%' PASSWORD EXPIRE;
    EOSQL
            fi
            if ! kill -s TERM "$pid" || ! wait "$pid"; then
                echo >&2 'MySQL init process failed.'
                exit 1
            fi

            echo
            echo 'MySQL init process done. Ready for start up.'
            echo
            touch "$DATADIR/$SENTINEL"
        elif [[ -n "$MYSQL_ROOT_PASSWORD" ]]; then
            echo 'Resetting MySQL root password.'
            mysqld --user=mysql --datadir="$DATADIR" --skip-networking --skip-grant-tables &
            pid="$!"

            mysql=( mysql "--protocol=socket" -uroot )
            wait_for_init_daemon

            "${mysql[@]}" <<-EOSQL
                FLUSH PRIVILEGES ;
                $(reset_root_password_statement) ;
                FLUSH PRIVILEGES ;
    EOSQL
            if ! kill -s TERM "$pid" || ! wait "$pid"; then
                echo >&2 'MySQL init process failed.'
                exit 1
            fi

            echo
            echo 'MySQL init process done. Ready for start up.'
            echo
        fi
    }
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: database
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: database-startup-scripts
  namespace: default
---
apiVersion: v1
data:
  ops: ""
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-acceptance-tests
  namespace: default
---
apiVersion: v1
data:
  ops: "\n\n# These operations set the default values explicitly in order to get the
    rotate-cc-database-key\n# errand working. Without them, the rotate-cc-database-key
    errand is not able to resolve the CC BOSH\n# link correctly.\n- type: replace\n
    \ path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/ccdb/max_connections?\n
    \ value: 25\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/ccdb/pool_timeout?\n
    \ value: 10\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/ccdb/ssl_verify_hostname?\n
    \ value: true\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/ccdb/read_timeout?\n
    \ value: 3600\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/ccdb/connection_validation_timeout?\n
    \ value: 3600\n- type: replace\n  path: /variables/-\n  value:\n    name: ccdb_key_label_encryption_key_0\n
    \   type: password\n\n# Set cc/database_encryption property in all CAPI jobs\n-
    path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock?/properties/cc/database_encryption\n
    \ type: replace\n  value: {\"current_key_label\":\"encryption_key_0\",\"keys\":{\"encryption_key_0\":\"((ccdb_key_label_encryption_key_0))\"}}\n-
    path: /instance_groups/name=api/jobs/name=cloud_controller_ng?/properties/cc/database_encryption\n
    \ type: replace\n  value: {\"current_key_label\":\"encryption_key_0\",\"keys\":{\"encryption_key_0\":\"((ccdb_key_label_encryption_key_0))\"}}\n-
    path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker?/properties/cc/database_encryption\n
    \ type: replace\n  value: {\"current_key_label\":\"encryption_key_0\",\"keys\":{\"encryption_key_0\":\"((ccdb_key_label_encryption_key_0))\"}}\n\n#
    core_file_pattern should be disabled as CC is not running on a VM.\n- path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock?/properties/cc/core_file_pattern\n
    \ type: replace\n  value: false\n- path: /instance_groups/name=api/jobs/name=cloud_controller_ng?/properties/cc/core_file_pattern\n
    \ type: replace\n  value: false\n- path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker?/properties/cc/core_file_pattern\n
    \ type: replace\n  value: false\n\n# Disable tuning /proc/sys kernel parameters
    as file_server is running on a container.\n- type: replace\n  path: /instance_groups/name=api/jobs/name=file_server/properties/set_kernel_parameters?\n
    \ value: false\n\n# We don't have a /var/vcap/job/*/packages directory, so we
    point to all the packages.\n- type: replace\n  path: /instance_groups/name=api/jobs/name=file_server/properties/diego?/file_server/static_directory\n
    \ value: \"/var/vcap/packages/\"\n\n# Enable volume services\n- path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock?/properties/cc/volume_services_enabled\n
    \ type: replace\n  value: true\n- path: /instance_groups/name=api/jobs/name=cloud_controller_ng?/properties/cc/volume_services_enabled\n
    \ type: replace\n  value: true\n- path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker?/properties/cc/volume_services_enabled\n
    \ type: replace\n  value: true\n\n# Add quarks properties for cloud_controller_ng.\n-
    type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/quarks?\n
    \ value:\n    ports:\n    - name: api\n      protocol: TCP\n      internal: 9022\n
    \   - name: api-tls\n      protocol: TCP\n      internal: 9023\n    - name: api-mutual-tls\n
    \     protocol: TCP\n      internal: 9024\n    run:\n      healthcheck:\n        ccng_monit_http_healthcheck:\n
    \         readiness:\n            # This job exists just to healthcheck cloud_controller_ng;
    we're\n            # already doing that separately.\n            exec:\n              command:
    [/bin/true]\n          liveness:\n            exec:\n              command:\n
    \             - /usr/bin/pgrep\n              - --full\n              - --exact\n
    \             - bash /var/vcap/jobs/cloud_controller_ng/bin/ccng_monit_http_healthcheck\n
    \       cloud_controller_ng:\n          readiness: &cloud_controller_ng_readiness\n
    \           exec:\n              command:\n              - curl\n              -
    --fail\n              - --head\n              - --silent\n              - --unix-socket\n
    \             - /var/vcap/data/cloud_controller_ng/cloud_controller.sock\n              -
    http:/healthz\n          # We don't want a liveness probe here as we do migration
    here, and we\n          # do not want to interrupt that.  We may want to consider
    using a\n          # startupProbe in the future (once that feature stabilizes).\n
    \         liveness: ~\n        local_worker_1:\n          readiness:\n            exec:\n
    \             command: [/usr/bin/pgrep, --full, cc_api_worker]\n        local_worker_2:\n
    \         readiness:\n            exec:\n              command: [/usr/bin/pgrep,
    --full, cc_api_worker]\n        nginx:\n          readiness:\n            httpGet:\n
    \             httpHeaders:\n              - name: Host\n                value:
    api\n              path: /healthz\n              port: 9024\n              scheme:
    HTTPS\n          liveness:\n            exec:\n              command: [/usr/bin/pgrep,
    --full, \"nginx: master process\"]\n    post_start:\n      condition:\n        exec:\n
    \         command: [\"curl\", \"--fail\", \"--head\", \"--silent\", \"http://127.0.0.1:9022/healthz\"]\n\n#
    Add quarks properties for cc_uploader.\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cc_uploader/properties/quarks?\n
    \ value:\n    ports:\n    - name: http\n      protocol: TCP\n      internal: 9090\n
    \   - name: https\n      protocol: TCP\n      internal: 9091\n    run:\n      healthcheck:\n
    \       cc_uploader:\n          readiness:\n            # cc-uploader does not
    have a health check endpoint; just use a TCP\n            # socket.\n            tcpSocket:\n
    \             port: 9091\n\n# Add quarks properties for file_server.\n- type:
    replace\n  path: /instance_groups/name=api/jobs/name=file_server/properties/quarks?\n
    \ value:\n    ports:\n    - name: file-server\n      protocol: TCP\n      internal:
    &file-server-port 8080\n    run:\n      healthcheck:\n        file_server:\n          readiness:\n
    \           httpGet:\n              path: /v1/static/file_server/bin/file-server\n
    \             port: *file-server-port\n\n# Add quarks properties for statsd_injector.\n-
    type: replace\n  path: /instance_groups/name=api/jobs/name=statsd_injector/properties/quarks?/run/healthcheck/statsd_injector/readiness/exec/command\n
    \ value:\n  - /bin/sh\n  - -c\n  - ss -nlu src localhost:8125 | grep :8125\n\n#
    Add quarks properties for policy-server.\n- type: replace\n  path: /instance_groups/name=api/jobs/name=policy-server/properties/quarks?\n
    \ value:\n    ports:\n    - name: policy-server\n      protocol: TCP\n      internal:
    4002\n    run:\n      healthcheck:\n        policy-server:\n          readiness:\n
    \           httpGet:\n              port: 4002\n              scheme: HTTPS\n
    \   post_start:\n      condition:\n        exec:\n          # policy-server doesn't
    support HTTP HEAD requests\n          command: [\"curl\", \"--insecure\", \"--fail\",
    \"--silent\", \"https://localhost:4002/\"]\n\n# Add quarks properties for policy-server-internal.\n-
    type: replace\n  path: /instance_groups/name=api/jobs/name=policy-server-internal/properties/quarks?\n
    \ value:\n    run:\n      healthcheck:\n        policy-server-internal:\n          readiness:
    &policy_server_internal_readiness\n            httpGet:\n              port: 31946\n
    \   post_start:\n      condition:\n        exec:\n          # policy-server-internal
    doesn't support HTTP HEAD requests\n          command: [\"curl\", \"--fail\",
    \"--silent\", \"http://localhost:31946/\"]\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=route_registrar/properties/quarks?/run/healthcheck/route_registrar\n
    \ value:\n    readiness: ~\n      # The route registrar doesn't expose anything
    to indicate if the\n      # routes are healthy\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=loggr-udp-forwarder/properties/quarks?\n
    \ value:\n    run:\n      healthcheck:\n        loggr-udp-forwarder:\n          readiness:\n
    \           exec:\n              command: [\"sh\", \"-c\", \"ss -nlu sport = 3457
    | grep :3457\"]\n    envs:\n    - name: INDEX\n      valueFrom:\n        fieldRef:\n
    \         fieldPath: metadata.labels['statefulset.kubernetes.io/pod-name']\n    -
    name: IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n-
    type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n
    \   target=\"/var/vcap/all-releases/jobs-src/capi/cloud_controller_ng/templates/bin/cloud_controller_ng.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    patch --verbose \"${target}\" <<'EOT'\n    @@
    -6,6 +6,7 @@ source /var/vcap/jobs/cloud_controller_ng/bin/blobstore_waiter.sh\n
    \    wait_for_blobstore\n    \n     cd /var/vcap/packages/cloud_controller_ng/cloud_controller_ng\n
    \   +patch -p0 < /var/vcap/all-releases/jobs-src/capi/cloud_controller_ng/yaml-anchor.patch\n
    \   \n     <% if spec.bootstrap && !p('cc.run_prestart_migrations') %>\n     echo
    'Running migrations and seeds'\n    EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\"\n
    \   \n    target=\"/var/vcap/all-releases/jobs-src/capi/cloud_controller_ng/templates/bin/local_worker.erb\"\n
    \   patch --verbose \"${target}\" <<'EOT'\n    @@ -6,4 +6,6 @@ source /var/vcap/jobs/cloud_controller_ng/bin/blobstore_waiter.sh\n
    \    wait_for_blobstore\n    \n     cd /var/vcap/packages/cloud_controller_ng/cloud_controller_ng\n
    \   +patch -p0 < /var/vcap/all-releases/jobs-src/capi/cloud_controller_ng/yaml-anchor.patch\n
    \   +\n     exec bundle exec rake \"jobs:local[cc_api_worker.<%= spec.job.name
    %>.<%= spec.index %>.${INDEX}]\"\n    EOT\n    \n    cat <<'EOT' > /var/vcap/all-releases/jobs-src/capi/cloud_controller_ng/yaml-anchor.patch\n
    \   diff --git app/controllers/v3/app_manifests_controller.rb app/controllers/v3/app_manifests_controller.rb\n
    \   index ed3ca78a6..9828cd7ed 100644\n    --- app/controllers/v3/app_manifests_controller.rb\n
    \   +++ app/controllers/v3/app_manifests_controller.rb\n    @@ -70,7 +70,7 @@
    class AppManifestsController < ApplicationController\n       def validate_content_type!\n
    \        if !request_content_type_is_yaml?\n           logger.error(\"Context-type
    isn't yaml: #{request.content_type}\")\n    -      invalid_request!('Content-Type
    must be yaml')\n    +      bad_request!('Content-Type must be yaml')\n         end\n
    \      end\n    \n    @@ -79,10 +79,10 @@ class AppManifestsController < ApplicationController\n
    \      end\n    \n       def parsed_app_manifest_params\n    -    parsed_application
    = params[:body]['applications'] && params[:body]['applications'].first\n    +
    \   parsed_application = parsed_yaml['applications'] && parsed_yaml['applications'].first\n
    \   \n    -    raise invalid_request!('Invalid app manifest') unless parsed_application.present?\n
    \   +    raise bad_request!('Invalid app manifest') unless parsed_application.present?\n
    \   \n    -    parsed_application.to_unsafe_h\n    +    parsed_application\n       end\n
    \    end\n    diff --git app/controllers/v3/application_controller.rb app/controllers/v3/application_controller.rb\n
    \   index 8221b866d..1a7e57757 100644\n    --- app/controllers/v3/application_controller.rb\n
    \   +++ app/controllers/v3/application_controller.rb\n    @@ -30,6 +30,10 @@ module
    V3ErrorsHelper\n         raise CloudController::Errors::ApiError.new_from_details('BadRequest',
    message)\n       end\n    \n    +  def message_parse_error!(message)\n    +    raise
    CloudController::Errors::ApiError.new_from_details('MessageParseError', message)\n
    \   +  end\n    +\n       def service_unavailable!(message)\n         raise CloudController::Errors::ApiError.new_from_details('ServiceUnavailable',
    message)\n       end\n    @@ -80,6 +84,17 @@ class ApplicationController < ActionController::Base\n
    \        JSON.parse(request.body.string)\n       end\n    \n    +  def parsed_yaml\n
    \   +    return @parsed_yaml if @parsed_yaml\n    +\n    +    allow_yaml_aliases
    = false\n    +    yaml = YAML.safe_load(request.body.string, [], [], allow_yaml_aliases)\n
    \   +    message_parse_error!('invalid request body') if !yaml.is_a? Hash\n    +
    \   @parsed_yaml = yaml\n    +  rescue Psych::BadAlias\n    +    bad_request!('Manifest
    does not support Anchors and Aliases')\n    +  end\n    +\n       def roles\n
    \        VCAP::CloudController::SecurityContext.roles\n       end\n    diff --git
    app/controllers/v3/space_manifests_controller.rb app/controllers/v3/space_manifests_controller.rb\n
    \   index 3f9f0db48..b213925bb 100644\n    --- app/controllers/v3/space_manifests_controller.rb\n
    \   +++ app/controllers/v3/space_manifests_controller.rb\n    @@ -15,7 +15,7 @@
    class SpaceManifestsController < ApplicationController\n         space_not_found!
    unless space && permission_queryer.can_read_from_space?(space.guid, space.organization.guid)\n
    \        unauthorized! unless permission_queryer.can_write_to_space?(space.guid)\n
    \   \n    -    messages = parsed_app_manifests.map(&:to_unsafe_h).map { |app_manifest|
    NamedAppManifestMessage.create_from_yml(app_manifest) }\n    +    messages = parsed_app_manifests.map
    { |app_manifest| NamedAppManifestMessage.create_from_yml(app_manifest) }\n         errors
    = messages.each_with_index.flat_map { |message, i| errors_for_message(message,
    i) }\n         compound_error!(errors) unless errors.empty?\n    \n    @@ -49,6
    +49,10 @@ class SpaceManifestsController < ApplicationController\n    \n         parsed_manifests
    = parsed_app_manifests.map(&:to_hash)\n    \n    +    messages = parsed_app_manifests.map
    { |app_manifest| NamedAppManifestMessage.create_from_yml(app_manifest) }\n    +
    \   errors = messages.each_with_index.flat_map { |message, i| errors_for_message(message,
    i) }\n    +    compound_error!(errors) unless errors.empty?\n    +\n         diff
    = SpaceDiffManifest.generate_diff(parsed_manifests, space)\n    \n         render
    status: :created, json: { diff: diff }\n    @@ -79,7 +83,7 @@ class SpaceManifestsController
    < ApplicationController\n       def validate_content_type!\n         if !request_content_type_is_yaml?\n
    \          logger.error(\"Content-type isn't yaml: #{request.content_type}\")\n
    \   -      invalid_request!('Content-Type must be yaml')\n    +      bad_request!('Content-Type
    must be yaml')\n         end\n       end\n    \n    @@ -88,13 +92,13 @@ class
    SpaceManifestsController < ApplicationController\n       end\n    \n       def
    check_version_is_supported!\n    -    version = params[:body]['version']\n    +
    \   version = parsed_yaml['version']\n         raise unprocessable!('Unsupported
    manifest schema version. Currently supported versions: [1].') unless !version
    || version == 1\n       end\n    \n       def parsed_app_manifests\n         check_version_is_supported!\n
    \   -    parsed_applications = params[:body].permit!['applications']\n    +    parsed_applications
    = parsed_yaml['applications']\n         raise unprocessable!(\"Cannot parse manifest
    with no 'applications' field.\") unless parsed_applications.present?\n    \n         parsed_applications\n
    \   diff --git config/application.rb config/application.rb\n    index a86039f69..d75394bf7
    100644\n    --- config/application.rb\n    +++ config/application.rb\n    @@ -2,18
    +2,6 @@ require 'action_controller/railtie'\n    \n     class Application < ::Rails::Application\n
    \      config.exceptions_app = self.routes\n    -\n    -  # For Rails 5 / Rack
    2 - this is how to add a new parser\n    -  original_parsers = ActionDispatch::Request.parameter_parsers\n
    \   -\n    -  allow_yaml_aliases = true\n    -  yaml_parser = lambda { |body|
    YAML.safe_load(body, [], [], allow_yaml_aliases).with_indifferent_access }\n    -
    \ new_parsers = original_parsers.merge({\n    -    Mime::Type.lookup('application/x-yaml')
    => yaml_parser,\n    -    Mime::Type.lookup('text/yaml') => yaml_parser,\n    -
    \ })\n    -  ActionDispatch::Request.parameter_parsers = new_parsers\n    -\n
    \      config.middleware.delete ActionDispatch::Session::CookieStore\n       config.middleware.delete
    ActionDispatch::Cookies\n       config.middleware.delete ActionDispatch::Flash\n
    \   EOT\n\n- type: replace\n  path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/quarks/pre_render_scripts/bpm/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n
    \   target=\"/var/vcap/all-releases/jobs-src/capi/cloud_controller_ng/templates/bpm.yml.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Patch a few things on the BPM:\n    #   - DYNO
    environment variable is not needed.\n    #   - We don't enable New Relic.\n    #
    \  - NGINX maintenance shouldn't run.\n    patch --verbose \"${target}\" <<'EOT'\n
    \   @@ -20,7 +20,7 @@\n         \"BUNDLE_GEMFILE\" => \"/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/Gemfile\",\n
    \        \"CLOUD_CONTROLLER_NG_CONFIG\" => \"/var/vcap/jobs/cloud_controller_ng/config/cloud_controller_ng.yml\",\n
    \        \"C_INCLUDE_PATH\" => \"/var/vcap/packages/libpq/include\",\n    -    \"DYNO\"
    => \"#{spec.job.name}-#{spec.index}\",\n    +    \"GRPC_DNS_RESOLVER\" => \"native\",\n
    \        \"HOME\" => \"/home/vcap\",\n         \"LANG\" => \"en_US.UTF-8\",\n
    \        \"LIBRARY_PATH\" => \"/var/vcap/packages/libpq/lib\",\n    @@ -79,8 +78,6
    @@\n       \"processes\" => [\n         cloud_controller_ng_config,\n         nginx_config,\n
    \   -    nginx_newrelic_plugin_config,\n    -    nginx_maintenance_config,\n         ccng_monit_http_healthcheck_config,\n
    \      ]\n     }\n    EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\""
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-api
  namespace: default
---
apiVersion: v1
data:
  ops: ""
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-app-autoscaler
  namespace: default
---
apiVersion: v1
data:
  ops: |-
    # Add quarks properties for the auctioneer job.
    - type: replace
      path: /instance_groups/name=auctioneer/jobs/name=auctioneer/properties/quarks?
      value:
        ports:
        - name: auctioneer
          protocol: TCP
          internal: 9016
        activePassiveProbes:
          auctioneer-auctioneer:
            exec:
              command:
              - bash
              - -ce
              - "head -c0 </dev/tcp/${HOSTNAME}/9016"
    # Set the alias auctioneer.service.cf.internal instance group to auctioneer.
    - type: replace
      path: /addons/name=bosh-dns-aliases/jobs/name=bosh-dns-aliases/properties/aliases/domain=auctioneer.service.cf.internal/targets/0/instance_group
      value: auctioneer
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-auctioneer
  namespace: default
---
apiVersion: v1
data:
  ops: ""
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-brain-tests
  namespace: default
---
apiVersion: v1
data:
  ops: "# Add quarks properties for cloud_controller_worker\n- type: replace\n  path:
    /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/quarks?/run/healthcheck/worker_1\n
    \ value:\n    readiness:\n      exec:\n        command: [/usr/bin/pgrep, --full,
    cc-worker-cloud_controller_worker]\n- type: replace\n  path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n
    \   target=\"/var/vcap/all-releases/jobs-src/capi/cloud_controller_worker/templates/bin/cloud_controller_worker.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    patch --verbose \"${target}\" <<'EOT'\n    @@
    -6,4 +6,6 @@ source /var/vcap/jobs/cloud_controller_worker/bin/blobstore_waiter.sh\n
    \    wait_for_blobstore\n    \n     cd /var/vcap/packages/cloud_controller_ng/cloud_controller_ng\n
    \   +patch -p0 < /var/vcap/all-releases/jobs-src/capi/cloud_controller_ng/yaml-anchor.patch\n
    \   +\n     exec bundle exec rake jobs:generic[cc_global_worker.<%= spec.job.name
    %>.<%= spec.index %>.${INDEX}]\n    EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\"\n
    \   \n    cat <<'EOT' > /var/vcap/all-releases/jobs-src/capi/cloud_controller_ng/yaml-anchor.patch\n
    \   diff --git app/controllers/v3/app_manifests_controller.rb app/controllers/v3/app_manifests_controller.rb\n
    \   index ed3ca78a6..9828cd7ed 100644\n    --- app/controllers/v3/app_manifests_controller.rb\n
    \   +++ app/controllers/v3/app_manifests_controller.rb\n    @@ -70,7 +70,7 @@
    class AppManifestsController < ApplicationController\n       def validate_content_type!\n
    \        if !request_content_type_is_yaml?\n           logger.error(\"Context-type
    isn't yaml: #{request.content_type}\")\n    -      invalid_request!('Content-Type
    must be yaml')\n    +      bad_request!('Content-Type must be yaml')\n         end\n
    \      end\n    \n    @@ -79,10 +79,10 @@ class AppManifestsController < ApplicationController\n
    \      end\n    \n       def parsed_app_manifest_params\n    -    parsed_application
    = params[:body]['applications'] && params[:body]['applications'].first\n    +
    \   parsed_application = parsed_yaml['applications'] && parsed_yaml['applications'].first\n
    \   \n    -    raise invalid_request!('Invalid app manifest') unless parsed_application.present?\n
    \   +    raise bad_request!('Invalid app manifest') unless parsed_application.present?\n
    \   \n    -    parsed_application.to_unsafe_h\n    +    parsed_application\n       end\n
    \    end\n    diff --git app/controllers/v3/application_controller.rb app/controllers/v3/application_controller.rb\n
    \   index 8221b866d..1a7e57757 100644\n    --- app/controllers/v3/application_controller.rb\n
    \   +++ app/controllers/v3/application_controller.rb\n    @@ -30,6 +30,10 @@ module
    V3ErrorsHelper\n         raise CloudController::Errors::ApiError.new_from_details('BadRequest',
    message)\n       end\n    \n    +  def message_parse_error!(message)\n    +    raise
    CloudController::Errors::ApiError.new_from_details('MessageParseError', message)\n
    \   +  end\n    +\n       def service_unavailable!(message)\n         raise CloudController::Errors::ApiError.new_from_details('ServiceUnavailable',
    message)\n       end\n    @@ -80,6 +84,17 @@ class ApplicationController < ActionController::Base\n
    \        JSON.parse(request.body.string)\n       end\n    \n    +  def parsed_yaml\n
    \   +    return @parsed_yaml if @parsed_yaml\n    +\n    +    allow_yaml_aliases
    = false\n    +    yaml = YAML.safe_load(request.body.string, [], [], allow_yaml_aliases)\n
    \   +    message_parse_error!('invalid request body') if !yaml.is_a? Hash\n    +
    \   @parsed_yaml = yaml\n    +  rescue Psych::BadAlias\n    +    bad_request!('Manifest
    does not support Anchors and Aliases')\n    +  end\n    +\n       def roles\n
    \        VCAP::CloudController::SecurityContext.roles\n       end\n    diff --git
    app/controllers/v3/space_manifests_controller.rb app/controllers/v3/space_manifests_controller.rb\n
    \   index 3f9f0db48..b213925bb 100644\n    --- app/controllers/v3/space_manifests_controller.rb\n
    \   +++ app/controllers/v3/space_manifests_controller.rb\n    @@ -15,7 +15,7 @@
    class SpaceManifestsController < ApplicationController\n         space_not_found!
    unless space && permission_queryer.can_read_from_space?(space.guid, space.organization.guid)\n
    \        unauthorized! unless permission_queryer.can_write_to_space?(space.guid)\n
    \   \n    -    messages = parsed_app_manifests.map(&:to_unsafe_h).map { |app_manifest|
    NamedAppManifestMessage.create_from_yml(app_manifest) }\n    +    messages = parsed_app_manifests.map
    { |app_manifest| NamedAppManifestMessage.create_from_yml(app_manifest) }\n         errors
    = messages.each_with_index.flat_map { |message, i| errors_for_message(message,
    i) }\n         compound_error!(errors) unless errors.empty?\n    \n    @@ -49,6
    +49,10 @@ class SpaceManifestsController < ApplicationController\n    \n         parsed_manifests
    = parsed_app_manifests.map(&:to_hash)\n    \n    +    messages = parsed_app_manifests.map
    { |app_manifest| NamedAppManifestMessage.create_from_yml(app_manifest) }\n    +
    \   errors = messages.each_with_index.flat_map { |message, i| errors_for_message(message,
    i) }\n    +    compound_error!(errors) unless errors.empty?\n    +\n         diff
    = SpaceDiffManifest.generate_diff(parsed_manifests, space)\n    \n         render
    status: :created, json: { diff: diff }\n    @@ -79,7 +83,7 @@ class SpaceManifestsController
    < ApplicationController\n       def validate_content_type!\n         if !request_content_type_is_yaml?\n
    \          logger.error(\"Content-type isn't yaml: #{request.content_type}\")\n
    \   -      invalid_request!('Content-Type must be yaml')\n    +      bad_request!('Content-Type
    must be yaml')\n         end\n       end\n    \n    @@ -88,13 +92,13 @@ class
    SpaceManifestsController < ApplicationController\n       end\n    \n       def
    check_version_is_supported!\n    -    version = params[:body]['version']\n    +
    \   version = parsed_yaml['version']\n         raise unprocessable!('Unsupported
    manifest schema version. Currently supported versions: [1].') unless !version
    || version == 1\n       end\n    \n       def parsed_app_manifests\n         check_version_is_supported!\n
    \   -    parsed_applications = params[:body].permit!['applications']\n    +    parsed_applications
    = parsed_yaml['applications']\n         raise unprocessable!(\"Cannot parse manifest
    with no 'applications' field.\") unless parsed_applications.present?\n    \n         parsed_applications\n
    \   diff --git config/application.rb config/application.rb\n    index a86039f69..d75394bf7
    100644\n    --- config/application.rb\n    +++ config/application.rb\n    @@ -2,18
    +2,6 @@ require 'action_controller/railtie'\n    \n     class Application < ::Rails::Application\n
    \      config.exceptions_app = self.routes\n    -\n    -  # For Rails 5 / Rack
    2 - this is how to add a new parser\n    -  original_parsers = ActionDispatch::Request.parameter_parsers\n
    \   -\n    -  allow_yaml_aliases = true\n    -  yaml_parser = lambda { |body|
    YAML.safe_load(body, [], [], allow_yaml_aliases).with_indifferent_access }\n    -
    \ new_parsers = original_parsers.merge({\n    -    Mime::Type.lookup('application/x-yaml')
    => yaml_parser,\n    -    Mime::Type.lookup('text/yaml') => yaml_parser,\n    -
    \ })\n    -  ActionDispatch::Request.parameter_parsers = new_parsers\n    -\n
    \      config.middleware.delete ActionDispatch::Session::CookieStore\n       config.middleware.delete
    ActionDispatch::Cookies\n       config.middleware.delete ActionDispatch::Flash\n
    \   EOT"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-cc-worker
  namespace: default
---
apiVersion: v1
data:
  ops: |2-


    # Remove directly from the cf-deployment.yml YAML file.
    - path: /addons/name=bosh-dns-aliases/jobs/name=bosh-dns-aliases/properties/aliases/domain=credhub.service.cf.internal?
      type: remove
    - path: /instance_groups/name=database/jobs/name=pxc-mysql/properties/seeded_databases/name=credhub?
      type: remove
    - path: /instance_groups/name=uaa/jobs/name=uaa/properties/uaa/clients/cc_service_key_client?
      type: remove
    - path: /instance_groups/name=uaa/jobs/name=uaa/properties/uaa/clients/credhub_admin_client?
      type: remove
    - path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/credhub_api?
      type: remove

    # XXX Is there a safer way to do edit the rep job property than to delete an array element by position?
    # XXX At least verify that trusted_ca_certificates[1] == "((credhub_tls.ca))" and fail if it doesn't?
    # XXX The current op could break silently during any cf-deployment upgrade that rearranges certs.
    - path: /instance_groups/name=diego-cell/jobs/name=rep/properties/containers/trusted_ca_certificates/1
      type: remove

    - path: /instance_groups/name=credhub?
      type: remove
    - path: /variables/name=credhub_encryption_password?
      type: remove
    - path: /variables/name=credhub_admin_client_secret?
      type: remove
    - path: /variables/name=credhub_database_password?
      type: remove
    - path: /variables/name=credhub_ca?
      type: remove
    - path: /variables/name=credhub_tls?
      type: remove
    - path: /releases/name=credhub?
      type: remove
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-credhub
  namespace: default
---
apiVersion: v1
data:
  ops: |-
    # Remove database instance group and variables.
    - type: remove
      path: /instance_groups/name=database

    # Remove the PXC release and related variables.
    - type: remove
      path: /releases/name=pxc
    - type: remove
      path: /variables/name=pxc_galera_ca
    - type: remove
      path: /variables/name=pxc_server_ca
    - type: remove
      path: /variables/name=galera_server_certificate
    - type: remove
      path: /variables/name=mysql_server_certificate

    # Add certificates for PXC.
    - type: replace
      path: /variables/-
      value:
        name: pxc_ca
        type: certificate
        options:
          common_name: pxc_ca
          is_ca: true
    - type: replace
      path: /variables/-
      value:
        name: pxc_tls
        type: certificate
        options:
          ca: pxc_ca
          common_name: &pxc-cluster-address database.default

    # Override the address and CA cert with the one from PXC.
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=bbs/properties/diego/bbs/sql/db_host?
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=bbs/properties/diego/bbs/sql/ca_cert?
      value: &pxc-cluster-ca ((pxc_tls.ca))

    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=locket/properties/diego/locket/sql/db_host?
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=locket/properties/diego/locket/sql/ca_cert?
      value: *pxc-cluster-ca

    - type: replace
      path: /instance_groups/name=uaa/jobs/name=uaa/properties/uaadb/address?
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=uaa/jobs/name=uaa/properties/uaa?/ca_certs
      value:
      - *pxc-cluster-ca

    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/ccdb/address?
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/ccdb/ca_cert?
      value: *pxc-cluster-ca

    - type: replace
      path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/ccdb/address?
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties/ccdb/ca_cert?
      value: *pxc-cluster-ca

    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/ccdb/address?
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/ccdb/ca_cert?
      value: *pxc-cluster-ca

    - type: replace
      path: /instance_groups/name=api/jobs/name=policy-server/properties/database/host
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=api/jobs/name=policy-server/properties/database/ca_cert?
      value: *pxc-cluster-ca

    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=silk-controller/properties/database/host
      value: *pxc-cluster-address
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=silk-controller/properties/database/ca_cert?
      value: *pxc-cluster-ca
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-database
  namespace: default
---
apiVersion: v1
data:
  ops: "# Override the addresses for the jobs under the diego-api instance group.\n-
    type: replace\n  path: /instance_groups/name=diego-api/jobs/name=bbs/properties/diego/bbs/locket?/api_location\n
    \ value: 127.0.0.1:8891\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=bbs/properties/diego/bbs/health_addr?\n
    \ value: 0.0.0.0:8890\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=cfdot/properties/bbs?/hostname\n
    \ value: 127.0.0.1\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=cfdot/properties/locket?/hostname\n
    \ value: 127.0.0.1\n- type: replace\n  path: /variables/name=diego_bbs_server/options?/alternative_names?/-\n
    \ value: '127.0.0.1'\n- type: replace\n  path: /variables/name=diego_locket_server/options?/alternative_names?/-\n
    \ value: '127.0.0.1'\n\n# Disable tuning /proc/sys kernel parameters as locket
    and bbs are running on containers.\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=locket/properties/set_kernel_parameters?\n
    \ value: false\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=bbs/properties/set_kernel_parameters?\n
    \ value: false\n\n# Add quarks properties for locket.\n- type: replace\n  path:
    /instance_groups/name=diego-api/jobs/name=locket/properties/quarks?\n  value:\n
    \   ports:\n    - name: locket\n      protocol: TCP\n      internal: 8891\n    run:\n
    \     healthcheck:\n        locket:\n          readiness:\n            exec:\n
    \             command:\n              - /var/vcap/packages/cfdot/bin/cfdot\n              -
    locks\n              - --locketAPILocation=127.0.0.1:8891\n              - --caCertFile=/var/vcap/jobs/cfdot/config/certs/cfdot/ca.crt\n
    \             - --clientCertFile=/var/vcap/jobs/cfdot/config/certs/cfdot/client.crt\n
    \             - --clientKeyFile=/var/vcap/jobs/cfdot/config/certs/cfdot/client.key\n\n#
    Add quarks properties for bbs.\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=bbs/properties/quarks?\n
    \ value:\n    ports:\n    - name: cell-bbs-api\n      protocol: TCP\n      internal:
    8889 # If you change this values, change the probe below too\n    activePassiveProbes:\n
    \     bbs-bbs:\n        exec:\n          command:\n          - bash\n          -
    -ce\n          - \"head -c0 </dev/tcp/${HOSTNAME}/8889\"\n\n- type: replace\n
    \ path: /instance_groups/name=diego-api/jobs/name=loggr-udp-forwarder/properties?/quarks/envs\n
    \ value:\n  - name: INDEX\n    valueFrom:\n      fieldRef:\n        fieldPath:
    metadata.labels['statefulset.kubernetes.io/pod-name']\n  - name: IP\n    valueFrom:\n
    \     fieldRef:\n        fieldPath: status.podIP\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=bbs/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n
    \   target=\"/var/vcap/all-releases/jobs-src/diego/bbs/templates/bbs.json.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Advertise our spec address.\n    patch --verbose
    \"${target}\" <<'EOT'\n    62c62\n    <     \"#{scheme}://#{name.gsub('_', '-')}-#{spec.index}.#{base}:#{port}\"\n
    \   ---\n    >     \"#{scheme}://#{spec.address}:#{port}\"\n    EOT\n    \n    sha256sum
    \"${target}\" > \"${sentinel}\"\n\n- type: replace\n  path: /instance_groups/name=diego-api/jobs/name=cfdot/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    ##\n    # ATTENTION: This is part of
    a set of six interconnected patches.\n    #            Two files spread over three
    instance groups.\n    # See\n    # - bosh/releases/pre_render_scripts/diego-cell/cfdot/jobs\n
    \   # - bosh/releases/pre_render_scripts/diego-api/cfdot/jobs\n    # - bosh/releases/pre_render_scripts/scheduler/cfdot/jobs\n
    \   \n    set -o errexit -o nounset\n    \n    target=\"/var/vcap/all-releases/jobs-src/diego/cfdot/templates/pre-start.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Place the cfdot related things into a location
    shared by all the jobs.\n    ##\n    # Implementation note: Given the small size
    of the pre-start script a\n    # patch is likely as large, or even larger than
    just the replacement\n    # script, we simply do the latter.\n    cat > \"${target}\"
    <<'EOT'\n    #!/bin/bash -e\n    \n    DEST=/var/vcap/data/cfdot/bin\n    mkdir
    -p \"${DEST}\"\n    \n    cp /var/vcap/jobs/cfdot/bin/setup     \"${DEST}/cfdot.sh\"\n
    \   cp /var/vcap/packages/cfdot/bin/cfdot \"${DEST}/cfdot\"\n    chown root:vcap
    \"${DEST}/cfdot.sh\"\n    EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\"\n\n-
    type: replace\n  path: /instance_groups/name=diego-api/jobs/name=cfdot/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    ##\n    # ATTENTION: This is part of
    a set of six interconnected patches.\n    #            Two files spread over three
    instance groups.\n    # See\n    # - bosh/releases/pre_render_scripts/diego-cell/cfdot/jobs\n
    \   # - bosh/releases/pre_render_scripts/diego-api/cfdot/jobs\n    # - bosh/releases/pre_render_scripts/scheduler/cfdot/jobs\n
    \   \n    set -o errexit -o nounset\n    \n    target=\"/var/vcap/all-releases/jobs-src/diego/cfdot/templates/setup.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Look for cfdot in the new, shared location.\n
    \   sed -i \"s|PATH=/var/vcap/packages|PATH=/var/vcap/data|g\" \"${target}\"\n
    \   \n    sha256sum \"${target}\" > \"${sentinel}\""
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-diego-api
  namespace: default
---
apiVersion: v1
data:
  ops: "\n\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=vxlan-policy-agent/properties/disable?\n
    \ value: true\n\n# Don't inject replica count for the diego cells\n# The instance
    count is not needed for diego cells, and removing it optimizes scaling up/down
    diego cells\n- type: replace\n  path: /instance_groups/name=diego-cell/env?/bosh/agent/settings/injectReplicasEnv?\n
    \ value: false\n# Also remove link instances for vpa, they are not required and
    they make the cells restart when scaling\n- type: replace\n  path: /instance_groups/name=diego-cell/env?/bosh/agent/settings/preRenderOps/instanceGroup?\n
    \ value:\n  - type: replace\n    path: /instance_groups/name=diego-cell/jobs/name=silk-daemon/properties/quarks/consumes/vpa/instances?\n
    \   value:\n    - address: diego-cell-0\n  - type: replace\n    path: /instance_groups/name=diego-cell/jobs/name=silk-cni/properties/quarks/consumes/vpa/instances?\n
    \   value:\n    - address: diego-cell-0\n\n# Enable BPM on garden.\n- type: replace\n
    \ path: /instance_groups/name=diego-cell/jobs/name=garden/properties/bpm?/enabled\n
    \ value: true\n\n# Configure the size of the diego cell grootfs store\n\n# We
    are repurposing reserved_space_for_other_jobs_in_mb as the size of the grootfs
    store\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=garden/properties/grootfs?/reserved_space_for_other_jobs_in_mb\n
    \ value: 40960\n\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=rep/properties/diego?/executor/disk_capacity_mb\n
    \ value: 40960\n\n# The loggr UDP forwarder needs some env vars specific to the
    container they're running in\n# The INDEX env var cannot be rendered properly
    as part of BPM rendering, it can only be set\n# correctly through pod reflection.
    Without this, the index would be 0 for any replica\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=loggr-udp-forwarder/properties?/quarks/envs?\n
    \ value:\n  - name: IP\n    valueFrom:\n      fieldRef:\n        fieldPath: status.podIP\n
    \ - name: INDEX\n    valueFrom:\n      fieldRef:\n        fieldPath: metadata.labels['statefulset.kubernetes.io/pod-name']\n\n#
    Rep data should not be a PVC - since that could end up as a PVC\n# and if it's
    NFS, garden won't work\n- type: replace\n  path: /instance_groups/name=diego-cell/env?/bosh/agent/settings/disks\n
    \ value:\n  - volume:\n      name: rep-data\n      emptyDir: {}\n    volumeMount:\n
    \     name: rep-data\n      mountPath: /var/vcap/data/rep\n    filters:\n      job_name:
    \"rep\"\n      process_name: \"rep\"\n  - volumeMount:\n      name: rep-data\n
    \     mountPath: /var/vcap/data/rep\n    filters:\n      job_name: \"garden\"\n
    \     process_name: \"garden\"\n  # An additional entry will be added for each
    stack-rootfs-setup job\n\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=garden/properties/garden/network_plugin?\n
    \ value: /var/vcap/data/runc-cni/bin/garden-external-networker\n\n- type: replace\n
    \ path: /instance_groups/name=diego-cell/jobs/name=garden-cni/properties/cni_plugin_dir?\n
    \ value: /var/vcap/data/silk-cni/bin\n\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=silk-cni/properties/dns_servers?\n
    \ value:\n  # The bosh-dns.<namespace>.svc gets resolved to the service ClusterIP
    it points to and then\n  # replaced with its value when rendering the silk-cni
    configuration file. This functionality is\n  # provided by a patch on a pre-render
    script.\n  - \"apps-dns.default.svc\"\n\n- type: remove\n  path: /instance_groups/name=diego-cell/jobs/name=bosh-dns-adapter?\n\n#
    Add quarks properties for garden.\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=garden/properties/quarks?\n
    \ value:\n    run:\n      healthcheck:\n        garden:\n          readiness:\n
    \           exec:\n              command: [\"curl\", \"--head\", \"--fail\", \"--silent\",
    \"http://127.0.0.1:17019/debug/vars\"]\n    post_start:\n      condition:\n        exec:\n
    \         command: [sh, -c, 'ss -nlt sport = 17019 | grep \"LISTEN.*:17019\"']\n\n#
    Add quarks properties for route_emitter.\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=route_emitter/properties/quarks?\n
    \ value:\n    run:\n      healthcheck:\n        route_emitter:\n          readiness:
    &route_emitter_readiness\n            exec:\n              command: [\"curl\",
    \"--fail\", \"--silent\", \"http://127.0.0.1:17011/ping\"]\n    post_start:\n
    \     condition: *route_emitter_readiness\n\n# Add quarks properties for rep.\n-
    type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=rep/properties/quarks?\n
    \ value:\n    # Disable the ephemeral rep disk\n    bpm:\n      processes:\n      -
    name: rep\n        ephemeral_disk: false\n    ports:\n    - name: rep-tls\n      protocol:
    TCP\n      internal: 1801\n    run:\n      security_context:\n        privileged:
    true\n      healthcheck:\n        rep:\n          readiness:\n            exec:\n
    \             command:\n              - curl\n              - --head\n              -
    --fail\n              - --insecure\n              - --cert\n              - /var/vcap/jobs/rep/config/certs/tls.crt\n
    \             - --key\n              - /var/vcap/jobs/rep/config/certs/tls.key\n
    \             - https://127.0.0.1:1800/ping\n    post_start:\n      condition:\n
    \       exec:\n          command: [sh, -c, 'ss -nlt sport = 1800 | grep \"LISTEN.*:1800\"']\n\n#
    Set the unconfined AppArmor profile for bpm-pre-start-rep in order to perform
    mounts.\n# This works in combination with CAP_SYS_ADMIN Linux capability.\n- type:
    replace\n  path: /instance_groups/name=diego-cell/env?/bosh/agent/settings/annotations/container.apparmor.security.beta.kubernetes.io~1bpm-pre-start-rep\n
    \ value: unconfined\n\n# Set the unconfined AppArmor profile for rep-rep in order
    to perform mounts.\n# This works in combination with CAP_SYS_ADMIN Linux capability.\n-
    type: replace\n  path: /instance_groups/name=diego-cell/env?/bosh/agent/settings/annotations/container.apparmor.security.beta.kubernetes.io~1rep-rep\n
    \ value: unconfined\n\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=garden/properties/garden/network_pool?\n
    \ value: 10.38.0.0/16\n\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=garden/properties/garden/apparmor_profile?\n
    \ value: \"\"\n\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=garden/properties/garden/cleanup_process_dirs_on_wait\n
    \ value: false\n\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=garden/properties?/garden?/disable_swap_limit?\n
    \ value: true\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=cfdot/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    ##\n    # ATTENTION: This is part of
    a set of six interconnected patches.\n    #            Two files spread over three
    instance groups.\n    # See\n    # - bosh/releases/pre_render_scripts/diego-cell/cfdot/jobs\n
    \   # - bosh/releases/pre_render_scripts/diego-api/cfdot/jobs\n    # - bosh/releases/pre_render_scripts/scheduler/cfdot/jobs\n
    \   \n    set -o errexit -o nounset\n    \n    target=\"/var/vcap/all-releases/jobs-src/diego/cfdot/templates/pre-start.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Place the cfdot related things into a location
    shared by all the jobs.\n    ##\n    # Implementation note: Given the small size
    of the pre-start script a\n    # patch is likely as large, or even larger than
    just the replacement\n    # script, we simply do the latter.\n    cat > \"${target}\"
    <<'EOT'\n    #!/bin/bash -e\n    \n    DEST=/var/vcap/data/cfdot/bin\n    mkdir
    -p \"${DEST}\"\n    \n    cp /var/vcap/jobs/cfdot/bin/setup     \"${DEST}/cfdot.sh\"\n
    \   cp /var/vcap/packages/cfdot/bin/cfdot \"${DEST}/cfdot\"\n    chown root:vcap
    \"${DEST}/cfdot.sh\"\n    EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\"\n\n-
    type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=cfdot/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    ##\n    # ATTENTION: This is part of
    a set of six interconnected patches.\n    #            Two files spread over three
    instance groups.\n    # See\n    # - bosh/releases/pre_render_scripts/diego-cell/cfdot/jobs\n
    \   # - bosh/releases/pre_render_scripts/diego-api/cfdot/jobs\n    # - bosh/releases/pre_render_scripts/scheduler/cfdot/jobs\n
    \   \n    set -o errexit -o nounset\n    \n    target=\"/var/vcap/all-releases/jobs-src/diego/cfdot/templates/setup.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Look for cfdot in the new, shared location.\n
    \   sed -i \"s|PATH=/var/vcap/packages|PATH=/var/vcap/data|g\" \"${target}\"\n
    \   \n    sha256sum \"${target}\" > \"${sentinel}\"\n\n- type: replace\n  path:
    /instance_groups/name=diego-cell/jobs/name=garden-cni/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n
    \   target=\"/var/vcap/all-releases/jobs-src/cf-networking/garden-cni/templates/pre-start.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Place the garden-external-networker into a
    location shared by all the jobs.\n    patch --verbose \"${target}\" <<'EOT'\n
    \   @@ -1,3 +1,8 @@\n     #!/bin/bash -eu\n    \n     rm -rf /var/vcap/data/garden-cni
    || true\n    +\n    +DEST=/var/vcap/data/runc-cni/bin/\n    +\n    +mkdir -p \"${DEST}\"\n
    \   +cp /var/vcap/packages/runc-cni/bin/garden-external-networker \"${DEST}/garden-external-networker\"\n
    \   EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\"\n\n- type: replace\n
    \ path: /instance_groups/name=diego-cell/jobs/name=garden/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n
    \   target=\"/var/vcap/all-releases/jobs-src/garden-runc/garden/templates/bin/bpm-pre-start.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Patch the pre-start script to setup /var/vcap/data\n
    \   patch --verbose \"${target}\" <<'EOT'\n    2a3,4\n    > find /var/vcap/data/grootfs/
    -iname '*' -delete\n    > \n    20a23,25\n    > \n    > # Ensure that runc and
    container processes can stat everything\n    > chmod ugo+rx /var/vcap/data/grootfs\n
    \   EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\"\n\n- type: replace\n
    \ path: /instance_groups/name=diego-cell/jobs/name=garden/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n
    \   target=\"/var/vcap/all-releases/jobs-src/garden-runc/garden/templates/bin/grootfs-utils.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Patch grootfs-utils to restrict the size of
    the sparse stores\n    patch --verbose \"${target}\" <<'EOT'\n    7,8c7,17\n    <
    \  /var/vcap/packages/thresholder/bin/thresholder \"<%= p(\"grootfs.reserved_space_for_other_jobs_in_mb\")
    %>\" \"$DATA_DIR\" \"$GARDEN_CONFIG_DIR/grootfs_config.yml\" \"<%= p(\"garden.graph_cleanup_threshold_in_mb\")
    %>\" \"<%= p(\"grootfs.graph_cleanup_threshold_in_mb\") %>\"\n    <   /var/vcap/packages/thresholder/bin/thresholder
    \"<%= p(\"grootfs.reserved_space_for_other_jobs_in_mb\") %>\" \"$DATA_DIR\" \"$GARDEN_CONFIG_DIR/privileged_grootfs_config.yml\"
    \"<%= p(\"garden.graph_cleanup_threshold_in_mb\") %>\" \"<%= p(\"grootfs.graph_cleanup_threshold_in_mb\")
    %>\"\n    ---\n    >   let grootfs_size=\"<%= p(\"grootfs.reserved_space_for_other_jobs_in_mb\")
    %>\"\n    >   let disk_size=`df -BM /var/vcap/data/grootfs/store/ | tail -n 1
    | awk '{gsub(\"M\", \"\", $2); print $2}'`\n    >   let reserved_disk=\"$disk_size
    - $grootfs_size\"\n    > \n    >   if [ \"$grootfs_size\" -gt \"$disk_size\" ];
    then\n    >     echo \"The node running this cell doesn't have enough disk space.
    You requested ${grootfs_size}M but the disk is ${disk_size}M in size.\"\n    >
    \    exit 1\n    >   fi\n    > \n    >   /var/vcap/packages/thresholder/bin/thresholder
    \"$reserved_disk\" \"$DATA_DIR\" \"$GARDEN_CONFIG_DIR/grootfs_config.yml\" \"<%=
    p(\"garden.graph_cleanup_threshold_in_mb\") %>\" \"<%= p(\"grootfs.graph_cleanup_threshold_in_mb\")
    %>\"\n    >   /var/vcap/packages/thresholder/bin/thresholder \"$reserved_disk\"
    \"$DATA_DIR\" \"$GARDEN_CONFIG_DIR/privileged_grootfs_config.yml\" \"<%= p(\"garden.graph_cleanup_threshold_in_mb\")
    %>\" \"<%= p(\"grootfs.graph_cleanup_threshold_in_mb\") %>\"\n    EOT\n    \n
    \   sha256sum \"${target}\" > \"${sentinel}\"\n\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=garden/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n
    \   target=\"/var/vcap/all-releases/jobs-src/garden-runc/garden/templates/bin/pre-start\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    patch --verbose \"${target}\" <<'EOT'\n    ---
    jobs/garden/templates/bin/pre-start\n    +++ jobs/garden/templates/bin/pre-start\n
    \   @@ -4,7 +4,5 @@ set -e\n    \n     source /var/vcap/jobs/garden/bin/envs\n
    \    source /var/vcap/jobs/garden/bin/grootfs-utils\n    -source /var/vcap/packages/greenskeeper/bin/system-preparation\n
    \   \n    -permit_device_control\n     invoke_thresholder\n    EOT\n    \n    sha256sum
    \"${target}\" > \"${sentinel}\"\n\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=rep/properties?/quarks/pre_render_scripts/bpm/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n
    \   target=\"/var/vcap/all-releases/jobs-src/diego/rep/templates/bpm.yml.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Patch BPM, since we're actually running in-cluster
    without BPM\n    patch --verbose \"${target}\" <<'EOT'\n    @@ -6,7 +6,7 @@\n
    \        open_files: 100000\n       hooks:\n         pre_start: /var/vcap/jobs/rep/bin/bpm-pre-start\n
    \   -  ephemeral_disk: true\n    +  ephemeral_disk: false\n       additional_volumes:\n
    \        - path: <%= p(\"diego.executor.volman.driver_paths\") %>\n         -
    path : /var/vcap/data/garden\n    EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\"\n\n-
    type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=rep/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n
    \   target=\"/var/vcap/all-releases/jobs-src/diego/rep/templates/bpm-pre-start.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Use the ephemeral data directory for the rootfs.\n
    \   patch --verbose \"${target}\" <<'EOT'\n    @@ -5,3 +5,7 @@\n     $bin_dir/set-rep-kernel-params\n
    \   \n     $bin_dir/setup_mounted_data_dirs\n    +\n    +mkdir -p /var/vcap/data/shared-packages/\n
    \   +cp -r /var/vcap/packages/healthcheck /var/vcap/data/shared-packages/\n    +cp
    -r /var/vcap/packages/proxy /var/vcap/data/shared-packages/\n    EOT\n    \n    sha256sum
    \"${target}\" > \"${sentinel}\"\n\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=rep/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n
    \   target=\"/var/vcap/all-releases/jobs-src/diego/rep/templates/rep.json.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Don't share /var/vcap/packages between containers.\n
    \   patch --verbose \"${target}\" <<'EOT'\n    @@ -39,7 +39,7 @@\n         disk_mb:
    p(\"diego.executor.disk_capacity_mb\").to_s,\n         enable_consul_service_registration:
    p(\"enable_consul_service_registration\"),\n         enable_declarative_healthcheck:
    p(\"enable_declarative_healthcheck\"),\n    -    declarative_healthcheck_path:
    \"/var/vcap/packages/healthcheck\",\n    +    declarative_healthcheck_path: \"/var/vcap/data/shared-packages/healthcheck\",\n
    \        enable_container_proxy: p(\"containers.proxy.enabled\"),\n         container_proxy_require_and_verify_client_certs:
    p(\"containers.proxy.require_and_verify_client_certificates\"),\n         container_proxy_trusted_ca_certs:
    p(\"containers.proxy.trusted_ca_certificates\"),\n    @@ -47,7 +47,7 @@\n         container_proxy_ads_addresses:
    p(\"containers.proxy.ads_addresses\"),\n         enable_unproxied_port_mappings:
    p(\"containers.proxy.enable_unproxied_port_mappings\"),\n         proxy_memory_allocation_mb:
    p(\"containers.proxy.additional_memory_allocation_mb\"),\n    -    container_proxy_path:
    \"/var/vcap/packages/proxy\",\n    +    container_proxy_path: \"/var/vcap/data/shared-packages/proxy\",\n
    \        container_proxy_config_path: \"/var/vcap/data/rep/shared/garden/proxy_config\",\n
    \        evacuation_polling_interval: \"#{p(\"diego.rep.evacuation_polling_interval_in_seconds\")}s\",\n
    \        evacuation_timeout: \"#{p(\"diego.rep.evacuation_timeout_in_seconds\")}s\",\n
    \   EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\"\n\n- type: replace\n
    \ path: /instance_groups/name=diego-cell/jobs/name=rep/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n
    \   target=\"/var/vcap/all-releases/jobs-src/diego/rep/templates/set-rep-kernel-params.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Disable ubuntu-specific uncontainerized connection
    limit.\n    patch --verbose \"${target}\" <<'EOT'\n    @@ -18,8 +18,8 @@\n     \n
    \    # NF_CONNTRACK_MAX\n     # Default value is 65536. We set it to a larger
    number to avoid running out of connections.\n    -modprobe nf_conntrack\n    -echo
    262144 > /proc/sys/net/netfilter/nf_conntrack_max\n    +#modprobe nf_conntrack\n
    \   +#echo 262144 > /proc/sys/net/netfilter/nf_conntrack_max\n     \n     echo
    2147483647 > /proc/sys/fs/inotify/max_user_watches\n     echo 2147483647 > /proc/sys/fs/inotify/max_user_instances\n
    \   EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\"\n\n- type: replace\n
    \ path: /instance_groups/name=diego-cell/jobs/name=silk-cni/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset -o pipefail\n
    \   \n    target=\"/var/vcap/all-releases/jobs-src/silk/silk-cni/templates/cni-wrapper-plugin.conflist.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Resolve DNS Servers passed via the `dns_servers`
    property if any is a hostname\n    # instead of an IP address.\n    patch --verbose
    \"${target}\" <<'EOT'\n    @@ -2,6 +2,7 @@\n     <%=\n       require 'ipaddr'\n
    \      require 'json'\n    +  require 'resolv'\n     \n       def compute_mtu\n
    \        vxlan_overhead = 50\n    @@ -41,6 +42,14 @@\n         end\n       end\n
    \    \n    +  dns_servers = p('dns_servers').map do |dns_server|\n    +    if
    !(dns_server =~ Regexp.union([Resolv::IPv4::Regex, Resolv::IPv6::Regex]))\n    +
    \     Resolv.getaddress dns_server\n    +    else\n    +      dns_server\n    +
    \   end\n    +  end\n    +\n       toRender = {\n         'name' => 'cni-wrapper',\n
    \        'cniVersion' => '0.3.1',\n    @@ -61,7 +70,7 @@\n           'ingress_tag'
    => 'ffff0000',\n           'vtep_name' => 'silk-vtep',\n           'policy_agent_force_poll_address'
    => '127.0.0.1:' + link('vpa').p('force_policy_poll_cycle_port').to_s,\n    -      'dns_servers'
    => p('dns_servers'),\n    +      'dns_servers' => dns_servers,\n           'host_tcp_services'
    => p('host_tcp_services'),\n           'host_udp_services' => p('host_udp_services'),\n
    \          'deny_networks' => {\n    EOT\n    \n    sha256sum \"${target}\" >
    \"${sentinel}\"\n\n- type: replace\n  path: /instance_groups/name=diego-cell/jobs/name=silk-cni/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n
    \   target=\"/var/vcap/all-releases/jobs-src/silk/silk-cni/templates/pre-start.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Place the silk-cni related things into a location
    shared by all the jobs.\n    patch --verbose \"${target}\" <<'EOT'\n    @@ -4,3
    +4,8 @@\n     /var/vcap/packages/silk-cni/bin/cni-teardown \\\n       --config
    /var/vcap/jobs/silk-cni/config/teardown-config.json\n     <% end %>\n    +\n    +DEST=/var/vcap/data/silk-cni/bin\n
    \   +\n    +mkdir -p \"${DEST}\"\n    +cp /var/vcap/packages/silk-cni/bin/* \"${DEST}/\"\n
    \   EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\""
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-diego-cell
  namespace: default
---
apiVersion: v1
data:
  ops: |-
    # Add quarks properties for doppler.
    - type: replace
      path: /instance_groups/name=doppler/jobs/name=doppler/properties/quarks?
      value:
        ports:
        - name: doppler-grpc
          protocol: TCP
          internal: 8082
        run:
          healthcheck:
            doppler:
              readiness:
                exec:
                  command: [sh, -c, 'ss -nlt sport = 8082 | grep "LISTEN.*:8082"']
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-doppler
  namespace: default
---
apiVersion: v1
data:
  ops: ""
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-eirini-helm
  namespace: default
---
apiVersion: v1
data:
  ops: |-
    # Add quarks properties for loggregator_trafficcontroller.
    - type: replace
      path: /instance_groups/name=log-api/jobs/name=loggregator_trafficcontroller/properties/quarks?
      value:
        envs:
        - name: TRAFFIC_CONTROLLER_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        run:
          healthcheck:
            loggregator_trafficcontroller:
              # The traffic controller doesn't expose anything to indicate its healthiness.
              readiness: ~

    # Add quarks properties for reverse_log_proxy.
    - type: replace
      path: /instance_groups/name=log-api/jobs/name=reverse_log_proxy/properties/reverse_log_proxy?/pprof/port
      value: "33047"
    - type: replace
      path: /instance_groups/name=log-api/jobs/name=reverse_log_proxy/properties/quarks?
      value:
        ports:
        - name: grpc-egress
          protocol: TCP
          internal: 8082
        run:
          healthcheck:
            reverse_log_proxy:
              readiness:
                exec:
                  command:
                  - curl
                  - --fail
                  - --head
                  - --silent
                  - http://localhost:33047/debug/pprof/cmdline

    - type: replace
      path: /instance_groups/name=log-api/jobs/name=reverse_log_proxy_gateway/properties/quarks?/bpm/processes/name=reverse_log_proxy_gateway/env/PPROF_PORT?
      value: "33045"
    - type: replace
      path: /instance_groups/name=log-api/jobs/name=reverse_log_proxy_gateway/properties/quarks?/run/healthcheck/reverse_log_proxy_gateway/readiness/exec/command
      value: ["curl", "--fail", "--head", "--silent", "http://localhost:33045/debug/pprof/cmdline"]

    - type: replace
      path: /instance_groups/name=log-api/jobs/name=route_registrar/properties/quarks?/run/healthcheck/route_registrar
      value:
        # The route registrar doesn't expose anything to indicate if the
        # routes are healthy.
        readiness: ~
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-log-api
  namespace: default
---
apiVersion: v1
data:
  ops: |-
    # Set instance count to 1; the default inherited from doppler is 4
    - type: replace
      path: /instance_groups/name=log-cache/instances
      value: 1

    # Change the log_cache_ca CN to avoid clashing with the other log-cache certificate CNs.
    - type: replace
      path: /variables/name=log_cache_ca/options/common_name
      value: log-cache-ca

    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache/properties/health_addr?
      value: "::6060"

    # Add quarks properties for log-cache.
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache/properties/quarks?
      value:
        ports:
        - name: log-cache-metrics
          protocol: TCP
          internal: 6060
        - name: log-cache
          protocol: TCP
          internal: 8080
        run:
          healthcheck:
            log-cache:
              readiness:
                exec:
                  command:
                  - curl
                  - --insecure
                  - --fail
                  - --head
                  - --silent
                  - --cert
                  - /var/vcap/jobs/log-cache/config/certs/metrics.crt
                  - --key
                  - /var/vcap/jobs/log-cache/config/certs/metrics.key
                  - https://localhost:6060/metrics

    # Add quarks properties for log-cache-gateway.
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache-gateway/properties/quarks?
      value:
        ports:
        - name: log-cache-gateway-metrics
          protocol: TCP
          internal: 6063
        run:
          healthcheck:
            log-cache-gateway:
              readiness:
                # Unfortunately, by default the health port listens on localhost only
                # and isn't easily configurable.
                exec:
                  command:
                  - curl
                  - --insecure
                  - --fail
                  - --head
                  - --silent
                  - --cert
                  - /var/vcap/jobs/log-cache/config/certs/metrics.crt
                  - --key
                  - /var/vcap/jobs/log-cache/config/certs/metrics.key
                  - https://localhost:6063/metrics

    # Add quarks properties for log-cache-nozzle.
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache-nozzle/properties/quarks?
      value:
        run:
          healthcheck:
            log-cache-nozzle:
              readiness:
                # Unfortunately, by default the health port listens on localhost only
                # and isn't easily configurable.
                exec:
                  command: [curl, --fail, --head, --silent, http://localhost:6061/debug/pprof/cmdline]

    # Add quarks properties for log-cache-cf-auth-proxy.
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache-cf-auth-proxy/properties/quarks?
      value:
        ports:
        - name: log-cache-cf-auth-proxy-metrics
          protocol: TCP
          internal: 6065
        - name: log-cache-cf-auth-proxy
          protocol: TCP
          internal: 8083
        run:
          healthcheck:
            log-cache-cf-auth-proxy:
              readiness:
                # Unfortunately, by default the health port listens on localhost only
                # and isn't easily configurable.
                exec:
                  command:
                  - curl
                  - --insecure
                  - --fail
                  - --head
                  - --silent
                  - --cert
                  - /var/vcap/jobs/log-cache/config/certs/metrics.crt
                  - --key
                  - /var/vcap/jobs/log-cache/config/certs/metrics.key
                  - https://localhost:6065/metrics

    # Add quarks properties for route_registrar.
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=route_registrar/properties/quarks?
      value:
        run:
          healthcheck:
            route_registrar:
              # The route registrar doesn't expose anything to indicate if the
              # routes are healthy.
              readiness: ~

    # Add log-cache.service.cf.internal DNS alias to be able to point CC to it.
    - type: replace
      path: /addons/name=bosh-dns-aliases/jobs/name=bosh-dns-aliases/properties/aliases/-
      value:
        domain: log-cache.service.cf.internal
        targets:
        - deployment: cf
          domain: bosh
          instance_group: log-cache
          network: default
          query: '*'

    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/cc/logcache/host
      value: log-cache.service.cf.internal
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-log-cache
  namespace: default
---
apiVersion: v1
data:
  ops: |-
    # Add quarks properties.
    - type: replace
      path: /instance_groups/name=nats/jobs/name=nats/properties/quarks?
      value:
        ports:
        - name: nats
          protocol: TCP
          internal: 4222
        - name: nats-routes
          protocol: TCP
          internal: 4223
        - name: nats-tls
          protocol: TCP
          internal: 4224
        - name: nats-tls-routes
          protocol: TCP
          internal: 4225
        run:
          healthcheck:
            nats:
              readiness:
                exec:
                  command:
                  - sh
                  - -c
                  - ss -nlt sport = 4222 | grep "LISTEN.*:4222" && ss -nlt sport = 4223 | grep "LISTEN.*:4223"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-nats
  namespace: default
---
apiVersion: v1
data:
  ops: |-
    - path: /instance_groups/name=rotate-cc-database-key/env?/bosh/agent/settings/disable_log_sidecar
      type: replace
      value: true
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-rotate-cc-database-key
  namespace: default
---
apiVersion: v1
data:
  ops: |-
    # Add quarks properties for the gorouter job.
    - type: replace
      path: /instance_groups/name=router/jobs/name=gorouter/properties/quarks?
      value:
        ports:
        - name: router
          protocol: TCP
          internal: 80
        - name: router-ssl
          protocol: TCP
          internal: 443
        run:
          healthcheck:
            gorouter:
              readiness:
                httpGet:
                  port: 8080
                  path: /health
        post_start:
          condition:
            exec:
              command: ["curl", "--fail", "--head", "http://127.0.0.1:8080/health"]

    # Disable tuning /proc/sys kernel parameters as things are running on a container.
    - type: replace
      path: /instance_groups/name=router/jobs/name=gorouter/properties/router?/set_kernel_parameters
      value: false

    - type: replace
      path: /instance_groups/name=router/jobs/name=loggr-udp-forwarder/properties/quarks?
      value:
        run:
          healthcheck:
            loggr-udp-forwarder:
              readiness:
                exec:
                  command: ["sh", "-c", "ss -nlu sport = 3457 | grep :3457"]
        envs:
        - name: INDEX
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['statefulset.kubernetes.io/pod-name']
        - name: IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP

    # Add necessary labels to the router instance group so that the service can select it to create the
    # endpoint.
    - type: replace
      path: /instance_groups/name=router/env?/bosh/agent/settings/labels/app.kubernetes.io~1component
      value: "router"
    - type: replace
      path: /instance_groups/name=router/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=router/env?/bosh/agent/settings/labels/app.kubernetes.io~1version
      value: "2.7.1"

    # Update cipher suites based on https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations
    - type: replace
      path: /instance_groups/name=router/jobs/name=gorouter/properties/router/cipher_suites?
      value: "\
        ECDHE-ECDSA-CHACHA20-POLY1305:\
        ECDHE-RSA-CHACHA20-POLY1305:\
        ECDHE-ECDSA-AES128-GCM-SHA256:\
        ECDHE-RSA-AES128-GCM-SHA256:\
        ECDHE-ECDSA-AES256-GCM-SHA384:\
        ECDHE-RSA-AES256-GCM-SHA384:\
        AES128-GCM-SHA256:\
        AES256-GCM-SHA384"

    # Trust the diego_instance_identity_ca certificate - trusting its CA
    # is insufficient with the cf-operator
    # Also add the CredHub CA
    - type: replace
      path: /instance_groups/name=router/jobs/name=gorouter/properties/router/ca_certs
      value: |
        ((diego_instance_identity_ca.ca))
        ((cc_tls.ca))
        ((uaa_ssl.ca))
        ((network_policy_server_external.ca))

    # If the router certificate is provided via Helm, don't generate the router_ca and router_ssl
    # certificates. router_ssl becomes an implicit variable.
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-router
  namespace: default
---
apiVersion: v1
data:
  ops: |2-

    # Disable routing-api.
    - type: remove
      path: /instance_groups/name=routing-api/jobs/name=routing-api

    - type: remove
      path: /instance_groups/name=database?/jobs/name=pxc-mysql/properties/seeded_databases/name=routing-api

    - type: replace
      path: /instance_groups/name=router/jobs/name=gorouter/properties/routing_api/enabled
      value: false

    - type: remove
      path: /addons/name=bosh-dns-aliases/jobs/name=bosh-dns-aliases/properties/aliases/domain=routing-api.service.cf.internal

    - type: remove
      path: /variables/name=routing_api_tls

    - type: remove
      path: /variables/name=routing_api_tls_client
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-routing-api
  namespace: default
---
apiVersion: v1
data:
  ops: "- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties/quarks?/run/healthcheck/cloud_controller_clock\n
    \ value:\n    readiness:\n      # There is no good readiness check for the scheduled
    tasks\n      exec:\n        command: [\"pgrep\", \"--full\", \"clock:start\"]\n\n-
    type: replace\n  path: /instance_groups/name=scheduler/jobs/name=statsd_injector/properties/quarks?/run/healthcheck/statsd_injector/readiness/exec/command\n
    \ value: [\"/bin/sh\", \"-c\", \"ss -nlu src localhost:8125 | grep :8125\"]\n\n-
    type: replace\n  path: /variables/name=cf_app_sd_ca/options/alternative_names?/-\n
    \ value: \"service-discovery-controller.default.svc\"\n- type: replace\n  path:
    /variables/name=cf_app_sd_client_tls/options/alternative_names?/-\n  value: \"service-discovery-controller.default.svc\"\n-
    type: replace\n  path: /variables/name=cf_app_sd_server_tls/options/alternative_names?/-\n
    \ value: \"service-discovery-controller.default.svc\"\n\n- type: replace\n  path:
    /instance_groups/name=scheduler/jobs/name=tps/properties/quarks?/run/healthcheck/watcher/readiness/exec/command\n
    \ value: [\"curl\", \"--fail\", \"--silent\", \"http://127.0.0.1:17015/debug/pprof/cmdline\"]\n\n#
    Add quarks properties for the ssh_proxy job.\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=ssh_proxy/properties/diego/ssh_proxy/disable_healthcheck_server\n
    \ value: false\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=ssh_proxy/properties/quarks?\n
    \ value:\n    ports:\n    - name: ssh-proxy\n      protocol: TCP\n      internal:
    2222\n    run:\n      healthcheck:\n        ssh_proxy:\n          readiness:\n
    \           httpGet:\n              port: 2223\n\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=service-discovery-controller/properties/quarks?/run/healthcheck/service-discovery-controller\n
    \ value:\n    readiness:\n      # Proper connection requires a TLS client cert;
    that's not worth it right now.\n      tcpSocket:\n        port: 8054\n\n# Add
    necessary labels to the scheduler instance group so that the service can select
    it to create\n# the endpoint.\n- type: replace\n  path: /instance_groups/name=scheduler/env?/bosh/agent/settings/labels/app.kubernetes.io~1component\n
    \ value: \"ssh-proxy\"\n- type: replace\n  path: /instance_groups/name=scheduler/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance\n
    \ value: \"kubecf\"\n- type: replace\n  path: /instance_groups/name=scheduler/env?/bosh/agent/settings/labels/app.kubernetes.io~1version\n
    \ value: \"2.7.1\"\n\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=loggr-udp-forwarder/properties?/quarks/envs\n
    \ value:\n  - name: INDEX\n    valueFrom:\n      fieldRef:\n        fieldPath:
    metadata.labels['statefulset.kubernetes.io/pod-name']\n  - name: IP\n    valueFrom:\n
    \     fieldRef:\n        fieldPath: status.podIP\n\n- type: replace\n  path: /instance_groups/name=scheduler/jobs/name=loggr-syslog-binding-cache/properties?/quarks\n
    \ value:\n    ports:\n    - name: binding-cache\n      protocol: TCP\n      internal:
    9000\n\n- type: remove\n  path: /instance_groups/name=scheduler/jobs/name=cc_deployment_updater\n-
    type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cfdot/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    ##\n    # ATTENTION: This is part of
    a set of six interconnected patches.\n    #            Two files spread over three
    instance groups.\n    # See\n    # - bosh/releases/pre_render_scripts/diego-cell/cfdot/jobs\n
    \   # - bosh/releases/pre_render_scripts/diego-api/cfdot/jobs\n    # - bosh/releases/pre_render_scripts/scheduler/cfdot/jobs\n
    \   \n    set -o errexit -o nounset\n    \n    target=\"/var/vcap/all-releases/jobs-src/diego/cfdot/templates/pre-start.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Place the cfdot related things into a location
    shared by all the jobs.\n    ##\n    # Implementation note: Given the small size
    of the pre-start script a\n    # patch is likely as large, or even larger than
    just the replacement\n    # script, we simply do the latter.\n    cat > \"${target}\"
    <<'EOT'\n    #!/bin/bash -e\n    \n    DEST=/var/vcap/data/cfdot/bin\n    mkdir
    -p \"${DEST}\"\n    \n    cp /var/vcap/jobs/cfdot/bin/setup     \"${DEST}/cfdot.sh\"\n
    \   cp /var/vcap/packages/cfdot/bin/cfdot \"${DEST}/cfdot\"\n    chown root:vcap
    \"${DEST}/cfdot.sh\"\n    EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\"\n\n-
    type: replace\n  path: /instance_groups/name=scheduler/jobs/name=cfdot/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    ##\n    # ATTENTION: This is part of
    a set of six interconnected patches.\n    #            Two files spread over three
    instance groups.\n    # See\n    # - bosh/releases/pre_render_scripts/diego-cell/cfdot/jobs\n
    \   # - bosh/releases/pre_render_scripts/diego-api/cfdot/jobs\n    # - bosh/releases/pre_render_scripts/scheduler/cfdot/jobs\n
    \   \n    set -o errexit -o nounset\n    \n    target=\"/var/vcap/all-releases/jobs-src/diego/cfdot/templates/setup.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Look for cfdot in the new, shared location.\n
    \   sed -i \"s|PATH=/var/vcap/packages|PATH=/var/vcap/data|g\" \"${target}\"\n
    \   \n    sha256sum \"${target}\" > \"${sentinel}\"\n\n- type: replace\n  path:
    /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n
    \   target=\"/var/vcap/all-releases/jobs-src/capi/cloud_controller_clock/templates/bin/cloud_controller_clock.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    patch --verbose \"${target}\" <<'EOT'\n    @@
    -2,4 +2,6 @@\n    \n     source /var/vcap/jobs/cloud_controller_clock/bin/ruby_version.sh\n
    \    cd /var/vcap/packages/cloud_controller_ng/cloud_controller_ng\n    +patch
    -p0 < /var/vcap/all-releases/jobs-src/capi/cloud_controller_ng/yaml-anchor.patch\n
    \   +\n     exec bundle exec rake clock:start\n    EOT\n    \n    sha256sum \"${target}\"
    > \"${sentinel}\"\n    \n    cat <<'EOT' > /var/vcap/all-releases/jobs-src/capi/cloud_controller_ng/yaml-anchor.patch\n
    \   diff --git app/controllers/v3/app_manifests_controller.rb app/controllers/v3/app_manifests_controller.rb\n
    \   index ed3ca78a6..9828cd7ed 100644\n    --- app/controllers/v3/app_manifests_controller.rb\n
    \   +++ app/controllers/v3/app_manifests_controller.rb\n    @@ -70,7 +70,7 @@
    class AppManifestsController < ApplicationController\n       def validate_content_type!\n
    \        if !request_content_type_is_yaml?\n           logger.error(\"Context-type
    isn't yaml: #{request.content_type}\")\n    -      invalid_request!('Content-Type
    must be yaml')\n    +      bad_request!('Content-Type must be yaml')\n         end\n
    \      end\n    \n    @@ -79,10 +79,10 @@ class AppManifestsController < ApplicationController\n
    \      end\n    \n       def parsed_app_manifest_params\n    -    parsed_application
    = params[:body]['applications'] && params[:body]['applications'].first\n    +
    \   parsed_application = parsed_yaml['applications'] && parsed_yaml['applications'].first\n
    \   \n    -    raise invalid_request!('Invalid app manifest') unless parsed_application.present?\n
    \   +    raise bad_request!('Invalid app manifest') unless parsed_application.present?\n
    \   \n    -    parsed_application.to_unsafe_h\n    +    parsed_application\n       end\n
    \    end\n    diff --git app/controllers/v3/application_controller.rb app/controllers/v3/application_controller.rb\n
    \   index 8221b866d..1a7e57757 100644\n    --- app/controllers/v3/application_controller.rb\n
    \   +++ app/controllers/v3/application_controller.rb\n    @@ -30,6 +30,10 @@ module
    V3ErrorsHelper\n         raise CloudController::Errors::ApiError.new_from_details('BadRequest',
    message)\n       end\n    \n    +  def message_parse_error!(message)\n    +    raise
    CloudController::Errors::ApiError.new_from_details('MessageParseError', message)\n
    \   +  end\n    +\n       def service_unavailable!(message)\n         raise CloudController::Errors::ApiError.new_from_details('ServiceUnavailable',
    message)\n       end\n    @@ -80,6 +84,17 @@ class ApplicationController < ActionController::Base\n
    \        JSON.parse(request.body.string)\n       end\n    \n    +  def parsed_yaml\n
    \   +    return @parsed_yaml if @parsed_yaml\n    +\n    +    allow_yaml_aliases
    = false\n    +    yaml = YAML.safe_load(request.body.string, [], [], allow_yaml_aliases)\n
    \   +    message_parse_error!('invalid request body') if !yaml.is_a? Hash\n    +
    \   @parsed_yaml = yaml\n    +  rescue Psych::BadAlias\n    +    bad_request!('Manifest
    does not support Anchors and Aliases')\n    +  end\n    +\n       def roles\n
    \        VCAP::CloudController::SecurityContext.roles\n       end\n    diff --git
    app/controllers/v3/space_manifests_controller.rb app/controllers/v3/space_manifests_controller.rb\n
    \   index 3f9f0db48..b213925bb 100644\n    --- app/controllers/v3/space_manifests_controller.rb\n
    \   +++ app/controllers/v3/space_manifests_controller.rb\n    @@ -15,7 +15,7 @@
    class SpaceManifestsController < ApplicationController\n         space_not_found!
    unless space && permission_queryer.can_read_from_space?(space.guid, space.organization.guid)\n
    \        unauthorized! unless permission_queryer.can_write_to_space?(space.guid)\n
    \   \n    -    messages = parsed_app_manifests.map(&:to_unsafe_h).map { |app_manifest|
    NamedAppManifestMessage.create_from_yml(app_manifest) }\n    +    messages = parsed_app_manifests.map
    { |app_manifest| NamedAppManifestMessage.create_from_yml(app_manifest) }\n         errors
    = messages.each_with_index.flat_map { |message, i| errors_for_message(message,
    i) }\n         compound_error!(errors) unless errors.empty?\n    \n    @@ -49,6
    +49,10 @@ class SpaceManifestsController < ApplicationController\n    \n         parsed_manifests
    = parsed_app_manifests.map(&:to_hash)\n    \n    +    messages = parsed_app_manifests.map
    { |app_manifest| NamedAppManifestMessage.create_from_yml(app_manifest) }\n    +
    \   errors = messages.each_with_index.flat_map { |message, i| errors_for_message(message,
    i) }\n    +    compound_error!(errors) unless errors.empty?\n    +\n         diff
    = SpaceDiffManifest.generate_diff(parsed_manifests, space)\n    \n         render
    status: :created, json: { diff: diff }\n    @@ -79,7 +83,7 @@ class SpaceManifestsController
    < ApplicationController\n       def validate_content_type!\n         if !request_content_type_is_yaml?\n
    \          logger.error(\"Content-type isn't yaml: #{request.content_type}\")\n
    \   -      invalid_request!('Content-Type must be yaml')\n    +      bad_request!('Content-Type
    must be yaml')\n         end\n       end\n    \n    @@ -88,13 +92,13 @@ class
    SpaceManifestsController < ApplicationController\n       end\n    \n       def
    check_version_is_supported!\n    -    version = params[:body]['version']\n    +
    \   version = parsed_yaml['version']\n         raise unprocessable!('Unsupported
    manifest schema version. Currently supported versions: [1].') unless !version
    || version == 1\n       end\n    \n       def parsed_app_manifests\n         check_version_is_supported!\n
    \   -    parsed_applications = params[:body].permit!['applications']\n    +    parsed_applications
    = parsed_yaml['applications']\n         raise unprocessable!(\"Cannot parse manifest
    with no 'applications' field.\") unless parsed_applications.present?\n    \n         parsed_applications\n
    \   diff --git config/application.rb config/application.rb\n    index a86039f69..d75394bf7
    100644\n    --- config/application.rb\n    +++ config/application.rb\n    @@ -2,18
    +2,6 @@ require 'action_controller/railtie'\n    \n     class Application < ::Rails::Application\n
    \      config.exceptions_app = self.routes\n    -\n    -  # For Rails 5 / Rack
    2 - this is how to add a new parser\n    -  original_parsers = ActionDispatch::Request.parameter_parsers\n
    \   -\n    -  allow_yaml_aliases = true\n    -  yaml_parser = lambda { |body|
    YAML.safe_load(body, [], [], allow_yaml_aliases).with_indifferent_access }\n    -
    \ new_parsers = original_parsers.merge({\n    -    Mime::Type.lookup('application/x-yaml')
    => yaml_parser,\n    -    Mime::Type.lookup('text/yaml') => yaml_parser,\n    -
    \ })\n    -  ActionDispatch::Request.parameter_parsers = new_parsers\n    -\n
    \      config.middleware.delete ActionDispatch::Session::CookieStore\n       config.middleware.delete
    ActionDispatch::Cookies\n       config.middleware.delete ActionDispatch::Flash\n
    \   EOT"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-scheduler
  namespace: default
---
apiVersion: v1
data:
  ops: "\n- type: replace\n  path: /instance_groups/name=singleton-blobstore/persistent_disk?\n
    \ value: 102400 # 100GB\n- type: remove\n  path: /instance_groups/name=singleton-blobstore/persistent_disk_type\n\n-
    type: replace\n  path: /instance_groups/name=singleton-blobstore/jobs/name=blobstore/properties/blobstore/internal_access_rules?\n
    \ value: [ \"allow 10.0.0.0/8;\",\"allow 172.16.0.0/12;\", \"allow 192.168.0.0/16;\",
    \"allow 100.64.0.0/10;\"]\n- type: replace\n  path: /instance_groups/name=singleton-blobstore/jobs/name=blobstore/properties/quarks?\n
    \ value:\n    ports:\n    - name: http\n      protocol: TCP\n      internal: 8080\n
    \   - name: https\n      protocol: TCP\n      internal: 4443\n    run:\n      security_context:\n
    \       runAsUser: 1000 # vcap\n      healthcheck:\n        nginx:\n          readiness:\n
    \           tcpSocket:\n              port: 8080\n          liveness:\n            exec:\n
    \             command: [pgrep, --full, 'nginx: master process']\n        url_signer:\n
    \         readiness:\n            exec:\n              command: [test, -S, /var/vcap/data/blobstore/signer.sock]\n\n-
    type: replace\n  path: /instance_groups/name=singleton-blobstore/jobs/name=route_registrar/properties/quarks?/run/healthcheck/route_registrar\n
    \ value:\n    readiness: ~\n      # The route registrar doesn't expose anything
    to indicate if the\n      # routes are healthy.\n- type: replace\n  path: /instance_groups/name=singleton-blobstore/jobs/name=blobstore/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n
    \   # Remove /var/vcap/packages from chowing.\n    \n    target=\"/var/vcap/all-releases/jobs-src/capi/blobstore/templates/pre-start.sh.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    patch --verbose \"${target}\" <<'EOT'\n    @@
    -9,7 +9,6 @@\n       local data_dir=/var/vcap/data/blobstore\n       local store_tmp_dir=$store_dir/tmp/uploads\n
    \      local data_tmp_dir=$data_dir/tmp/uploads\n    -  local nginx_webdav_dir=/var/vcap/packages/nginx_webdav\n
    \   \n       mkdir -p $run_dir\n       mkdir -p $log_dir\n    @@ -19,7 +18,7 @@\n
    \      mkdir -p $data_tmp_dir\n    \n       chown vcap:vcap $store_dir\n    -
    \ local dirs=\"$run_dir $log_dir $store_tmp_dir $data_dir $data_tmp_dir $nginx_webdav_dir
    ${nginx_webdav_dir}/..\"\n    +  local dirs=\"$run_dir $log_dir $store_tmp_dir
    $data_dir $data_tmp_dir\"\n       local num_needing_chown=$(find $dirs -not -user
    vcap -or -not -group vcap | wc -l)\n    \n       if [ $num_needing_chown -gt 0
    ]; then\n    EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\""
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-singleton-blobstore
  namespace: default
---
apiVersion: v1
data:
  ops: |2-


    - type: remove
      path: /instance_groups/name=smoke-tests
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-smoke-tests
  namespace: default
---
apiVersion: v1
data:
  ops: |-
    # SITS only makes sense when using Diego, for this reason, we only enable it if Eirini is not
    # enabled.
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-sync-integration-tests
  namespace: default
---
apiVersion: v1
data:
  ops: |2-

    # Disable the tcp-router ig, as there will be no routing-api SSE events to read from.
    - type: remove
      path: /instance_groups/name=tcp-router

    - type: remove
      path: /variables/name=uaa_clients_tcp_router_secret

    - type: remove
      path: /instance_groups/name=uaa/jobs/name=uaa/properties/uaa/clients/tcp_router
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-tcp-router
  namespace: default
---
apiVersion: v1
data:
  ops: "# Add quarks properties.\n- type: replace\n  path: /instance_groups/name=uaa/jobs/name=uaa/properties/quarks?\n
    \ value:\n    ports:\n    - name: http\n      protocol: TCP\n      internal: 8080\n
    \   - name: https\n      protocol: TCP\n      internal: &uaa_https_port 8443\n
    \   run:\n      healthcheck:\n        uaa:\n          # UAA has a long period
    of cert import, so we can't set up a liveness\n          # check without killing
    it accidentally.\n          readiness: &uaa_readiness\n            exec:\n              command:
    ['sh', '-c', '/var/vcap/jobs/uaa/bin/health_check']\n    post_start:\n      condition:
    *uaa_readiness\n\n- type: replace\n  path: /instance_groups/name=uaa/jobs/name=route_registrar/properties/quarks?/run/healthcheck/route_registrar\n
    \ value:\n    readiness: ~\n      # The route registrar doesn't expose anything
    to indicate if the\n      # routes are healthy.\n\n- type: replace\n  path: /instance_groups/name=uaa/jobs/name=statsd_injector/properties/quarks?/run/healthcheck/statsd_injector/readiness/exec/command\n
    \ value: [\"/bin/sh\", \"-c\", \"ss -nlu src localhost:8125 | grep :8125\"]\n-
    type: replace\n  path: /instance_groups/name=uaa/jobs/name=uaa/properties?/quarks/pre_render_scripts/jobs/-\n
    \ value: |\n    #!/usr/bin/env bash\n    \n    set -o errexit -o nounset\n    \n
    \   target=\"/var/vcap/all-releases/jobs-src/uaa/uaa/templates/bin/pre-start.erb\"\n
    \   sentinel=\"${target}.patch_sentinel\"\n    if [[ -f \"${sentinel}\" ]]; then\n
    \     if sha256sum --check \"${sentinel}\" ; then\n        echo \"Patch already
    applied. Skipping\"\n        exit 0\n      fi\n      echo \"Sentinel mismatch,
    re-patching\"\n    fi\n    \n    # Patch bin/pre-start.erb for the certificates
    to work with SUSE.\n    patch --verbose \"${target}\" <<'EOT'\n    --- pre-start.erb
    \ 2019-12-04 08:37:51.046503943 +0100\n    +++ - 2019-12-04 08:41:36.055142488
    +0100\n    @@ -32,9 +32,29 @@\n         <% end %>\n    \n         log \"Trying
    to run update-ca-certificates...\"\n    -    # --certbundle is an undocumented
    flag in the update-ca-certificates script\n    -    # https://salsa.debian.org/debian/ca-certificates/blob/master/sbin/update-ca-certificates#L53\n
    \   -    timeout --signal=KILL 180s /usr/sbin/update-ca-certificates -f -v --certbundle
    \"$(basename \"${OS_CERTS_FILE}\")\"\n    +    source /etc/os-release\n    +    case
    \"${ID}\" in\n    +      *ubuntu*)\n    +        # --certbundle is an undocumented
    flag in the update-ca-certificates script\n    +        # https://salsa.debian.org/debian/ca-certificates/blob/master/sbin/update-ca-certificates#L53\n
    \   +        timeout --signal=KILL 180s /usr/sbin/update-ca-certificates -f -v
    --certbundle \"$(basename \"${OS_CERTS_FILE}\")\"\n    +      ;;\n    +\n    +
    \     *suse|sles*)\n    +        timeout --signal=KILL 180s /usr/sbin/update-ca-certificates
    -f -v\n    +        mv /var/lib/ca-certificates/ca-bundle.pem /etc/ssl/certs/\"$(basename
    \"${OS_CERTS_FILE}\")\"\n    +      ;;\n    +\n    +      *rhel|centos|fedora*)\n
    \   +        timeout --signal=KILL 180s /usr/bin/update-ca-trust\n    +        cp
    /etc/ssl/certs/ca-bundle.crt ${OS_CERTS_FILE}\n    +      ;;\n    +\n    +      *)\n
    \   +        echo \"Unsupported operating system: ${PRETTY_NAME}\"\n    +        exit
    42\n    +      ;;\n    +    esac\n     }\n    \n     function new_cache_files_are_identical
    {\n    EOT\n    \n    sha256sum \"${target}\" > \"${sentinel}\""
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-uaa
  namespace: default
---
apiVersion: v1
data:
  ops: |2-

    - type: replace
      path: /instance_groups/name=api/jobs/name=binary-buildpack?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=cc_uploader/properties?/quarks/bpm/processes/name=cc_uploader/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=cc_uploader/properties?/quarks/bpm/processes/name=cc_uploader/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/quarks/bpm/processes/name=ccng_monit_http_healthcheck/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/quarks/bpm/processes/name=ccng_monit_http_healthcheck/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/quarks/bpm/processes/name=cloud_controller_ng/limits/memory
      value: 2048Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/quarks/bpm/processes/name=cloud_controller_ng/requests/memory
      value: 1024Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/quarks/bpm/processes/name=local_worker_1/limits/memory
      value: 800Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/quarks/bpm/processes/name=local_worker_1/requests/memory
      value: 400Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/quarks/bpm/processes/name=local_worker_2/limits/memory
      value: 800Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/quarks/bpm/processes/name=local_worker_2/requests/memory
      value: 400Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/quarks/bpm/processes/name=nginx/limits/memory
      value: 512Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/quarks/bpm/processes/name=nginx/requests/memory
      value: 128Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=dotnet-core-buildpack?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=file_server/properties?/quarks/bpm/processes/name=file_server/limits/memory
      value: 96Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=file_server/properties?/quarks/bpm/processes/name=file_server/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=go-buildpack?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=java-buildpack?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=loggr-udp-forwarder/properties?/quarks/bpm/processes/name=loggr-udp-forwarder/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=loggr-udp-forwarder/properties?/quarks/bpm/processes/name=loggr-udp-forwarder/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=nginx-buildpack?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=nodejs-buildpack?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=php-buildpack?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=policy-server/properties?/quarks/bpm/processes/name=policy-server/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=policy-server/properties?/quarks/bpm/processes/name=policy-server/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=policy-server-internal/properties?/quarks/bpm/processes/name=policy-server-internal/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=policy-server-internal/properties?/quarks/bpm/processes/name=policy-server-internal/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=python-buildpack?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=r-buildpack?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=route_registrar/properties?/quarks/bpm/processes/name=route_registrar/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=route_registrar/properties?/quarks/bpm/processes/name=route_registrar/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=ruby-buildpack?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=staticfile-buildpack?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=statsd_injector/properties?/quarks/bpm/processes/name=statsd_injector/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=api/jobs/name=statsd_injector/properties?/quarks/bpm/processes/name=statsd_injector/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=auctioneer/jobs/name=auctioneer/properties?/quarks/bpm/processes/name=auctioneer/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=auctioneer/jobs/name=auctioneer/properties?/quarks/bpm/processes/name=auctioneer/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties?/quarks/bpm/processes/name=worker_1/limits/memory
      value: 512Mi
    - type: replace
      path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties?/quarks/bpm/processes/name=worker_1/requests/memory
      value: 384Mi
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=bbs/properties?/quarks/bpm/processes/name=bbs/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=bbs/properties?/quarks/bpm/processes/name=bbs/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=cfdot?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=locket/properties?/quarks/bpm/processes/name=locket/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=locket/properties?/quarks/bpm/processes/name=locket/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=loggr-udp-forwarder/properties?/quarks/bpm/processes/name=loggr-udp-forwarder/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=loggr-udp-forwarder/properties?/quarks/bpm/processes/name=loggr-udp-forwarder/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=silk-controller/properties?/quarks/bpm/processes/name=silk-controller/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=diego-api/jobs/name=silk-controller/properties?/quarks/bpm/processes/name=silk-controller/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=cfdot?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=cflinuxfs3-rootfs-setup?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=garden/properties?/quarks/bpm/processes/name=garden/limits/memory
      value: 524288Mi
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=garden/properties?/quarks/bpm/processes/name=garden/requests/memory
      value: 16Mi
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=garden-cni?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=loggr-udp-forwarder/properties?/quarks/bpm/processes/name=loggr-udp-forwarder/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=loggr-udp-forwarder/properties?/quarks/bpm/processes/name=loggr-udp-forwarder/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=netmon/properties?/quarks/bpm/processes/name=netmon/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=netmon/properties?/quarks/bpm/processes/name=netmon/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=rep/properties?/quarks/bpm/processes/name=rep/limits/memory
      value: 3072Mi
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=rep/properties?/quarks/bpm/processes/name=rep/requests/memory
      value: 2048Mi
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=route_emitter/properties?/quarks/bpm/processes/name=route_emitter/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=route_emitter/properties?/quarks/bpm/processes/name=route_emitter/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=silk-cni?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=silk-daemon/properties?/quarks/bpm/processes/name=silk-daemon/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=silk-daemon/properties?/quarks/bpm/processes/name=silk-daemon/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=vxlan-policy-agent/properties?/quarks/bpm/processes/name=vxlan-policy-agent/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=vxlan-policy-agent/properties?/quarks/bpm/processes/name=vxlan-policy-agent/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=doppler/jobs/name=doppler/properties?/quarks/bpm/processes/name=doppler/limits/memory
      value: 256Mi
    - type: replace
      path: /instance_groups/name=doppler/jobs/name=doppler/properties?/quarks/bpm/processes/name=doppler/requests/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=log-api/jobs/name=loggregator_trafficcontroller/properties?/quarks/bpm/processes/name=loggregator_trafficcontroller/limits/memory
      value: 128Mi
    - type: replace
      path: /instance_groups/name=log-api/jobs/name=loggregator_trafficcontroller/properties?/quarks/bpm/processes/name=loggregator_trafficcontroller/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=log-api/jobs/name=reverse_log_proxy/properties?/quarks/bpm/processes/name=reverse_log_proxy/limits/memory
      value: 128Mi
    - type: replace
      path: /instance_groups/name=log-api/jobs/name=reverse_log_proxy/properties?/quarks/bpm/processes/name=reverse_log_proxy/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=log-api/jobs/name=reverse_log_proxy_gateway/properties?/quarks/bpm/processes/name=reverse_log_proxy_gateway/limits/memory
      value: 128Mi
    - type: replace
      path: /instance_groups/name=log-api/jobs/name=reverse_log_proxy_gateway/properties?/quarks/bpm/processes/name=reverse_log_proxy_gateway/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=log-api/jobs/name=route_registrar/properties?/quarks/bpm/processes/name=route_registrar/limits/memory
      value: 128Mi
    - type: replace
      path: /instance_groups/name=log-api/jobs/name=route_registrar/properties?/quarks/bpm/processes/name=route_registrar/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache/properties?/quarks/bpm/processes/name=log-cache/limits/memory
      value: 2048Mi
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache/properties?/quarks/bpm/processes/name=log-cache/requests/memory
      value: 512Mi
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache-cf-auth-proxy/properties?/quarks/bpm/processes/name=log-cache-cf-auth-proxy/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache-cf-auth-proxy/properties?/quarks/bpm/processes/name=log-cache-cf-auth-proxy/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache-gateway/properties?/quarks/bpm/processes/name=log-cache-gateway/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache-gateway/properties?/quarks/bpm/processes/name=log-cache-gateway/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache-nozzle/properties?/quarks/bpm/processes/name=log-cache-nozzle/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=log-cache-nozzle/properties?/quarks/bpm/processes/name=log-cache-nozzle/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=route_registrar/properties?/quarks/bpm/processes/name=route_registrar/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=log-cache/jobs/name=route_registrar/properties?/quarks/bpm/processes/name=route_registrar/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=nats/jobs/name=nats/properties?/quarks/bpm/processes/name=nats/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=nats/jobs/name=nats/properties?/quarks/bpm/processes/name=nats/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=nats/jobs/name=nats-tls/properties?/quarks/bpm/processes/name=nats-tls/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=nats/jobs/name=nats-tls/properties?/quarks/bpm/processes/name=nats-tls/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=rotate-cc-database-key/jobs/name=rotate_cc_database_key/properties?/quarks/bpm/processes/name=rotate/limits/memory
      value: 512Mi
    - type: replace
      path: /instance_groups/name=rotate-cc-database-key/jobs/name=rotate_cc_database_key/properties?/quarks/bpm/processes/name=rotate/requests/memory
      value: 192Mi
    - type: replace
      path: /instance_groups/name=router/jobs/name=gorouter/properties?/quarks/bpm/processes/name=gorouter/limits/memory
      value: 200Mi
    - type: replace
      path: /instance_groups/name=router/jobs/name=gorouter/properties?/quarks/bpm/processes/name=gorouter/requests/memory
      value: 50Mi
    - type: replace
      path: /instance_groups/name=router/jobs/name=loggr-udp-forwarder/properties?/quarks/bpm/processes/name=loggr-udp-forwarder/limits/memory
      value: 200Mi
    - type: replace
      path: /instance_groups/name=router/jobs/name=loggr-udp-forwarder/properties?/quarks/bpm/processes/name=loggr-udp-forwarder/requests/memory
      value: 50Mi
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=cfdot?/properties/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties?/quarks/bpm/processes/name=cloud_controller_clock/limits/memory
      value: 512Mi
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock/properties?/quarks/bpm/processes/name=cloud_controller_clock/requests/memory
      value: 128Mi
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=loggr-syslog-binding-cache/properties?/quarks/bpm/processes/name=loggr-syslog-binding-cache/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=loggr-syslog-binding-cache/properties?/quarks/bpm/processes/name=loggr-syslog-binding-cache/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=loggr-udp-forwarder/properties?/quarks/bpm/processes/name=loggr-udp-forwarder/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=loggr-udp-forwarder/properties?/quarks/bpm/processes/name=loggr-udp-forwarder/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=service-discovery-controller/properties?/quarks/bpm/processes/name=service-discovery-controller/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=service-discovery-controller/properties?/quarks/bpm/processes/name=service-discovery-controller/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=ssh_proxy/properties?/quarks/bpm/processes/name=ssh_proxy/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=ssh_proxy/properties?/quarks/bpm/processes/name=ssh_proxy/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=statsd_injector/properties?/quarks/bpm/processes/name=statsd_injector/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=statsd_injector/properties?/quarks/bpm/processes/name=statsd_injector/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=tps/properties?/quarks/bpm/processes/name=watcher/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=scheduler/jobs/name=tps/properties?/quarks/bpm/processes/name=watcher/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=singleton-blobstore/jobs/name=blobstore/properties?/quarks/bpm/processes/name=nginx/limits/memory
      value: 768Mi
    - type: replace
      path: /instance_groups/name=singleton-blobstore/jobs/name=blobstore/properties?/quarks/bpm/processes/name=nginx/requests/memory
      value: 192Mi
    - type: replace
      path: /instance_groups/name=singleton-blobstore/jobs/name=blobstore/properties?/quarks/bpm/processes/name=url_signer/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=singleton-blobstore/jobs/name=blobstore/properties?/quarks/bpm/processes/name=url_signer/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=singleton-blobstore/jobs/name=route_registrar/properties?/quarks/bpm/processes/name=route_registrar/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=singleton-blobstore/jobs/name=route_registrar/properties?/quarks/bpm/processes/name=route_registrar/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=uaa/jobs/name=route_registrar/properties?/quarks/bpm/processes/name=route_registrar/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=uaa/jobs/name=route_registrar/properties?/quarks/bpm/processes/name=route_registrar/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=uaa/jobs/name=statsd_injector/properties?/quarks/bpm/processes/name=statsd_injector/limits/memory
      value: 64Mi
    - type: replace
      path: /instance_groups/name=uaa/jobs/name=statsd_injector/properties?/quarks/bpm/processes/name=statsd_injector/requests/memory
      value: 32Mi
    - type: replace
      path: /instance_groups/name=uaa/jobs/name=uaa/properties?/quarks/bpm/processes/name=uaa/limits/memory
      value: 1400Mi
    - type: replace
      path: /instance_groups/name=uaa/jobs/name=uaa/properties?/quarks/bpm/processes/name=uaa/requests/memory
      value: 350Mi
    - type: replace
      path: /addons/name=forwarder_agent/jobs/name=loggr-forwarder-agent/properties?/quarks/bpm/processes/name=loggr-forwarder-agent/limits/memory
      value: 64Mi
    - type: replace
      path: /addons/name=forwarder_agent/jobs/name=loggr-forwarder-agent/properties?/quarks/bpm/processes/name=loggr-forwarder-agent/requests/memory
      value: 32Mi
    - type: replace
      path: /addons/name=loggr-syslog-agent/jobs/name=loggr-syslog-agent/properties?/quarks/bpm/processes/name=loggr-syslog-agent/limits/memory
      value: 64Mi
    - type: replace
      path: /addons/name=loggr-syslog-agent/jobs/name=loggr-syslog-agent/properties?/quarks/bpm/processes/name=loggr-syslog-agent/requests/memory
      value: 32Mi
    - type: replace
      path: /addons/name=loggregator_agent/jobs/name=loggregator_agent/properties?/quarks/bpm/processes/name=loggregator_agent/limits/memory
      value: 64Mi
    - type: replace
      path: /addons/name=loggregator_agent/jobs/name=loggregator_agent/properties?/quarks/bpm/processes/name=loggregator_agent/requests/memory
      value: 32Mi
    - type: replace
      path: /addons/name=prom_scraper/jobs/name=prom_scraper/properties?/quarks/bpm/processes/name=prom_scraper/limits/memory
      value: 64Mi
    - type: replace
      path: /addons/name=prom_scraper/jobs/name=prom_scraper/properties?/quarks/bpm/processes/name=prom_scraper/requests/memory
      value: 32Mi
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-resource-limits
  namespace: default
---
apiVersion: v1
data:
  ops: |-
    - type: remove
      path: /addons/name=bpm

    # The loggr forwarder agent needs a special env var with tags that identify the job
    # The AGENT_INDEX env var cannot be rendered properly as part of BPM rendering, it can only be set
    # correctly through pod reflection. Without this, the index would be 0 for any replica
    - type: replace
      path: /addons/name=forwarder_agent/jobs/name=loggr-forwarder-agent/properties?/quarks/envs
      value:
      - name: AGENT_IP
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
      - name: AGENT_INDEX
        valueFrom:
          fieldRef:
            fieldPath: metadata.labels['statefulset.kubernetes.io/pod-name']
      - name: AGENT_DEPLOYMENT
        valueFrom:
          fieldRef:
            fieldPath: metadata.labels['quarks.cloudfoundry.org/deployment-name']
      - name: AGENT_IG_NAME
        valueFrom:
          fieldRef:
            fieldPath: metadata.labels['quarks.cloudfoundry.org/instance-group-name']
      - name: AGENT_TAGS
        value: "deployment:$(AGENT_DEPLOYMENT),job:$(AGENT_IG_NAME)-loggr-forwarder-agent,index:$(AGENT_INDEX),ip:$(AGENT_IP)"

    # The loggregator agent needs env vars to identify the job
    # The AGENT_INDEX env var cannot be rendered properly as part of BPM rendering, it can only be set
    # correctly through pod reflection. Without this, the index would be 0 for any replica
    - type: replace
      path: /addons/name=loggregator_agent/jobs/name=loggregator_agent/properties?/quarks/envs
      value:
      - name: AGENT_IP
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
      - name: AGENT_INDEX
        valueFrom:
          fieldRef:
            fieldPath: metadata.labels['statefulset.kubernetes.io/pod-name']
      - name: AGENT_TAGS
        value: "deployment:$(AGENT_DEPLOYMENT),job:$(AGENT_JOB),index:$(AGENT_INDEX),ip:$(AGENT_IP)"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-addons
  namespace: default
---
apiVersion: v1
data:
  ops: |2-

    - type: remove
      path: /instance_groups/name=nats/azs?

    - type: remove
      path: /instance_groups/name=diego-api/azs?
    - type: remove
      path: /instance_groups/name=uaa/azs?
    - type: remove
      path: /instance_groups/name=singleton-blobstore/azs?
    - type: remove
      path: /instance_groups/name=api/azs?
    - type: remove
      path: /instance_groups/name=cc-worker/azs?
    - type: remove
      path: /instance_groups/name=scheduler/azs?
    - type: remove
      path: /instance_groups/name=router/azs?
    - type: remove
      path: /instance_groups/name=doppler/azs?
    - type: remove
      path: /instance_groups/name=log-api/azs?
    - type: remove
      path: /instance_groups/name=diego-cell/azs?
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-azs
  namespace: default
---
apiVersion: v1
data:
  ops: |2-

    - type: replace
      path: /instance_groups/name=api/env?/bosh/agent/settings/labels/app.kubernetes.io~1component
      value: "api"
    - type: replace
      path: /instance_groups/name=api/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=api/env?/bosh/agent/settings/labels/app.kubernetes.io~1managed-by
      value: "Helm"
    - type: replace
      path: /instance_groups/name=api/env?/bosh/agent/settings/labels/app.kubernetes.io~1name
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=api/env?/bosh/agent/settings/labels/app.kubernetes.io~1version
      value: "2.7.1"
    - type: replace
      path: /instance_groups/name=api/env?/bosh/agent/settings/labels/helm.sh~1chart
      value: "kubecf-2.7.1"
    - type: replace
      path: /instance_groups/name=auctioneer/env?/bosh/agent/settings/labels/app.kubernetes.io~1component
      value: "auctioneer"
    - type: replace
      path: /instance_groups/name=auctioneer/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=auctioneer/env?/bosh/agent/settings/labels/app.kubernetes.io~1managed-by
      value: "Helm"
    - type: replace
      path: /instance_groups/name=auctioneer/env?/bosh/agent/settings/labels/app.kubernetes.io~1name
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=auctioneer/env?/bosh/agent/settings/labels/app.kubernetes.io~1version
      value: "2.7.1"
    - type: replace
      path: /instance_groups/name=auctioneer/env?/bosh/agent/settings/labels/helm.sh~1chart
      value: "kubecf-2.7.1"
    - type: replace
      path: /instance_groups/name=cc-worker/env?/bosh/agent/settings/labels/app.kubernetes.io~1component
      value: "cc-worker"
    - type: replace
      path: /instance_groups/name=cc-worker/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=cc-worker/env?/bosh/agent/settings/labels/app.kubernetes.io~1managed-by
      value: "Helm"
    - type: replace
      path: /instance_groups/name=cc-worker/env?/bosh/agent/settings/labels/app.kubernetes.io~1name
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=cc-worker/env?/bosh/agent/settings/labels/app.kubernetes.io~1version
      value: "2.7.1"
    - type: replace
      path: /instance_groups/name=cc-worker/env?/bosh/agent/settings/labels/helm.sh~1chart
      value: "kubecf-2.7.1"
    - type: replace
      path: /instance_groups/name=diego-api/env?/bosh/agent/settings/labels/app.kubernetes.io~1component
      value: "diego-api"
    - type: replace
      path: /instance_groups/name=diego-api/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=diego-api/env?/bosh/agent/settings/labels/app.kubernetes.io~1managed-by
      value: "Helm"
    - type: replace
      path: /instance_groups/name=diego-api/env?/bosh/agent/settings/labels/app.kubernetes.io~1name
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=diego-api/env?/bosh/agent/settings/labels/app.kubernetes.io~1version
      value: "2.7.1"
    - type: replace
      path: /instance_groups/name=diego-api/env?/bosh/agent/settings/labels/helm.sh~1chart
      value: "kubecf-2.7.1"
    - type: replace
      path: /instance_groups/name=diego-cell/env?/bosh/agent/settings/labels/app.kubernetes.io~1component
      value: "diego-cell"
    - type: replace
      path: /instance_groups/name=diego-cell/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=diego-cell/env?/bosh/agent/settings/labels/app.kubernetes.io~1managed-by
      value: "Helm"
    - type: replace
      path: /instance_groups/name=diego-cell/env?/bosh/agent/settings/labels/app.kubernetes.io~1name
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=diego-cell/env?/bosh/agent/settings/labels/app.kubernetes.io~1version
      value: "2.7.1"
    - type: replace
      path: /instance_groups/name=diego-cell/env?/bosh/agent/settings/labels/helm.sh~1chart
      value: "kubecf-2.7.1"
    - type: replace
      path: /instance_groups/name=doppler/env?/bosh/agent/settings/labels/app.kubernetes.io~1component
      value: "doppler"
    - type: replace
      path: /instance_groups/name=doppler/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=doppler/env?/bosh/agent/settings/labels/app.kubernetes.io~1managed-by
      value: "Helm"
    - type: replace
      path: /instance_groups/name=doppler/env?/bosh/agent/settings/labels/app.kubernetes.io~1name
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=doppler/env?/bosh/agent/settings/labels/app.kubernetes.io~1version
      value: "2.7.1"
    - type: replace
      path: /instance_groups/name=doppler/env?/bosh/agent/settings/labels/helm.sh~1chart
      value: "kubecf-2.7.1"
    - type: replace
      path: /instance_groups/name=log-api/env?/bosh/agent/settings/labels/app.kubernetes.io~1component
      value: "log-api"
    - type: replace
      path: /instance_groups/name=log-api/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=log-api/env?/bosh/agent/settings/labels/app.kubernetes.io~1managed-by
      value: "Helm"
    - type: replace
      path: /instance_groups/name=log-api/env?/bosh/agent/settings/labels/app.kubernetes.io~1name
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=log-api/env?/bosh/agent/settings/labels/app.kubernetes.io~1version
      value: "2.7.1"
    - type: replace
      path: /instance_groups/name=log-api/env?/bosh/agent/settings/labels/helm.sh~1chart
      value: "kubecf-2.7.1"
    - type: replace
      path: /instance_groups/name=log-cache/env?/bosh/agent/settings/labels/app.kubernetes.io~1component
      value: "log-cache"
    - type: replace
      path: /instance_groups/name=log-cache/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=log-cache/env?/bosh/agent/settings/labels/app.kubernetes.io~1managed-by
      value: "Helm"
    - type: replace
      path: /instance_groups/name=log-cache/env?/bosh/agent/settings/labels/app.kubernetes.io~1name
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=log-cache/env?/bosh/agent/settings/labels/app.kubernetes.io~1version
      value: "2.7.1"
    - type: replace
      path: /instance_groups/name=log-cache/env?/bosh/agent/settings/labels/helm.sh~1chart
      value: "kubecf-2.7.1"
    - type: replace
      path: /instance_groups/name=nats/env?/bosh/agent/settings/labels/app.kubernetes.io~1component
      value: "nats"
    - type: replace
      path: /instance_groups/name=nats/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=nats/env?/bosh/agent/settings/labels/app.kubernetes.io~1managed-by
      value: "Helm"
    - type: replace
      path: /instance_groups/name=nats/env?/bosh/agent/settings/labels/app.kubernetes.io~1name
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=nats/env?/bosh/agent/settings/labels/app.kubernetes.io~1version
      value: "2.7.1"
    - type: replace
      path: /instance_groups/name=nats/env?/bosh/agent/settings/labels/helm.sh~1chart
      value: "kubecf-2.7.1"
    - type: replace
      path: /instance_groups/name=rotate-cc-database-key/env?/bosh/agent/settings/labels/app.kubernetes.io~1component
      value: "rotate-cc-database-key"
    - type: replace
      path: /instance_groups/name=rotate-cc-database-key/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=rotate-cc-database-key/env?/bosh/agent/settings/labels/app.kubernetes.io~1managed-by
      value: "Helm"
    - type: replace
      path: /instance_groups/name=rotate-cc-database-key/env?/bosh/agent/settings/labels/app.kubernetes.io~1name
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=rotate-cc-database-key/env?/bosh/agent/settings/labels/app.kubernetes.io~1version
      value: "2.7.1"
    - type: replace
      path: /instance_groups/name=rotate-cc-database-key/env?/bosh/agent/settings/labels/helm.sh~1chart
      value: "kubecf-2.7.1"
    - type: replace
      path: /instance_groups/name=router/env?/bosh/agent/settings/labels/app.kubernetes.io~1component
      value: "router"
    - type: replace
      path: /instance_groups/name=router/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=router/env?/bosh/agent/settings/labels/app.kubernetes.io~1managed-by
      value: "Helm"
    - type: replace
      path: /instance_groups/name=router/env?/bosh/agent/settings/labels/app.kubernetes.io~1name
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=router/env?/bosh/agent/settings/labels/app.kubernetes.io~1version
      value: "2.7.1"
    - type: replace
      path: /instance_groups/name=router/env?/bosh/agent/settings/labels/helm.sh~1chart
      value: "kubecf-2.7.1"
    - type: replace
      path: /instance_groups/name=scheduler/env?/bosh/agent/settings/labels/app.kubernetes.io~1component
      value: "scheduler"
    - type: replace
      path: /instance_groups/name=scheduler/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=scheduler/env?/bosh/agent/settings/labels/app.kubernetes.io~1managed-by
      value: "Helm"
    - type: replace
      path: /instance_groups/name=scheduler/env?/bosh/agent/settings/labels/app.kubernetes.io~1name
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=scheduler/env?/bosh/agent/settings/labels/app.kubernetes.io~1version
      value: "2.7.1"
    - type: replace
      path: /instance_groups/name=scheduler/env?/bosh/agent/settings/labels/helm.sh~1chart
      value: "kubecf-2.7.1"
    - type: replace
      path: /instance_groups/name=singleton-blobstore/env?/bosh/agent/settings/labels/app.kubernetes.io~1component
      value: "singleton-blobstore"
    - type: replace
      path: /instance_groups/name=singleton-blobstore/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=singleton-blobstore/env?/bosh/agent/settings/labels/app.kubernetes.io~1managed-by
      value: "Helm"
    - type: replace
      path: /instance_groups/name=singleton-blobstore/env?/bosh/agent/settings/labels/app.kubernetes.io~1name
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=singleton-blobstore/env?/bosh/agent/settings/labels/app.kubernetes.io~1version
      value: "2.7.1"
    - type: replace
      path: /instance_groups/name=singleton-blobstore/env?/bosh/agent/settings/labels/helm.sh~1chart
      value: "kubecf-2.7.1"
    - type: replace
      path: /instance_groups/name=uaa/env?/bosh/agent/settings/labels/app.kubernetes.io~1component
      value: "uaa"
    - type: replace
      path: /instance_groups/name=uaa/env?/bosh/agent/settings/labels/app.kubernetes.io~1instance
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=uaa/env?/bosh/agent/settings/labels/app.kubernetes.io~1managed-by
      value: "Helm"
    - type: replace
      path: /instance_groups/name=uaa/env?/bosh/agent/settings/labels/app.kubernetes.io~1name
      value: "kubecf"
    - type: replace
      path: /instance_groups/name=uaa/env?/bosh/agent/settings/labels/app.kubernetes.io~1version
      value: "2.7.1"
    - type: replace
      path: /instance_groups/name=uaa/env?/bosh/agent/settings/labels/helm.sh~1chart
      value: "kubecf-2.7.1"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-instance-groups
  namespace: default
---
apiVersion: v1
data:
  ops: |-
    # Remove all releases from cf-deployment; we'll only add back those we need
    - type: remove
      path: /releases
    - type: replace
      path: /releases?/name=binary-buildpack/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=binary-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=binary-buildpack/version?
      value: "1.0.36"
    - type: replace
      path: /releases?/name=capi/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=capi/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=capi/version?
      value: "1.98.0"
    - type: replace
      path: /releases?/name=cf-cli/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=cf-cli/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=cf-cli/version?
      value: "1.29.0"
    - type: replace
      path: /releases?/name=cf-networking/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=cf-networking/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=cf-networking/version?
      value: "2.33.0"
    - type: replace
      path: /releases?/name=cf-smoke-tests/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=cf-smoke-tests/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=cf-smoke-tests/version?
      value: "41.0.1"
    - type: replace
      path: /releases?/name=cflinuxfs3/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=cflinuxfs3/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=cflinuxfs3/version?
      value: "0.210.0"
    - type: replace
      path: /releases?/name=diego/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=diego/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=diego/version?
      value: "2.48.0"
    - type: replace
      path: /releases?/name=dotnet-core-buildpack/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=dotnet-core-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=dotnet-core-buildpack/version?
      value: "2.3.13"
    - type: replace
      path: /releases?/name=garden-runc/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=garden-runc/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=garden-runc/version?
      value: "1.19.17"
    - type: replace
      path: /releases?/name=go-buildpack/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=go-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=go-buildpack/version?
      value: "1.9.16"
    - type: replace
      path: /releases?/name=java-buildpack/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=java-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=java-buildpack/version?
      value: "4.32.1"
    - type: replace
      path: /releases?/name=log-cache/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=log-cache/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=log-cache/version?
      value: "2.8.0"
    - type: replace
      path: /releases?/name=loggregator/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=loggregator/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=loggregator/version?
      value: "106.3.10"
    - type: replace
      path: /releases?/name=loggregator-agent/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=loggregator-agent/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=loggregator-agent/version?
      value: "6.1.1"
    - type: replace
      path: /releases?/name=nats/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=nats/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=nats/version?
      value: "34"
    - type: replace
      path: /releases?/name=nginx-buildpack/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=nginx-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=nginx-buildpack/version?
      value: "1.1.12"
    - type: replace
      path: /releases?/name=nodejs-buildpack/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=nodejs-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=nodejs-buildpack/version?
      value: "1.7.25"
    - type: replace
      path: /releases?/name=php-buildpack/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=php-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=php-buildpack/version?
      value: "4.4.19"
    - type: replace
      path: /releases?/name=python-buildpack/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=python-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=python-buildpack/version?
      value: "1.7.18"
    - type: replace
      path: /releases?/name=r-buildpack/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=r-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=r-buildpack/version?
      value: "1.1.7"
    - type: replace
      path: /releases?/name=routing/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=routing/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=routing/version?
      value: "0.206.0"
    - type: replace
      path: /releases?/name=ruby-buildpack/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=ruby-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=ruby-buildpack/version?
      value: "1.8.23"
    - type: replace
      path: /releases?/name=silk/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=silk/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=silk/version?
      value: "2.33.0"
    - type: replace
      path: /releases?/name=sle15/url
      value: registry.suse.com/cap-staging
    - type: replace
      path: /releases/name=sle15/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=sle15/version?
      value: "24.94"
    - type: replace
      path: /releases?/name=staticfile-buildpack/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=staticfile-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=staticfile-buildpack/version?
      value: "1.5.9"
    - type: replace
      path: /releases?/name=statsd-injector/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=statsd-injector/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=statsd-injector/version?
      value: "1.11.15"
    - type: replace
      path: /releases?/name=suse-binary-buildpack/url
      value: registry.suse.com/cap-staging
    - type: replace
      path: /releases/name=suse-binary-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=suse-binary-buildpack/version?
      value: "1.0.36.1"
    - type: replace
      path: /releases?/name=suse-dotnet-core-buildpack/url
      value: registry.suse.com/cap-staging
    - type: replace
      path: /releases/name=suse-dotnet-core-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=suse-dotnet-core-buildpack/version?
      value: "2.3.18.1"
    - type: replace
      path: /releases?/name=suse-go-buildpack/url
      value: registry.suse.com/cap-staging
    - type: replace
      path: /releases/name=suse-go-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=suse-go-buildpack/version?
      value: "1.9.23.1"
    - type: replace
      path: /releases?/name=suse-java-buildpack/url
      value: registry.suse.com/cap-staging
    - type: replace
      path: /releases/name=suse-java-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=suse-java-buildpack/version?
      value: "4.34.0.1"
    - type: replace
      path: /releases?/name=suse-nginx-buildpack/url
      value: registry.suse.com/cap-staging
    - type: replace
      path: /releases/name=suse-nginx-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=suse-nginx-buildpack/version?
      value: "1.1.18.1"
    - type: replace
      path: /releases?/name=suse-nodejs-buildpack/url
      value: registry.suse.com/cap-staging
    - type: replace
      path: /releases/name=suse-nodejs-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=suse-nodejs-buildpack/version?
      value: "1.7.35.1"
    - type: replace
      path: /releases?/name=suse-php-buildpack/url
      value: registry.suse.com/cap-staging
    - type: replace
      path: /releases/name=suse-php-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=suse-php-buildpack/version?
      value: "4.4.26.1"
    - type: replace
      path: /releases?/name=suse-python-buildpack/url
      value: registry.suse.com/cap-staging
    - type: replace
      path: /releases/name=suse-python-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=suse-python-buildpack/version?
      value: "1.7.26.1"
    - type: replace
      path: /releases?/name=suse-ruby-buildpack/url
      value: registry.suse.com/cap-staging
    - type: replace
      path: /releases/name=suse-ruby-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=suse-ruby-buildpack/version?
      value: "1.8.27.1"
    - type: replace
      path: /releases?/name=suse-staticfile-buildpack/url
      value: registry.suse.com/cap-staging
    - type: replace
      path: /releases/name=suse-staticfile-buildpack/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=suse-staticfile-buildpack/version?
      value: "1.5.13.1"
    - type: replace
      path: /releases?/name=uaa/url
      value: ghcr.io/cloudfoundry-incubator
    - type: replace
      path: /releases/name=uaa/stemcell?
      value: {"os":"SLE_15_SP2","version":"29.1-7.0.0_374.gb8e8e6af"}
    - type: replace
      path: /releases/name=uaa/version?
      value: "74.24.0"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-releases
  namespace: default
---
apiVersion: v1
data:
  ops: |-
    # We specify the new dependencies (I) and remove the old serialization
    # hints (II).
    #
    # I. Custom pod dependencies for startup sequencing
    #

    # Dependency graph (the first to run is at the left):
    #
    # api        --> cc-worker
    #            --> scheduler
    # nats       --> log-api
    #            --> singleton-blobstore       (feature: singleton-blobstore)
    # uaa        --> log-cache
    #            --> routing-api --> router    (feature: routing-api)
    #            --> tcp-router                (feature: routing-api)
    #            --> credhub                   (feature: credhub)
    #            --> diego-cell                (feature: not eirini)
    # deigo-api  --> auctioneer                (feature: not eirini)
    # asdatabase --> asactors                  (feature: autoscaler)
    #            --> asapi                     (feature: autoscaler)
    #            --> asmetrics   --> asnozzle  (feature: autoscaler)
    - type: replace
      path: /instance_groups/name=cc-worker/env?/bosh/agent/settings/annotations/quarks.cloudfoundry.org~1wait-for
      # ATTENTION: Annotations are strings. While the cf-operator's
      # controller will unmarshall the value as a json array later on,
      # specification must be done as a string using json syntax inside.
      value: "[\"api\"]"
    - type: replace
      path: /instance_groups/name=scheduler/env?/bosh/agent/settings/annotations/quarks.cloudfoundry.org~1wait-for
      # ATTENTION: Annotations are strings. While the cf-operator's
      # controller will unmarshall the value as a json array later on,
      # specification must be done as a string using json syntax inside.
      value: "[\"api\"]"
    - type: replace
      path: /instance_groups/name=log-api/env?/bosh/agent/settings/annotations/quarks.cloudfoundry.org~1wait-for
      # ATTENTION: Annotations are strings. While the cf-operator's
      # controller will unmarshall the value as a json array later on,
      # specification must be done as a string using json syntax inside.
      value: "[\"nats\"]"
    - type: replace
      path: /instance_groups/name=log-cache/env?/bosh/agent/settings/annotations/quarks.cloudfoundry.org~1wait-for
      # ATTENTION: Annotations are strings. While the cf-operator's
      # controller will unmarshall the value as a json array later on,
      # specification must be done as a string using json syntax inside.
      value: "[\"uaa\"]"
    - type: replace
      path: /instance_groups/name=auctioneer/env?/bosh/agent/settings/annotations/quarks.cloudfoundry.org~1wait-for
      # ATTENTION: Annotations are strings. While the cf-operator's
      # controller will unmarshall the value as a json array later on,
      # specification must be done as a string using json syntax inside.
      value: "[\"diego-api\"]"
    - type: replace
      path: /instance_groups/name=diego-cell/env?/bosh/agent/settings/annotations/quarks.cloudfoundry.org~1wait-for
      # ATTENTION: Annotations are strings. While the cf-operator's
      # controller will unmarshall the value as a json array later on,
      # specification must be done as a string using json syntax inside.
      value: "[\"uaa\"]"
    - type: replace
      path: /instance_groups/name=singleton-blobstore/env?/bosh/agent/settings/annotations/quarks.cloudfoundry.org~1wait-for
      # ATTENTION: Annotations are strings. While the cf-operator's
      # controller will unmarshall the value as a json array later on,
      # specification must be done as a string using json syntax inside.
      value: "[\"nats\"]"

    #
    # II. Disable upstream pod startup serialization hints.
    #
    # Note: The serialization hints on the various test tasks are
    # irrelevant, and therefore ignored, and not changed.
    #
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-sequencing
  namespace: default
---
apiVersion: v1
data:
  ops: |-
    - type: replace
      path: /name
      value: kubecf
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-set-deployment-name
  namespace: default
---
apiVersion: v1
data:
  ops: |2-

    - type: replace
      path: /instance_groups/name=api/instances
      value: 1
    - type: replace
      path: /instance_groups/name=api/env?/bosh/agent/settings/affinity
      value:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: quarks.cloudfoundry.org/quarks-statefulset-name
                  operator: In
                  values:
                  - api
              topologyKey: kubernetes.io/hostname
    - type: replace
      path: /instance_groups/name=cc-worker/instances
      value: 1
    - type: replace
      path: /instance_groups/name=cc-worker/env?/bosh/agent/settings/affinity
      value:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: quarks.cloudfoundry.org/quarks-statefulset-name
                  operator: In
                  values:
                  - cc-worker
              topologyKey: kubernetes.io/hostname
    - type: replace
      path: /instance_groups/name=diego-api/instances
      value: 1
    - type: replace
      path: /instance_groups/name=diego-api/env?/bosh/agent/settings/affinity
      value:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: quarks.cloudfoundry.org/quarks-statefulset-name
                  operator: In
                  values:
                  - diego-api
              topologyKey: kubernetes.io/hostname
    - type: replace
      path: /instance_groups/name=doppler/instances
      value: 1
    - type: replace
      path: /instance_groups/name=doppler/env?/bosh/agent/settings/affinity
      value:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: quarks.cloudfoundry.org/quarks-statefulset-name
                  operator: In
                  values:
                  - doppler
              topologyKey: kubernetes.io/hostname
    - type: replace
      path: /instance_groups/name=log-api/instances
      value: 1
    - type: replace
      path: /instance_groups/name=log-api/env?/bosh/agent/settings/affinity
      value:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: quarks.cloudfoundry.org/quarks-statefulset-name
                  operator: In
                  values:
                  - log-api
              topologyKey: kubernetes.io/hostname
    - type: replace
      path: /instance_groups/name=nats/instances
      value: 1
    - type: replace
      path: /instance_groups/name=nats/env?/bosh/agent/settings/affinity
      value:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: quarks.cloudfoundry.org/quarks-statefulset-name
                  operator: In
                  values:
                  - nats
              topologyKey: kubernetes.io/hostname
    - type: replace
      path: /instance_groups/name=router/instances
      value: 1
    - type: replace
      path: /instance_groups/name=router/env?/bosh/agent/settings/affinity
      value:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: quarks.cloudfoundry.org/quarks-statefulset-name
                  operator: In
                  values:
                  - router
                  - diego-cell
              topologyKey: kubernetes.io/hostname
    - type: replace
      path: /instance_groups/name=scheduler/instances
      value: 1
    - type: replace
      path: /instance_groups/name=scheduler/env?/bosh/agent/settings/affinity
      value:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: quarks.cloudfoundry.org/quarks-statefulset-name
                  operator: In
                  values:
                  - scheduler
              topologyKey: kubernetes.io/hostname
    - type: replace
      path: /instance_groups/name=uaa/instances
      value: 1
    - type: replace
      path: /instance_groups/name=uaa/env?/bosh/agent/settings/affinity
      value:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: quarks.cloudfoundry.org/quarks-statefulset-name
                  operator: In
                  values:
                  - uaa
              topologyKey: kubernetes.io/hostname
    - type: replace
      path: /instance_groups/name=auctioneer/instances
      value: 1
    - type: replace
      path: /instance_groups/name=auctioneer/env?/bosh/agent/settings/affinity
      value:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: quarks.cloudfoundry.org/quarks-statefulset-name
                  operator: In
                  values:
                  - auctioneer
              topologyKey: kubernetes.io/hostname
    - type: replace
      path: /instance_groups/name=diego-cell/instances
      value: 1
    - type: replace
      path: /instance_groups/name=diego-cell/env?/bosh/agent/settings/affinity
      value:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: quarks.cloudfoundry.org/quarks-statefulset-name
                  operator: In
                  values:
                  - diego-cell
                  - router
              topologyKey: kubernetes.io/hostname
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-sizing
  namespace: default
---
apiVersion: v1
data:
  ops: |2-

    - type: replace
      path: /instance_groups/name=api/jobs/name=binary-buildpack?/name
      value: "binary-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=binary-buildpack/release?
      value: "binary-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=binary-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=dotnet-core-buildpack?/name
      value: "dotnet-core-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=dotnet-core-buildpack/release?
      value: "dotnet-core-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=dotnet-core-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=go-buildpack?/name
      value: "go-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=go-buildpack/release?
      value: "go-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=go-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=java-buildpack?/name
      value: "java-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=java-buildpack/release?
      value: "java-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=java-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=nginx-buildpack?/name
      value: "nginx-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=nginx-buildpack/release?
      value: "nginx-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=nginx-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=nodejs-buildpack?/name
      value: "nodejs-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=nodejs-buildpack/release?
      value: "nodejs-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=nodejs-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=php-buildpack?/name
      value: "php-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=php-buildpack/release?
      value: "php-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=php-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=python-buildpack?/name
      value: "python-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=python-buildpack/release?
      value: "python-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=python-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=r-buildpack?/name
      value: "r-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=r-buildpack/release?
      value: "r-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=r-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=ruby-buildpack?/name
      value: "ruby-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=ruby-buildpack/release?
      value: "ruby-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=ruby-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=staticfile-buildpack?/name
      value: "staticfile-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=staticfile-buildpack/release?
      value: "staticfile-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=staticfile-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-binary-buildpack?/name
      value: "suse-binary-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-binary-buildpack/release?
      value: "suse-binary-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-binary-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-dotnet-core-buildpack?/name
      value: "suse-dotnet-core-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-dotnet-core-buildpack/release?
      value: "suse-dotnet-core-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-dotnet-core-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-go-buildpack?/name
      value: "suse-go-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-go-buildpack/release?
      value: "suse-go-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-go-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-java-buildpack?/name
      value: "suse-java-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-java-buildpack/release?
      value: "suse-java-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-java-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-nginx-buildpack?/name
      value: "suse-nginx-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-nginx-buildpack/release?
      value: "suse-nginx-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-nginx-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-nodejs-buildpack?/name
      value: "suse-nodejs-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-nodejs-buildpack/release?
      value: "suse-nodejs-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-nodejs-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-php-buildpack?/name
      value: "suse-php-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-php-buildpack/release?
      value: "suse-php-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-php-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-python-buildpack?/name
      value: "suse-python-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-python-buildpack/release?
      value: "suse-python-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-python-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-ruby-buildpack?/name
      value: "suse-ruby-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-ruby-buildpack/release?
      value: "suse-ruby-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-ruby-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-staticfile-buildpack?/name
      value: "suse-staticfile-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-staticfile-buildpack/release?
      value: "suse-staticfile-buildpack"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-staticfile-buildpack/properties?/quarks/bpm/processes
      value: []
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=cflinuxfs3-rootfs-setup?
      value:
        name: cflinuxfs3-rootfs-setup
        release: cflinuxfs3
        properties:
          cflinuxfs3-rootfs:
            trusted_certs:
            - ((diego_instance_identity_ca.ca))
            - ((uaa_ssl.ca))
          quarks:
            bpm:
              processes: []
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=sle15-rootfs-setup?
      value:
        name: sle15-rootfs-setup
        release: sle15
        properties:
          sle15-rootfs:
            trusted_certs:
            - ((diego_instance_identity_ca.ca))
            - ((uaa_ssl.ca))
          quarks:
            bpm:
              processes: []

    # set default stack
    - path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock?/properties/cc/default_stack
      type: replace
      value: "sle15"
    - path: /instance_groups/name=api/jobs/name=cloud_controller_ng?/properties/cc/default_stack
      type: replace
      value: "sle15"
    - path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker?/properties/cc/default_stack
      type: replace
      value: "sle15"

    # set stack list
    - path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock?/properties/cc/stacks
      type: replace
      value: [{"description":"SUSE Linux Enterprise-based filesystem (SLE 15 SP2)","name":"sle15"},{"description":"Cloud Foundry Linux-based filesystem (Ubuntu 18.04)","name":"cflinuxfs3"}]
    - path: /instance_groups/name=api/jobs/name=cloud_controller_ng?/properties/cc/stacks
      type: replace
      value: [{"description":"SUSE Linux Enterprise-based filesystem (SLE 15 SP2)","name":"sle15"},{"description":"Cloud Foundry Linux-based filesystem (Ubuntu 18.04)","name":"cflinuxfs3"}]
    - path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker?/properties/cc/stacks
      type: replace
      value: [{"description":"SUSE Linux Enterprise-based filesystem (SLE 15 SP2)","name":"sle15"},{"description":"Cloud Foundry Linux-based filesystem (Ubuntu 18.04)","name":"cflinuxfs3"}]

    # set lifecycle bundles
    - path: /instance_groups/name=scheduler/jobs/name=cloud_controller_clock?/properties/cc/diego/lifecycle_bundles
      type: replace
      value: {"buildpack/cflinuxfs3":"buildpack_app_lifecycle/buildpack_app_lifecycle.tgz","buildpack/sle15":"buildpack_app_lifecycle/buildpack_app_lifecycle.tgz","docker":"docker_app_lifecycle/docker_app_lifecycle.tgz"}
    - path: /instance_groups/name=api/jobs/name=cloud_controller_ng?/properties/cc/diego/lifecycle_bundles
      type: replace
      value: {"buildpack/cflinuxfs3":"buildpack_app_lifecycle/buildpack_app_lifecycle.tgz","buildpack/sle15":"buildpack_app_lifecycle/buildpack_app_lifecycle.tgz","docker":"docker_app_lifecycle/docker_app_lifecycle.tgz"}
    - path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker?/properties/cc/diego/lifecycle_bundles
      type: replace
      value: {"buildpack/cflinuxfs3":"buildpack_app_lifecycle/buildpack_app_lifecycle.tgz","buildpack/sle15":"buildpack_app_lifecycle/buildpack_app_lifecycle.tgz","docker":"docker_app_lifecycle/docker_app_lifecycle.tgz"}
    # set list of pre-loaded rootfses
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=rep/properties/diego/rep/preloaded_rootfses
      value:
      - sle15:/var/vcap/data/rep/sle15/rootfs.tar
      - cflinuxfs3:/var/vcap/data/rep/cflinuxfs3/rootfs.tar
    - type: replace
      path: /instance_groups/name=diego-cell/env?/bosh/agent/settings/disks/-
      value:
        volumeMount:
          name: rep-data
          mountPath: /var/vcap/data/rep
        filters:
          job_name: "sle15-rootfs-setup"
          process_name: "sle15-rootfs-setup"
    - type: replace
      path: /instance_groups/name=diego-cell/env?/bosh/agent/settings/disks/-
      value:
        volumeMount:
          name: rep-data
          mountPath: /var/vcap/data/rep
        filters:
          job_name: "cflinuxfs3-rootfs-setup"
          process_name: "cflinuxfs3-rootfs-setup"

    # set list of all buildpacks in install order
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties/cc/install_buildpacks
      value:
      - name: "staticfile_buildpack"
        file: "/var/vcap/data/shared-packages/suse-staticfile-buildpack/staticfile-buildpack-sle15-v1.5.13.1.zip"
      - name: "java_buildpack"
        file: "/var/vcap/data/shared-packages/suse-java-buildpack/java-buildpack-sle15-v4.34.0.1.zip"
      - name: "ruby_buildpack"
        file: "/var/vcap/data/shared-packages/suse-ruby-buildpack/ruby-buildpack-sle15-v1.8.27.1.zip"
      - name: "dotnet-core_buildpack"
        file: "/var/vcap/data/shared-packages/suse-dotnet-core-buildpack/dotnet-core-buildpack-sle15-v2.3.18.1.zip"
      - name: "nodejs_buildpack"
        file: "/var/vcap/data/shared-packages/suse-nodejs-buildpack/nodejs-buildpack-sle15-v1.7.35.1.zip"
      - name: "go_buildpack"
        file: "/var/vcap/data/shared-packages/suse-go-buildpack/go-buildpack-sle15-v1.9.23.1.zip"
      - name: "python_buildpack"
        file: "/var/vcap/data/shared-packages/suse-python-buildpack/python-buildpack-sle15-v1.7.26.1.zip"
      - name: "php_buildpack"
        file: "/var/vcap/data/shared-packages/suse-php-buildpack/php-buildpack-sle15-v4.4.26.1.zip"
      - name: "nginx_buildpack"
        file: "/var/vcap/data/shared-packages/suse-nginx-buildpack/nginx-buildpack-sle15-v1.1.18.1.zip"
      - name: "binary_buildpack"
        file: "/var/vcap/data/shared-packages/suse-binary-buildpack/binary-buildpack-sle15-v1.0.36.1.zip"
      - name: "binary_buildpack"
        file: "/var/vcap/data/shared-packages/binary-buildpack/binary-buildpack-cflinuxfs3-v1.0.36.zip"
      - name: "dotnet-core_buildpack"
        file: "/var/vcap/data/shared-packages/dotnet-core-buildpack/dotnet-core-buildpack-cflinuxfs3-v2.3.13.zip"
      - name: "go_buildpack"
        file: "/var/vcap/data/shared-packages/go-buildpack/go-buildpack-cflinuxfs3-v1.9.16.zip"
      - name: "java_buildpack"
        file: "/var/vcap/data/shared-packages/java-buildpack/java-buildpack-cflinuxfs3-v4.32.1.zip"
      - name: "nodejs_buildpack"
        file: "/var/vcap/data/shared-packages/nodejs-buildpack/nodejs-buildpack-cflinuxfs3-v1.7.25.zip"
      - name: "nginx_buildpack"
        file: "/var/vcap/data/shared-packages/nginx-buildpack/nginx-buildpack-cflinuxfs3-v1.1.12.zip"
      - name: "r_buildpack"
        file: "/var/vcap/data/shared-packages/r-buildpack/r-buildpack-cflinuxfs3-v1.1.7.zip"
      - name: "php_buildpack"
        file: "/var/vcap/data/shared-packages/php-buildpack/php-buildpack-cflinuxfs3-v4.4.19.zip"
      - name: "python_buildpack"
        file: "/var/vcap/data/shared-packages/python-buildpack/python-buildpack-cflinuxfs3-v1.7.18.zip"
      - name: "ruby_buildpack"
        file: "/var/vcap/data/shared-packages/ruby-buildpack/ruby-buildpack-cflinuxfs3-v1.8.23.zip"
      - name: "staticfile_buildpack"
        file: "/var/vcap/data/shared-packages/staticfile-buildpack/staticfile-buildpack-cflinuxfs3-v1.5.9.zip"

    # create pre-rendering scripts
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=sle15-rootfs-setup/properties/quarks?/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        target="/var/vcap/all-releases/jobs-src/sle15/sle15-rootfs-setup/templates/pre-start"
        sentinel="${target}.patch_sentinel"
        if [[ -f "${sentinel}" ]]; then
          if sha256sum --check "${sentinel}" ; then
            echo "Patch already applied. Skipping"
            exit 0
          fi
          echo "Sentinel mismatch, re-patching"
        fi

        # Use the ephemeral data directory for the rootfs
        perl -p -i -e 's#\$ROOTFS_PACKAGE/rootfs#/var/vcap/data/rep/sle15/rootfs#' "${target}"

        sha256sum "${target}" > "${sentinel}"
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-staticfile-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="suse-staticfile-buildpack"
        buildpack="suse-staticfile-buildpack"
        package="staticfile-buildpack-sle15"
        version="1.5.13.1"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-staticfile-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="suse-staticfile-buildpack"
        job="suse-staticfile-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-java-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="suse-java-buildpack"
        buildpack="suse-java-buildpack"
        package="java-buildpack-sle15"
        version="4.34.0.1"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-java-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="suse-java-buildpack"
        job="suse-java-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-ruby-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="suse-ruby-buildpack"
        buildpack="suse-ruby-buildpack"
        package="ruby-buildpack-sle15"
        version="1.8.27.1"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-ruby-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="suse-ruby-buildpack"
        job="suse-ruby-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-dotnet-core-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="suse-dotnet-core-buildpack"
        buildpack="suse-dotnet-core-buildpack"
        package="dotnet-core-buildpack-sle15"
        version="2.3.18.1"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-dotnet-core-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="suse-dotnet-core-buildpack"
        job="suse-dotnet-core-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-nodejs-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="suse-nodejs-buildpack"
        buildpack="suse-nodejs-buildpack"
        package="nodejs-buildpack-sle15"
        version="1.7.35.1"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-nodejs-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="suse-nodejs-buildpack"
        job="suse-nodejs-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-go-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="suse-go-buildpack"
        buildpack="suse-go-buildpack"
        package="go-buildpack-sle15"
        version="1.9.23.1"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-go-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="suse-go-buildpack"
        job="suse-go-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-python-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="suse-python-buildpack"
        buildpack="suse-python-buildpack"
        package="python-buildpack-sle15"
        version="1.7.26.1"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-python-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="suse-python-buildpack"
        job="suse-python-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-php-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="suse-php-buildpack"
        buildpack="suse-php-buildpack"
        package="php-buildpack-sle15"
        version="4.4.26.1"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-php-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="suse-php-buildpack"
        job="suse-php-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-nginx-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="suse-nginx-buildpack"
        buildpack="suse-nginx-buildpack"
        package="nginx-buildpack-sle15"
        version="1.1.18.1"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-nginx-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="suse-nginx-buildpack"
        job="suse-nginx-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-binary-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="suse-binary-buildpack"
        buildpack="suse-binary-buildpack"
        package="binary-buildpack-sle15"
        version="1.0.36.1"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=suse-binary-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="suse-binary-buildpack"
        job="suse-binary-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=diego-cell/jobs/name=cflinuxfs3-rootfs-setup/properties/quarks?/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        target="/var/vcap/all-releases/jobs-src/cflinuxfs3/cflinuxfs3-rootfs-setup/templates/pre-start"
        sentinel="${target}.patch_sentinel"
        if [[ -f "${sentinel}" ]]; then
          if sha256sum --check "${sentinel}" ; then
            echo "Patch already applied. Skipping"
            exit 0
          fi
          echo "Sentinel mismatch, re-patching"
        fi

        # Use the ephemeral data directory for the rootfs
        perl -p -i -e 's#\$ROOTFS_PACKAGE/rootfs#/var/vcap/data/rep/cflinuxfs3/rootfs#' "${target}"

        sha256sum "${target}" > "${sentinel}"
    - type: replace
      path: /instance_groups/name=api/jobs/name=binary-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="binary-buildpack"
        buildpack="binary-buildpack"
        package="binary-buildpack-cflinuxfs3"
        version="1.0.36"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=binary-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="binary-buildpack"
        job="binary-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=dotnet-core-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="dotnet-core-buildpack"
        buildpack="dotnet-core-buildpack"
        package="dotnet-core-buildpack-cflinuxfs3"
        version="2.3.13"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=dotnet-core-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="dotnet-core-buildpack"
        job="dotnet-core-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=go-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="go-buildpack"
        buildpack="go-buildpack"
        package="go-buildpack-cflinuxfs3"
        version="1.9.16"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=go-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="go-buildpack"
        job="go-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=java-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="java-buildpack"
        buildpack="java-buildpack"
        package="java-buildpack-cflinuxfs3"
        version="4.32.1"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=java-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="java-buildpack"
        job="java-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=nodejs-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="nodejs-buildpack"
        buildpack="nodejs-buildpack"
        package="nodejs-buildpack-cflinuxfs3"
        version="1.7.25"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=nodejs-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="nodejs-buildpack"
        job="nodejs-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=nginx-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="nginx-buildpack"
        buildpack="nginx-buildpack"
        package="nginx-buildpack-cflinuxfs3"
        version="1.1.12"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=nginx-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="nginx-buildpack"
        job="nginx-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=r-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="r-buildpack"
        buildpack="r-buildpack"
        package="r-buildpack-cflinuxfs3"
        version="1.1.7"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=r-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="r-buildpack"
        job="r-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=php-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="php-buildpack"
        buildpack="php-buildpack"
        package="php-buildpack-cflinuxfs3"
        version="4.4.19"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=php-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="php-buildpack"
        job="php-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=python-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="python-buildpack"
        buildpack="python-buildpack"
        package="python-buildpack-cflinuxfs3"
        version="1.7.18"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=python-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="python-buildpack"
        job="python-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=ruby-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="ruby-buildpack"
        buildpack="ruby-buildpack"
        package="ruby-buildpack-cflinuxfs3"
        version="1.8.23"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=ruby-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="ruby-buildpack"
        job="ruby-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=staticfile-buildpack/properties?/quarks/pre_render_scripts/jobs/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Create the pre-start script that copies the buildpack package to /var/vcap/data/shared-packages/.

        release="staticfile-buildpack"
        buildpack="staticfile-buildpack"
        package="staticfile-buildpack-cflinuxfs3"
        version="1.5.9"

        pre_start="/var/vcap/all-releases/jobs-src/${release}/${buildpack}/templates/bin/pre-start"
        copy_dst="/var/vcap/data/shared-packages/${buildpack}"
        mkdir -p "$(dirname "${pre_start}")"
        cat <<EOT > "${pre_start}"
        #!/usr/bin/env bash
        set -o errexit
        mkdir -p "${copy_dst}"
        cp /var/vcap/packages/${package}/*.zip "${copy_dst}/${package}-v${version}.zip"
        EOT
    - type: replace
      path: /instance_groups/name=api/jobs/name=staticfile-buildpack/properties?/quarks/pre_render_scripts/ig_resolver/-
      value: |
        #!/usr/bin/env bash
        set -o errexit -o nounset

        # Add bin/pre-start to the buildpack job templates.

        release="staticfile-buildpack"
        job="staticfile-buildpack"

        job_mf="/var/vcap/all-releases/jobs-src/${release}/${job}/job.MF"

        sed -i 's|templates: {}||' "${job_mf}"
        cat <<EOT > "${job_mf}"
        templates:
          bin/pre-start: bin/pre-start
        EOT
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-stacks
  namespace: default
---
apiVersion: v1
data:
  ops: |2-


    - type: replace
      path: /stemcells/alias=default
      value:
        alias: default
        os: "SLE_15_SP2"
        version: "29.1-7.0.0_374.gb8e8e6af"
    - type: replace
      path: /addons/name=forwarder_agent/include/stemcell/os=ubuntu-xenial/os
      value: "SLE_15_SP2"
    - type: replace
      path: /addons/name=loggr-syslog-agent/include/stemcell/os=ubuntu-xenial/os
      value: "SLE_15_SP2"
    - type: replace
      path: /addons/name=loggregator_agent/include/stemcell/os=ubuntu-xenial/os
      value: "SLE_15_SP2"
    - type: replace
      path: /addons/name=prom_scraper/include/stemcell/os=ubuntu-xenial/os
      value: "SLE_15_SP2"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: ops-stemcells
  namespace: default
---
apiVersion: v1
data:
  ops: |-
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/build
      value: "2.7.1"
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/cc/jobs/local/number_of_workers
      value: 2
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/description
      value: "CloudFoundry KubeCF"
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/name
      value: "KubeCF"
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/support_address
      value: "https://kubecf.io/"
    - type: replace
      path: /instance_groups/name=api/jobs/name=cloud_controller_ng/properties?/version
      value: 2
    - type: replace
      path: /instance_groups/name=cc-worker/jobs/name=cloud_controller_worker/properties?/cc/jobs/generic/number_of_workers
      value: 1
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: operations
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: user-provided-properties
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kubecf-default-psp
  namespace: default
rules:
- apiGroups:
  - policy
  resourceNames:
  - kubecf-default
  resources:
  - podsecuritypolicies
  verbs:
  - use
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  name: kubecf-default-psp
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubecf-default-psp
subjects:
- kind: ServiceAccount
  name: default
  namespace: default
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: apps-dns
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: apps-dns
  namespace: default
spec:
  ports:
  - name: dns-udp
    port: 53
    protocol: UDP
    targetPort: dns-udp
  - name: dns-tcp
    port: 53
    protocol: TCP
    targetPort: dns-tcp
  selector:
    app: apps-dns
    app.kubernetes.io/name: kubecf
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: database
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: database
  namespace: default
spec:
  ports:
  - name: mysql
    port: 3306
    targetPort: mysql
  selector:
    app.kubernetes.io/component: database
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/name: kubecf
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: service-discovery-controller
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: service-discovery-controller
  namespace: default
spec:
  ports:
  - name: service-discovery-controller
    port: 8054
    protocol: TCP
    targetPort: 8054
  selector:
    quarks.cloudfoundry.org/deployment-name: kubecf
    quarks.cloudfoundry.org/instance-group-name: scheduler
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kbld.k14s.io/images: |
      - Metas:
        - Tag: 0.1.0
          Type: resolved
          URL: ghcr.io/cfcontainerizationbot/kubecf-apps-dns:0.1.0
        URL: ghcr.io/cfcontainerizationbot/kubecf-apps-dns@sha256:82db593bd6320e48209e1f326ce561f0796df0f5c0ee0537069349363cdca5d3
  labels:
    app: apps-dns
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: cf-apps-dns
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apps-dns
      app.kubernetes.io/instance: kubecf
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kubecf
  template:
    metadata:
      labels:
        app: apps-dns
        app.kubernetes.io/instance: kubecf
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kubecf
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - apps-dns
              topologyKey: kubernetes.io/hostname
            weight: 100
      containers:
      - args:
        - -conf
        - /config/Corefile
        image: ghcr.io/cfcontainerizationbot/kubecf-apps-dns@sha256:82db593bd6320e48209e1f326ce561f0796df0f5c0ee0537069349363cdca5d3
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        name: apps-dns
        ports:
        - containerPort: 53
          name: dns-udp
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        volumeMounts:
        - mountPath: /config/Corefile
          name: corefile-config
          readOnly: true
          subPath: Corefile
        - mountPath: /config/forward.conf
          name: forward-config
          readOnly: true
          subPath: forward.conf
        - mountPath: /tls
          name: client-tls
          readOnly: true
      initContainers:
      - args:
        - --upstream-dns-host
        - coredns-quarks.default.svc
        - --out
        - /config/forward.conf
        command:
        - /resolvwriter
        image: ghcr.io/cfcontainerizationbot/kubecf-apps-dns@sha256:82db593bd6320e48209e1f326ce561f0796df0f5c0ee0537069349363cdca5d3
        imagePullPolicy: IfNotPresent
        name: resolv-writer
        volumeMounts:
        - mountPath: /config
          name: forward-config
      volumes:
      - configMap:
          name: apps-dns
        name: corefile-config
      - emptyDir: {}
        name: forward-config
      - name: client-tls
        secret:
          items:
          - key: ca
            path: ca.pem
          - key: certificate
            path: cert.pem
          - key: private_key
            path: key.pem
          secretName: var-cf-app-sd-client-tls
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/backend-protocol: HTTPS
    nginx.ingress.kubernetes.io/proxy-body-size: 64m
    nginx.ingress.kubernetes.io/secure-backends: "true"
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.org/websocket-services: router
  labels:
    app.kubernetes.io/component: ingress
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: kubecf
  namespace: default
spec:
  rules:
  - host: '*.kubecf.vcap.me'
    http:
      paths:
      - backend:
          serviceName: router
          servicePort: 443
        path: /
  tls:
  - hosts:
    - '*.kubecf.vcap.me'
    secretName: kubecf-ingress-tls
---
apiVersion: quarks.cloudfoundry.org/v1alpha1
kind: BOSHDeployment
metadata:
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: kubecf
  namespace: default
spec:
  manifest:
    name: cf-deployment
    type: configmap
  ops:
  - name: ops-acceptance-tests
    type: configmap
  - name: ops-api
    type: configmap
  - name: ops-app-autoscaler
    type: configmap
  - name: ops-auctioneer
    type: configmap
  - name: ops-brain-tests
    type: configmap
  - name: ops-cc-worker
    type: configmap
  - name: ops-credhub
    type: configmap
  - name: ops-database
    type: configmap
  - name: ops-diego-api
    type: configmap
  - name: ops-diego-cell
    type: configmap
  - name: ops-doppler
    type: configmap
  - name: ops-eirini-helm
    type: configmap
  - name: ops-log-api
    type: configmap
  - name: ops-log-cache
    type: configmap
  - name: ops-nats
    type: configmap
  - name: ops-rotate-cc-database-key
    type: configmap
  - name: ops-router
    type: configmap
  - name: ops-routing-api
    type: configmap
  - name: ops-scheduler
    type: configmap
  - name: ops-singleton-blobstore
    type: configmap
  - name: ops-smoke-tests
    type: configmap
  - name: ops-sync-integration-tests
    type: configmap
  - name: ops-tcp-router
    type: configmap
  - name: ops-uaa
    type: configmap
  - name: ops-azs
    type: configmap
  - name: ops-sizing
    type: configmap
  - name: ops-addons
    type: configmap
  - name: ops-instance-groups
    type: configmap
  - name: ops-releases
    type: configmap
  - name: ops-sequencing
    type: configmap
  - name: ops-set-deployment-name
    type: configmap
  - name: ops-stacks
    type: configmap
  - name: ops-stemcells
    type: configmap
  - name: ops-resource-limits
    type: configmap
  - name: user-provided-properties
    type: configmap
  vars: []
---
apiVersion: quarks.cloudfoundry.org/v1alpha1
kind: QuarksJob
metadata:
  annotations:
    kbld.k14s.io/images: |
      - Metas:
        - Tag: 0.9.11
          Type: resolved
          URL: ghcr.io/cloudfoundry-incubator/pxc:0.9.11
        URL: ghcr.io/cloudfoundry-incubator/pxc@sha256:c3a0d231824d2d2920696411d17626d473d408be299a7ca13bbb1f614887b05d
  labels:
    app.kubernetes.io/component: database
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: database-migrate-charset
  namespace: default
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/component: database
        app.kubernetes.io/instance: kubecf
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kubecf
        app.kubernetes.io/version: 2.7.1
        helm.sh/chart: kubecf-2.7.1
    spec:
      template:
        metadata:
          labels:
            app.kubernetes.io/component: database
            app.kubernetes.io/instance: kubecf
            app.kubernetes.io/managed-by: Helm
            app.kubernetes.io/name: kubecf
            app.kubernetes.io/version: 2.7.1
            helm.sh/chart: kubecf-2.7.1
        spec:
          containers:
          - command:
            - /bin/bash
            - -c
            - |-
              #!/usr/bin/env bash

              # This script migrates the character set and collate of the KubeCF databases. This was originally
              # introduced to migrate from latin1 to utf8 on existing installations up to v2.1.0.

              set -o errexit -o nounset -o pipefail

              cat > "${HOME}/.my.cnf" <<EOF
              [mysql]
              host=${DATABASE_HOST}
              user=root
              password=${DATABASE_ROOT_PASSWORD}
              EOF

              echo "Waiting for database to be ready..."
              until echo "SELECT 'Ready!'" | mysql --connect-timeout=3 1> /dev/null 2> /dev/null; do
                sleep 1
              done

              function show_tables() {
                local database=$1
                mysql \
                  --database="${database}" \
                  --batch \
                  --skip-column-names \
                  --execute "SHOW TABLES"
              }

              function show_columns() {
                local database=$1
                local table=$2
                mysql \
                  --database="${database}" \
                  --batch \
                  --skip-column-names \
                  --execute "SHOW COLUMNS FROM ${table}"
              }

              function alter_tables() {
                local database=$1

                while read -r table; do
                  >&2 echo "Generating statements for \`${database}\`.\`${table}\`..."

                  echo "ALTER TABLE \`${table}\` DEFAULT CHARACTER SET ${CHARACTER_SET};"

                  while read -r column; do
                    field_type=$(awk --field-separator="\t" '{ print toupper($2) }' <<<"${column}")

                    awk_program='/^(CHAR|VARCHAR|TINYTEXT|TEXT|MEDIUMTEXT|LONGTEXT|ENUM|SET)/ { print "modify" }'
                    if [[ "$(awk "${awk_program}" <<<"${field_type}")" == "modify" ]]; then
                      field_name=$(awk --field-separator="\t" '{ print $1 }' <<<"${column}")
                      null_opt=$(awk --field-separator="\t" '{ print ($3 == "YES") ? "NULL" : "NOT NULL"}' <<<"${column}")
                      field_default=$(awk --field-separator="\t" '{ print $5 }' <<<"${column}")
                      if [[ "${field_default}" == "NULL" ]]; then
                        if [[ "${null_opt}" == "NOT NULL" ]]; then
                          default_opt=""
                        else
                          default_opt="DEFAULT NULL"
                        fi
                      else
                        default_opt="DEFAULT '${field_default}'"
                      fi

                      echo "ALTER TABLE \`${table}\` MODIFY \`${field_name}\` ${field_type} CHARACTER SET ${CHARACTER_SET} ${null_opt} ${default_opt};"
                    fi
                  done < <(show_columns "${database}" "${table}")
                done < <(show_tables "${database}")
              }

              function get_charset() {
                local database=$1
                mysql \
                  --database="${database}" \
                  --batch \
                  --skip-column-names \
                  --execute "SELECT default_character_set_name FROM information_schema.SCHEMATA WHERE schema_name = '${database}';"
              }

              function alter_database() {
                local database=$1

                current_charset=$(get_charset "${database}")
                if [[ "${current_charset}" == "${CHARACTER_SET}" ]]; then
                  return 0
                fi

                echo "ALTER DATABASE \`${database}\` DEFAULT CHARACTER SET ${CHARACTER_SET} DEFAULT COLLATE ${COLLATE};"
                echo "USE \`${database}\`;"
                echo "SET sql_mode = 'NO_AUTO_VALUE_ON_ZERO';"
                echo "SET foreign_key_checks = 0;"

                alter_tables "${database}"

                echo "SET foreign_key_checks = 1;"
                echo -e "\n"
              }

              STATEMENT=$(
              echo "START TRANSACTION;"

              alter_database "kubecf"

              for database in ${DATABASES}; do
                if [[ -z "${database}" ]]; then continue; fi
                alter_database "${database}"
              done

              echo "COMMIT;"
              )

              mysql <<<"${STATEMENT}"
            env:
            - name: DATABASE_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: var-pxc-root-password
            - name: DATABASE_HOST
              value: database.default
            - name: CHARACTER_SET
              value: utf8
            - name: COLLATE
              value: utf8_unicode_ci
            - name: DATABASES
              value: |-
                cloud_controller
                diego
                locket
                network_connectivity
                network_policy
                uaa
            image: ghcr.io/cloudfoundry-incubator/pxc@sha256:c3a0d231824d2d2920696411d17626d473d408be299a7ca13bbb1f614887b05d
            imagePullPolicy: null
            name: migrate-charset
            volumeMounts:
            - mountPath: /passwords/cloud_controller
              name: cc-database-password
              readOnly: true
            - mountPath: /passwords/diego
              name: diego-database-password
              readOnly: true
            - mountPath: /passwords/locket
              name: locket-database-password
              readOnly: true
            - mountPath: /passwords/network_connectivity
              name: network-connectivity-database-password
              readOnly: true
            - mountPath: /passwords/network_policy
              name: network-policy-database-password
              readOnly: true
            - mountPath: /passwords/uaa
              name: uaa-database-password
              readOnly: true
          restartPolicy: Never
          volumes:
          - name: cc-database-password
            secret:
              secretName: var-cc-database-password
          - name: diego-database-password
            secret:
              secretName: var-diego-database-password
          - name: locket-database-password
            secret:
              secretName: var-locket-database-password
          - name: network-connectivity-database-password
            secret:
              secretName: var-network-connectivity-database-password
          - name: network-policy-database-password
            secret:
              secretName: var-network-policy-database-password
          - name: uaa-database-password
            secret:
              secretName: var-uaa-database-password
  trigger:
    strategy: manual
  updateOnConfigChange: true
---
apiVersion: quarks.cloudfoundry.org/v1alpha1
kind: QuarksJob
metadata:
  annotations:
    kbld.k14s.io/images: |
      - Metas:
        - Tag: 0.9.11
          Type: resolved
          URL: ghcr.io/cloudfoundry-incubator/pxc:0.9.11
        URL: ghcr.io/cloudfoundry-incubator/pxc@sha256:c3a0d231824d2d2920696411d17626d473d408be299a7ca13bbb1f614887b05d
  labels:
    app.kubernetes.io/component: database
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: database-seeder
  namespace: default
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/component: database
        app.kubernetes.io/instance: kubecf
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kubecf
        app.kubernetes.io/version: 2.7.1
        helm.sh/chart: kubecf-2.7.1
    spec:
      template:
        metadata:
          labels:
            app.kubernetes.io/component: database
            app.kubernetes.io/instance: kubecf
            app.kubernetes.io/managed-by: Helm
            app.kubernetes.io/name: kubecf
            app.kubernetes.io/version: 2.7.1
            helm.sh/chart: kubecf-2.7.1
        spec:
          containers:
          - command:
            - /bin/bash
            - -c
            - |-
              #!/usr/bin/env bash

              set -o errexit -o nounset -o pipefail

              cat > "${HOME}/.my.cnf" <<EOF
              [mysql]
              host=${DATABASE_HOST}
              user=root
              password=${DATABASE_ROOT_PASSWORD}
              EOF

              echo "Waiting for database to be ready..."
              until echo "SELECT 'Ready!'" | mysql --connect-timeout="${DATABASE_CONNECT_TIMEOUT}" 1> /dev/null 2> /dev/null; do
                sleep 1
              done

              mysql < <(
                echo "START TRANSACTION;"
                echo "\
                  CREATE DATABASE IF NOT EXISTS kubecf
                    DEFAULT CHARACTER SET ${CHARACTER_SET}
                    DEFAULT COLLATE ${COLLATE};
                  USE kubecf;
                  CREATE TABLE IF NOT EXISTS db_leader_election (
                    anchor tinyint(3) unsigned NOT NULL,
                    host varchar(128) NOT NULL,
                    last_seen_active timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    PRIMARY KEY (anchor)
                  );"
                for database in ${DATABASES}; do
                  if [[ -z "${database}" ]]; then continue; fi

                  password=$(</passwords/"${database}"/password)

                  echo "\
                    CREATE USER IF NOT EXISTS \`${database}\`;
                    ALTER USER \`${database}\` IDENTIFIED BY '${password}';

                    CREATE DATABASE IF NOT EXISTS \`${database}\`
                      DEFAULT CHARACTER SET ${CHARACTER_SET}
                      DEFAULT COLLATE ${COLLATE};

                    GRANT ALL ON \`${database}\`.* TO '${database}'@'%';
                  "

                  # Print out the name of the database for troubleshooting; this container
                  # otherwise has very little output.
                  echo "    ... will update database ${database}" >&2
                done
                echo "COMMIT;"
              )
              echo "Done!"
            env:
            - name: DATABASE_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: var-pxc-root-password
            - name: DATABASE_HOST
              value: database.default
            - name: DATABASE_CONNECT_TIMEOUT
              value: "3"
            - name: CHARACTER_SET
              value: utf8
            - name: COLLATE
              value: utf8_unicode_ci
            - name: DATABASES
              value: |-
                cloud_controller
                diego
                locket
                network_connectivity
                network_policy
                uaa
            image: ghcr.io/cloudfoundry-incubator/pxc@sha256:c3a0d231824d2d2920696411d17626d473d408be299a7ca13bbb1f614887b05d
            imagePullPolicy: null
            name: seeder
            volumeMounts:
            - mountPath: /passwords/cloud_controller
              name: cc-database-password
              readOnly: true
            - mountPath: /passwords/diego
              name: diego-database-password
              readOnly: true
            - mountPath: /passwords/locket
              name: locket-database-password
              readOnly: true
            - mountPath: /passwords/network_connectivity
              name: network-connectivity-database-password
              readOnly: true
            - mountPath: /passwords/network_policy
              name: network-policy-database-password
              readOnly: true
            - mountPath: /passwords/uaa
              name: uaa-database-password
              readOnly: true
          restartPolicy: Never
          volumes:
          - name: cc-database-password
            secret:
              secretName: var-cc-database-password
          - name: diego-database-password
            secret:
              secretName: var-diego-database-password
          - name: locket-database-password
            secret:
              secretName: var-locket-database-password
          - name: network-connectivity-database-password
            secret:
              secretName: var-network-connectivity-database-password
          - name: network-policy-database-password
            secret:
              secretName: var-network-policy-database-password
          - name: uaa-database-password
            secret:
              secretName: var-uaa-database-password
  trigger:
    strategy: once
  updateOnConfigChange: true
---
apiVersion: quarks.cloudfoundry.org/v1alpha1
kind: QuarksSecret
metadata:
  labels:
    app.kubernetes.io/component: database
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: var-pxc-root-password
  namespace: default
spec:
  secretName: var-pxc-root-password
  type: password
---
apiVersion: quarks.cloudfoundry.org/v1alpha1
kind: QuarksSecret
metadata:
  labels:
    app.kubernetes.io/component: database
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: var-pxc-password
  namespace: default
spec:
  secretName: var-pxc-password
  type: password
---
apiVersion: quarks.cloudfoundry.org/v1alpha1
kind: QuarksStatefulSet
metadata:
  annotations:
    kbld.k14s.io/images: |
      - Metas:
        - Tag: 0.9.11
          Type: resolved
          URL: ghcr.io/cloudfoundry-incubator/pxc:0.9.11
        URL: ghcr.io/cloudfoundry-incubator/pxc@sha256:c3a0d231824d2d2920696411d17626d473d408be299a7ca13bbb1f614887b05d
  labels:
    app.kubernetes.io/component: database
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubecf
    app.kubernetes.io/version: 2.7.1
    helm.sh/chart: kubecf-2.7.1
  name: database
  namespace: default
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/component: database
        app.kubernetes.io/instance: kubecf
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kubecf
        app.kubernetes.io/version: 2.7.1
        helm.sh/chart: kubecf-2.7.1
      name: database
      namespace: default
    spec:
      replicas: 1
      selector:
        matchLabels:
          app.kubernetes.io/component: database
          app.kubernetes.io/instance: kubecf
          app.kubernetes.io/name: kubecf
      serviceName: database
      template:
        metadata:
          annotations:
            quarks.cloudfoundry.org/restart-on-update: "true"
          labels:
            app.kubernetes.io/component: database
            app.kubernetes.io/instance: kubecf
            app.kubernetes.io/managed-by: Helm
            app.kubernetes.io/name: kubecf
            app.kubernetes.io/version: 2.7.1
            helm.sh/chart: kubecf-2.7.1
        spec:
          containers:
          - command:
            - /bin/bash
            - /startup-scripts/entrypoint.sh
            env:
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: var-pxc-root-password
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: var-pxc-password
            - name: ALLOW_ROOT_FROM
              value: '%'
            - name: PXC_STRICT_MODE
              value: ENFORCING
            image: ghcr.io/cloudfoundry-incubator/pxc@sha256:c3a0d231824d2d2920696411d17626d473d408be299a7ca13bbb1f614887b05d
            imagePullPolicy: null
            livenessProbe:
              exec:
                command:
                - /bin/bash
                - -c
                - mysqladmin ping || test -e /var/lib/mysql/sst_in_progress
              initialDelaySeconds: 300
              timeoutSeconds: 2
            name: database
            ports:
            - containerPort: 3306
              name: mysql
            readinessProbe:
              exec:
                command:
                - mysql
                - -h
                - 127.0.0.1
                - -e
                - SELECT 1
              initialDelaySeconds: 30
              timeoutSeconds: 2
            volumeMounts:
            - mountPath: /etc/mysql/tls/certs
              name: pxc-tls
            - mountPath: /var/lib/mysql
              name: pxc-data
            - mountPath: /etc/mysql/conf.d
              name: pxc-config-files
            - mountPath: /startup-scripts
              name: pxc-startup-scripts
            - mountPath: /root
              name: slash-root
            - mountPath: /var/log
              name: var-log
          - command:
            - tail
            - -f
            - /var/log/mysqld.log
            image: ghcr.io/cloudfoundry-incubator/pxc@sha256:c3a0d231824d2d2920696411d17626d473d408be299a7ca13bbb1f614887b05d
            imagePullPolicy: null
            name: logs
            volumeMounts:
            - mountPath: /var/log
              name: var-log
          initContainers:
          - command:
            - rm
            - -fr
            - /var/lib/mysql/lost+found
            image: ghcr.io/cloudfoundry-incubator/pxc@sha256:c3a0d231824d2d2920696411d17626d473d408be299a7ca13bbb1f614887b05d
            imagePullPolicy: null
            name: remove-lost-found
            volumeMounts:
            - mountPath: /var/lib/mysql
              name: pxc-data
          volumes:
          - emptyDir: {}
            name: slash-root
          - emptyDir: {}
            name: var-log
          - configMap:
              name: database-config-files
            name: pxc-config-files
          - configMap:
              name: database-startup-scripts
            name: pxc-startup-scripts
          - name: pxc-tls
            secret:
              secretName: var-pxc-tls
      volumeClaimTemplates:
      - metadata:
          name: pxc-data
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 20Gi
          storageClassName: null
  updateOnConfigChange: true
---
apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-2"
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
  name: pre-delete-helm-hook
  namespace: default
---
apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    helm.sh/hook: pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-2"
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
  name: pre-upgrade-helm-hook
  namespace: default
---
apiVersion: v1
data: {}
kind: ConfigMap
metadata:
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-2"
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
  name: pre-delete-hook-scripts
  namespace: default
---
apiVersion: v1
data:
  remove-deployment-updater-readiness.sh: "#!/bin/bash\nset -o errexit -o nounset
    -o pipefail -o xtrace\n\n# Remove the cc-deployment-updater readiness probes;
    they are incorrectly\n# constructed for an active/passive job, and cause the scheduler
    to show as\n# unready, and therefore blocking upgrades. See \n# https://github.com/cloudfoundry-incubator/kubecf/issues/1589
    for details.\n\n# Patch the StatefulSet so that any new instances we create will
    not have the\n# probe.\n\npatch='\n---\nspec:\n  template:\n    spec:\n      containers:\n
    \     - name: cc-deployment-updater-cc-deployment-updater\n        readinessProbe:
    ~\n'\n\nkubectl patch statefulset --namespace \"$NAMESPACE\" scheduler --patch
    \"$patch\"\n\n# Delete all existing scheduler pods; we can't just patch them as
    changing\n# existing readiness probes is not allowed.\n\nmapfile -t pods < <(kubectl
    get pods --namespace=\"${NAMESPACE}\" --output=name \\\n    --selector \"quarks.cloudfoundry.org/instance-group-name=scheduler\")\n\nquery='{.spec.containers[?(.name
    == \"cc-deployment-updater-cc-deployment-updater\")].readinessProbe.exec.command}'\n\nfor
    pod in \"${pods[@]}\"; do\n    probe=\"$(kubectl get --namespace=\"${NAMESPACE}\"
    \"${pod}\" --output=jsonpath=\"${query}\")\"\n    if [[ -n \"${probe}\" ]]; then\n
    \       kubectl delete --namespace=\"${NAMESPACE}\" \"${pod}\"\n    fi\ndone\n"
kind: ConfigMap
metadata:
  annotations:
    helm.sh/hook: pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-2"
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
  name: pre-upgrade-hook-scripts
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-2"
  name: pre-delete-helm-hook-role
  namespace: default
rules:
- apiGroups:
  - apps
  resources:
  - statefulsets
  verbs:
  - get
  - patch
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - get
  - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  annotations:
    helm.sh/hook: pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-2"
  name: pre-upgrade-helm-hook-role
  namespace: default
rules:
- apiGroups:
  - apps
  resources:
  - statefulsets
  verbs:
  - get
  - patch
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - get
  - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-2"
  creationTimestamp: null
  name: pre-delete-helm-hook-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pre-delete-helm-hook-role
subjects:
- kind: ServiceAccount
  name: pre-delete-helm-hook
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  annotations:
    helm.sh/hook: pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-2"
  creationTimestamp: null
  name: pre-upgrade-helm-hook-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pre-upgrade-helm-hook-role
subjects:
- kind: ServiceAccount
  name: pre-upgrade-helm-hook
  namespace: default
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-1"
    kbld.k14s.io/images: |
      - Metas:
        - Tag: v1.19.2
          Type: resolved
          URL: ghcr.io/cfcontainerizationbot/kubecf-kubectl:v1.19.2
        URL: ghcr.io/cfcontainerizationbot/kubecf-kubectl@sha256:61daa1bba5cb7146aa4dc503772933cf4c110a911668bd88d0998d718003c128
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
  name: kubecf-pre-delete-hook
  namespace: default
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: kubecf
        app.kubernetes.io/managed-by: Helm
        helm.sh/chart: kubecf-2.7.1
      name: kubecf
    spec:
      containers:
      - args:
        - |
          shopt -s nullglob
          for f in /hooks/*.sh; do
           bash "$f" || exit 1
          done
        command:
        - /bin/bash
        - -cex
        env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        image: ghcr.io/cfcontainerizationbot/kubecf-kubectl@sha256:61daa1bba5cb7146aa4dc503772933cf4c110a911668bd88d0998d718003c128
        name: pre-delete-job
        volumeMounts:
        - mountPath: /hooks
          name: hooks
          readOnly: true
      restartPolicy: Never
      serviceAccountName: pre-delete-helm-hook
      volumes:
      - configMap:
          name: pre-delete-hook-scripts
        name: hooks
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-1"
    kbld.k14s.io/images: |
      - Metas:
        - Tag: v1.19.2
          Type: resolved
          URL: ghcr.io/cfcontainerizationbot/kubecf-kubectl:v1.19.2
        URL: ghcr.io/cfcontainerizationbot/kubecf-kubectl@sha256:61daa1bba5cb7146aa4dc503772933cf4c110a911668bd88d0998d718003c128
  labels:
    app.kubernetes.io/instance: kubecf
    app.kubernetes.io/managed-by: Helm
  name: kubecf-pre-upgrade-hook
  namespace: default
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: kubecf
        app.kubernetes.io/managed-by: Helm
        helm.sh/chart: kubecf-2.7.1
      name: kubecf
    spec:
      containers:
      - args:
        - |
          shopt -s nullglob
          for f in /hooks/*.sh; do
           bash "$f" || exit 1
          done
        command:
        - /bin/bash
        - -cex
        env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        image: ghcr.io/cfcontainerizationbot/kubecf-kubectl@sha256:61daa1bba5cb7146aa4dc503772933cf4c110a911668bd88d0998d718003c128
        name: pre-upgrade-job
        volumeMounts:
        - mountPath: /hooks
          name: hooks
          readOnly: true
      restartPolicy: Never
      serviceAccountName: pre-upgrade-helm-hook
      volumes:
      - configMap:
          name: pre-upgrade-hook-scripts
        name: hooks
